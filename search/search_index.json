{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenOmni Framework","text":"<p>Multimodal Open Source Framework for Conversational Agents Research and Development.</p> <p> </p> <ul> <li>Motivation</li> <li>Video Demonstration</li> <li>Approaches<ul> <li>Traditional conversational agents</li> <li>OpenAI GPT-4o</li> <li>Hybrid Approach</li> <li>Summary</li> </ul> </li> <li>Our System Design<ul> <li>Requirements</li> <li>System Architecture</li> <li>Main Components<ul> <li>Client</li> <li>API</li> <li>Agent</li> </ul> </li> </ul> </li> <li>Benchmark Examples</li> <li>Deployment Options</li> <li>Tutorial</li> </ul>"},{"location":"#motivation","title":"Motivation","text":"<p>The development of Multimodal Large Language Models (LLMs) is opening new frontiers in conversational agent research and applications. Multimodal end-to-end conversational agents represents a significant advancement in our pursuit of general AI. This progress, however, is not without its challenges. Balancing cost, accuracy, and latency remains a difficult task.</p> <p>GPT-4 has demonstrated the capability of a fully end-to-end multimodal model to handle complex multimodal inputs, including audio and images, and generate coherent and contextually appropriate responses in a timely manner. Compared to traditional approaches such as speech-to-text, text generation, and text-to-speech, which suffer from latency issues that hinder their real-world applicability, GPT-4 has shown the potential to overcome these challenges. This showcases the potential of advanced AI models to navigate the rough path towards a bright future in conversational agents.</p> <p>Despite these advancements, achieving the right balance between cost, accuracy, and latency is a significant hurdle. This difficulty is the motivation behind our project. We aim to establish an open-source framework that allows researchers to easily test their contributions and collaborate effectively. By doing so, we hope to facilitate the research and development process, accelerating the journey towards more efficient and capable multimodal conversational Agent systems.</p>"},{"location":"#video-demonstration","title":"Video Demonstration","text":""},{"location":"#approaches","title":"Approaches","text":"<ul> <li>Traditional approach is to Divide and Conquer, however it suffers from the latency issue</li> <li>GPT-4o is End-to-End model, which is different from the traditional approach, and overcome the latency issue.   However, it is hard to implement for wider community.</li> <li>Other approaches is Hybrid Approach, an end to end voice model, which take audio and texts as input, and generate   audio as output. Video modality input can be converted to text, and then feed into the model.</li> </ul>"},{"location":"#traditional-conversational-agents","title":"Traditional conversational Agents","text":"<p>The whole pipeline divided into several components, mainly including:</p> <ul> <li>Speech2Text</li> <li>Information Retrieval (Text based or Video based)</li> <li>Text Generation</li> <li>Text2Speech</li> </ul> <p>Before the era of LLM, Speech2Text and Text2Speech tasks are the most successful ones, delivering quite a few successful products. However, generate in-context response is a challenging task, which is a bottleneck for the whole pipeline, which is the Accuracy issue. Development of LLMs solves this problem directly.</p> <p>However, it is still one more problem, which is the Latency issue, before it can really impact the real world seriously.</p>"},{"location":"#openai-gpt-4o","title":"OpenAI GPT-4o","text":"<p>Demonstration from OpenAI indicates that they have achieved acceptable Latency performance, which is around 250ms. At the same time, the Accuracy is also impressive, according to their released benchmark: GPT-4o Benchmark.</p> <p>However, we noticed that there is a cable connected to the iPhone when they are doing the demonstration, and it is described to maintain stable and high speed internet connection. Which means, reasonable latency is achievable, however, not yet scalable to the real world. The delay of public release of GPT-4o with audio and video ability proves this point to some extent. Our testing with current GPT-4o endpoints indicates even the fastest pipeline combination with OpenAI API ( Whisper,GPT-3.5,OpenAI TTS) still have 5-8 seconds latency.</p>"},{"location":"#hybrid-approach","title":"Hybrid Approach","text":"<p>Unlike GPT-4o, with audio, video, or/and text as input, which is challenging to implement, some researchers are exploring a hybrid approach, which is to take audio and/or text as input, and generate audio and/or text as output.</p> <p>Moshi is one of the examples. Images and Videos normally are very large, and require a lot of computation power to process, this approaches can overcome this issue. Another advantage of this is that without take the video as input, it avoids the privacy issue, act not like a \"Big Brother is Watch you\", potentially is more acceptable by the public.</p>"},{"location":"#summary","title":"Summary","text":"<p>The research focus for multimodal conversational agents is shifting towards end-to-end models, however, it is a **money ** game. To achieve that, you will need intelligent people (which we have in research community), data, and money. Data and money is not evenly distributed within the research community, which makes the wider research community hard to catch up with the cutting edge research.</p> <p>Sadly to say that, but reality is the rest research community without enough data and money can work towards the direction: adopt and applying conversational agents the into the real world, and make it more accessible to the public. Contributing to the bigger picture as a small piece of the puzzle.</p> <p>This leads to the development and goal of OpenOmni Framework Project.</p> <ol> <li>Collaborative Efforts: Crowd efforts where everyone focuses on advancing their own part without reinventing the    wheel.</li> <li>High Agility: A process that allows people to easily test their models, understand real application issues, and    improve them.</li> <li>Gathering Data: More data to advance system development, especially for complex conversational scenarios. This    includes understanding who is talking, the context, and the emotion, which is lacking in current datasets.</li> </ol> <p>We want to:</p> <ul> <li>Allows people to easily test their models, whether end-to-end or single components within the pipeline.</li> <li>Can be easily deployed, enabling researchers to collect the data they need with minimal effort for adaptation.</li> </ul> <p>Our ultimate goal is to:</p> <ul> <li>Develop an open-source, end-to-end conversational agents robot that match the capabilities of OpenAI's GPT-4o.</li> <li>Enable easy data collection for advancing the development of end-to-end conversational agents.</li> <li>Inspire and foster the development of innovative conversational agent applications and products.</li> </ul>"},{"location":"#system-design","title":"System Design","text":""},{"location":"#requirements","title":"Requirements","text":"<p>What the functions we need from the system perspective:</p> <ol> <li>Data Collection: Video and audio inputs are collected from hardware devices.</li> <li>Data Transfer: Data is sent to the API for downstream processing.</li> <li>Data Processing: The API uses Agents and ML models to process the data, generate responses.</li> <li>Reaction: The client side is notified to play the speech.</li> </ol> <p>There are two key metrics we are focusing on:</p> <ul> <li>Model Latency: The time it takes for the model to generate a response.</li> <li>Model Accuracy: Whether the model generates the in-context response or accurate response.</li> </ul> <p>Which will allow to react to user's query in a timely manner, and provide the most in-context response.</p>"},{"location":"#system-architecture","title":"System Architecture","text":""},{"location":"#main-components","title":"Main Components","text":"<p>So as you can see from the system architecture, we have three main components: Client, API, and AI.</p> <p>Client will be in charge of data acquisition, and data presentation. Which means the video and audio data will be collected from the client side, and the speech generated by the Agent will be played by the client side.</p> <p>API will be the orchestrator, which will manage the models, and provide the API interface for the Client/Agent module to access. It also provides the access and CURD to the data sources (Neo4j, PostgreSQL and Hard Disk). It also includes an interface for end users (Researchers and Annotators) to interact with the system.</p> <p>Agent module is seperated to make sure it can be scaled to use different computation resources, and easily to integrate new work in.</p> <p>All modules are written in Python.</p> <ul> <li>Client side, Listener and Responder are two separate python package, which will allow the separate deployment of data   acquisition and playing audio speech.</li> <li>API end is built with Django, and have a customised admin interface, which will allow the management of the data   sources, and the models.</li> <li>Data side, we have Neo4j, PostgreSQL, and Hard Disk, which will allow the data to be stored in different places.</li> <li>Agent module is also a Python package.</li> </ul>"},{"location":"#client","title":"Client","text":"<p>Detailed information can be found in Client.</p> <p>The client side will mainly in charge of</p> <ul> <li>Data Acquisition<ul> <li>Audio</li> <li>Video</li> </ul> </li> <li>Data Transfer<ul> <li>Transfer the data to storage places</li> </ul> </li> <li>Data Presentation<ul> <li>Play the speech</li> </ul> </li> </ul>"},{"location":"#api","title":"API","text":"<p>Detailed information can be found in API.</p> <p>This is the Brain of the system, which will be in charge of:</p> <ul> <li>Knowledge Base<ul> <li>Relational Database</li> <li>Graph Database</li> <li>Hard Disk</li> </ul> </li> <li>Models Orchestration<ul> <li>LLM</li> <li>GPT-4 API</li> <li>Self-developed models</li> </ul> </li> <li>API Interface<ul> <li>RESTful API</li> </ul> </li> <li>Data Management with an Admin interface</li> <li>Annotation Interface for the annotators</li> <li>Benchmark report for the researchers</li> </ul>"},{"location":"#agent","title":"Agent","text":"<p>Detailed information can be found in Agent.</p> <ul> <li>Running the ML or Agent models<ul> <li>Running the models which require intensive computation</li> <li>LLM models</li> <li>Text2Speech models</li> <li>Emotion Recognition models</li> <li>etc.</li> </ul> </li> </ul>"},{"location":"#benchmark-examples","title":"Benchmark Examples","text":"<p>We provided benchmarks for both latency and accuracy.</p>"},{"location":"#latency","title":"Latency","text":"<p>When we log the time within the pipeline, we will log both timestamp information for critical time points, and the time duration it takes to finish a specific task. In this way, we can generate a latency report for each component within the pipeline, and also operate whether the latency is caused by the data transfer or the model inference process, so that we can further optimize the system. With the timestamp information of the time point, we can easily retrieve the timeline of the process, with good observability, it will allow us to debug easily, or notice the potential issues.</p> <p>We provide two report interface for the latency:</p> <ul> <li>Summary Report: Which will provide the average latency for each component within the pipeline (in seconds).   </li> <li>Detail Report: Which will provide the detailed information round of conversation, timeline and overall latency  (   in seconds).   </li> </ul> <p>You can configure the system to run the same datasets through different pipelines, and in the end evaluate the performance, do the comparison.</p>"},{"location":"#accuracy","title":"Accuracy","text":"<p>Accuracy probably is not the most correct name for it, as for the text generation task, you will want to see whether it is in-context, for text2speech task, you will want to check the emotion is proper or not. However, if we just use the general word to \"Performance\" to refer this, it will normally cause the confusion between the developers and researchers. So we stick to use this to refer to the model performance.</p> <p>However, for most of the accuracy performance, it does not like the latency, the measurement is diverse for a task and between different tasks. For example, you will want to evaluate whether the generated text is in-context from the knowledge, reasoning, tone perspective. So we need more freedom here. And automatically to evaluate the tasks from these different perspectives is also a challenging and underexplored area.</p> <p>So we decide to provide a customisable interface which will allow annotators to annotate the accuracy performance for the model under different measurement metrics. It can be evaluated by multiple annotators, and then the system will provide a summary report for the performance. You can also evaluate the overall performance for several rounds of conversation, and then generate a report for that.</p> <p>We will show how to do this evaluation annotation in tutorial section.</p> <p>You can track the evaluation process via our accuracy detail page, and then get a summary report after the evaluation annotation is done.</p> <ul> <li>Summary Report: Which will provide the average accuracy for each component under each measurement metrics within   the pipeline.    </li> <li> <p>Detail Report: Which will provide the detailed information and progress for each round of conversation annotation    </p> </li> <li> <p>Multi-Turn Conversation: You can group several conversation into a muti-turn conversation, and then annotate the   overall performance for that.   </p> </li> </ul>"},{"location":"#deployment-options","title":"Deployment Options","text":"<p>Because the whole design is modular and decoupled, so you can use our system in several ways:</p> <ul> <li>Trial on Cloud: We deploy the API end on https://openomni.ai4wa.com<ul> <li>You can use this as the API end</li> <li>Deploy the client side on your local machine to collect audio and video, and play the audio</li> <li>Deploy the Agent module on your local machine or anywhere with good computation power, can be a cloud server, HPC   center, etc</li> </ul> </li> <li>All in One Local Machine: You can deploy the whole system on your PC or laptop<ul> <li>You can use the camera, microphone, and speaker on your laptop</li> <li>You can use the local storage to store the data, and get the data transfer process to be \"None\"</li> <li>You can use the computation power on your laptop to run the Agent models (Which sometimes not that good)</li> </ul> </li> <li>Private Offline Network: You can deploy the whole system on a private network, different machines<ul> <li>Camera, Microphone, and Speaker can be one different machine within the same network</li> <li>API will be running on another machine, which in charge of manage the whole system</li> <li>Agent module can be running on a high speciation machine, which in charge of running the Agent models, or a local   high   performance GPU cluster</li> </ul> </li> <li>Your Cloud: This is similar to the Trial on Cloud, only difference is that you deploy the API end on your own   cloud server, and you can have more control over the system.</li> </ul> <p>Client side can be deployed to very cheap devices like Raspberry Pi with camera, microphone and speaker, like show in </p> <p>We have detailed instruction about how to deploy the system in different ways in Deployment.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this package useful, please consider citing our work:</p> <pre><code>@misc{sun2024openomnicollaborativeopensource,\n      title={OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents}, \n      author={Qiang Sun and Yuanyi Luo and Sirui Li and Wenxiao Zhang and Wei Liu},\n      year={2024},\n      eprint={2408.03047},\n      archivePrefix={arXiv},\n      primaryClass={cs.HC},\n      url={https://arxiv.org/abs/2408.03047}, \n}\n</code></pre>"},{"location":"Deployment/","title":"Deployment Guide","text":"<p>As we suggest in the introduction, we have four modes of deployment:</p> <ul> <li>Trial on Cloud</li> <li>All in One Local Machine</li> <li>Private Offline Deployment</li> <li>Your Cloud</li> </ul> <p>If you want to easily get start, you can use our deployed API, the link is https://openomni.ai4wa.com, to manage the tasks.</p> <p>If you want to test out the full setup locally, you can follow the guide in the <code>All in One Local Machine</code> section.</p> <p>If you are thinking about deploy it as a product, which is fully locally within a home network, addressing the privacy issue, you can follow the guide in the <code>Private Offline Deployment</code> section.</p> <p>If you are doing research with cluster of computing resources, or you want annotators to work on the same platform for a serious project, you can follow the guide in the <code>Your Cloud</code> section.</p>"},{"location":"Deployment/#modules","title":"Modules","text":"<p>We have three components in the stack to deploy:</p> <ul> <li>API</li> <li>Agent</li> <li>Client<ul> <li>Listener (Audio and Video)</li> <li>Responder (Audio)</li> </ul> </li> </ul>"},{"location":"Deployment/#api","title":"API","text":"<ul> <li>Required Resource<ul> <li>A server (If on cloud will require a Public IP)</li> <li>Minimum 1 CPU and 1 GB RAM</li> <li>Deployment method:<ul> <li>For cloud server: Docker + Docker Compose + Nginx</li> <li>For local server: Docker + Docker Compose</li> </ul> </li> </ul> </li> </ul>"},{"location":"Deployment/#agent","title":"Agent","text":"<ul> <li>Required Resource<ul> <li>Any high-end computational Nvidia GPU resources<ul> <li>Can be HPC Clusters</li> <li>Can work on demand, which means, you can spin the tasks when needed</li> <li>Can work on CPU as well, but the performance will be degraded</li> </ul> </li> <li>Minimum storage of 500 GB<ul> <li>This is required to store the models and the data, especially the LLM models</li> </ul> </li> <li>Python 3.8+</li> </ul> </li> </ul>"},{"location":"Deployment/#client","title":"Client","text":"<ul> <li>Required Resource:<ul> <li>Hardware:<ul> <li>Microphone: To gather the audio data</li> <li>Camera: To gather the video data</li> <li>Speaker: To play the audio data</li> <li>Minimum 2 GB RAM</li> <li>Minimum 1 CPU</li> <li>Minimum 32 GB storage</li> <li>It can be running on a laptop, or working with a Raspberry Pi</li> </ul> </li> <li>Python 3.8+</li> </ul> </li> </ul> <p>Something like this</p> <p></p>"},{"location":"Deployment/#storage-solution","title":"Storage solution","text":"<p>All the metadata will be communicated via the API, so here we need to think about how can we share the video and audio data between Agent/Client/API.</p> <p>We have four STORAGE_SOLUTION for this four different scenarios:</p> <ul> <li>api: audio and video data will be upload and download via api endpoint, this is for the trial on cloud.</li> <li>volume: all the files will be shared on the same machine via the docker volume and file system, so there is no   need to sync anything</li> <li>local: all the modules will be deployed on the same local network, but different machines, so we need to sync the   data between them, with rsync</li> <li>s3: API is on your cloud, Agent is anywhere, so we will use s3 to be the storage place for the data, to make sure   stable   and fast access.</li> </ul> <p>To switch between these four modes, all you need to do is to set the <code>STORAGE_SOLUTION</code> environment variable before start the API</p> <pre><code>export STORAGE_SOLUTION=api\n</code></pre>"},{"location":"Deployment/all-in-one-local-machine/","title":"All in One Local Machine","text":"<p>It will be what we have introduced in the Tutorial/setup Section.</p> <p>Shared Access of the Files (videos, images and audios)</p> <p>Under this mode, all the files will be shared on the same machine via the docker volume and file system, so there is no need to transfer the video and audio data between different machines, to ensure API/Agent/Client all have access to the files.</p> <p>You can check it from here: Tutorial/setup</p>"},{"location":"Deployment/private-offline-deployment/","title":"Private Offline Deployment","text":"<p>It is to offload the Agent/Client/API modules into different machines however within the same network, for potential use cases like privacy concerns, or to have a more robust system.</p> <p>One of the example deployment scenario for this is:</p> <ul> <li>Client: Raspberry Pi to gather the audio and video data</li> <li>API: A server to run the API, which can be a laptop or a desktop</li> <li>Agent: A PC with Nvidia GPU to run the Agent models</li> </ul> <p>Local Network File Sync</p> <p>To ensure the API/Agent/Client all have access to the files, we will need to sync the files between different machines.</p> <ul> <li>Client to API: Audio and Video data</li> <li>API to Agent: Audio and Video data</li> <li>Agent to API: Speech audio data</li> </ul> <p>As other deployment methods, we will first deploy the <code>API</code> module.</p>"},{"location":"Deployment/private-offline-deployment/#step-1-get-api-running","title":"Step 1: Get API running","text":"<p>Login to the machine your API will deploy on, and clone the repo:</p> <pre><code>git clone git@github.com:AI4WA/OpenOmniFramework.git\ncd ./OpenOmniFramework\ncd ./API\nexport STORAGE_SOLUTION=local # this is for local mode\n\n# Run it inside docker, this is the easiest way to get started\ndocker compose up\n</code></pre> <p>Get your private ip of this machine.</p> <p>For Mac:</p> <pre><code>ipconfig getifaddr en0\n</code></pre> <p>For Linux:</p> <pre><code>hostname -I\n</code></pre> <p>For Windows:</p> <pre><code>ipconfig\n</code></pre> <p>After this, you should be able to access the API at <code>http://&lt;private-ip&gt;:8000</code> for any device within the same network.</p>"},{"location":"Deployment/private-offline-deployment/#step-2-get-the-token","title":"Step 2: Get the token","text":"<p>Login to the API admin, go to <code>http://&lt;private-ip&gt;:8000/authtoken/tokenproxy/</code> and click <code>Add Token</code>.</p> <p></p>"},{"location":"Deployment/private-offline-deployment/#step-3-sync-the-files-between-different-machines","title":"Step 3: Sync the files between different machines","text":"<p>If you are a Linux or Mac for API module, then you can use <code>rsync</code> to sync the files between different machines.</p> <p>In this way, all you need to do is to start a new terminal and run the following command:</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\nsource venv/bin/activate\n\n# under this way, STORAGE_SOLUTION in API is local mode\n# sync the audio and video data to the API machine\npython3 storage.py --token your_token_from_step_2 --dest_dir api_machine_user@api_private_ip:/where/api/folder/is/Client/Listener/data --dest_password api_machine_password\n</code></pre> <p>If you are a Windows user, you can use the <code>api</code> mode storage solution to sync the files between different machines.</p> <p>All you need to do is in Step 1, before starting the API, you need to run the following command:</p> <pre><code>export API_STORAGE_MODE=api\n</code></pre> <p>And then within the <code>Listener</code> module, you can run the following command:</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\nsource venv/bin/activate\n\n# sync the audio and video data to the API machine\npython3 storage.py --token your_token_from_step_2 --api_domain http://&lt;private-ip&gt;:8000\n</code></pre> <p>However, this way will be a bit slower than the <code>rsync</code> way, but should not be noticeable for testing purposes.</p>"},{"location":"Deployment/private-offline-deployment/#step-4-collect-audio-and-video-data","title":"Step 4: Collect Audio and Video Data","text":"<p>Login to the machine your Client will deploy on, which should have the Camera, Microphone and Speaker, and clone the repo:</p> <pre><code># switch to a proper directory\ngit clone git@github.com:AI4WA/OpenOmniFramework.git\n</code></pre> <p>For the <code>Listener</code> part, you will need to run the following commands:</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\nexport DISPLAY=:0.0 # THIS IS SPECIFIC FOR RASPBERRY PI\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run video acquire\npython3 videos_acquire.py --token your_token_from_step_2 --api_domain http://&lt;private-ip&gt;:8000\n</code></pre> <p>You should be able to see something like this:</p> <p></p> <p>Then open a new terminal</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\nsource venv/bin/activate\n\n# run audio acquire\npython3 audios_acquire.py --token your_token_from_step_2 --track_cluster CLUSTER_GPT_4O_ETE_CONVERSATION  --api_domain http://&lt;private-ip&gt;:8000\n\n# you can change the cluster to the one your need\n</code></pre> <p>You will see something like this:</p> <p></p>"},{"location":"Deployment/private-offline-deployment/#step-5-run-agent-models","title":"Step 5: Run Agent models","text":"<p>Login to the machine your Agent will deploy on, and clone the repo:</p> <pre><code># switch to a proper directory\ngit clone git@github.com:AI4WA/OpenOmniFramework.git\n</code></pre> <p>Before you start the Agent, you will also need to first sort out the file sync between the API and Agent machine.</p> <p>Same as above, if you are a Linux or Mac user, you can use <code>rsync</code> to sync the files between different machines.</p> <pre><code>cd ./OpenOmniFramework\ncd ./Agent\n\npython3 -m venv venv\n\nsource venv/bin/activate\n\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run storage sync from API to Agent, both direction \n\npython3 storage.py --token your_token_from_step_2 --api_domain http://&lt;private-ip&gt;:8000 --dest_dir api_machine_user@api_private_ip:/where/api/folder/is/OpenOmniFramework/Agent/data --dest_password api_machine_password\n</code></pre> <p>And then you are free to run the Agent models.</p> <pre><code>cd ./OpenOmniFramework\ncd ./Agent\n\nsource venv/bin/activate\n\n# run the Agent models\n\npython3 main.py --token your_token_from_step_2 --api_domain http://&lt;private-ip&gt;:8000\n</code></pre>"},{"location":"Deployment/private-offline-deployment/#step-6-play-the-response","title":"Step 6: Play the response","text":"<p>The speech will be feed with the url, so it is fine, the complex logic is handled within the API side.</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Responder\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run the audio player\n\npython3 play_speech.py --token your_token\n</code></pre>"},{"location":"Deployment/trial-on-cloud/","title":"Trial On Cloud","text":"<p>In this one, the STORAGE_SOLUTION is <code>api</code>, which means the audio and video data will be uploaded and downloaded via the API endpoint.</p> <p>Step 0: Get a token</p> <p>Our deployed API is on https://openomni.ai4wa.com, you can use it to manage the tasks.</p> <p>Login with username <code>admin</code> and password <code>password</code>, do not change the password, as it is a shared account.</p> <p>Then you are free to create a new account for yourself.</p> <p>And then use your own account to create a Token.</p> <p></p> <p>So here all you need to do is deploy the <code>Client</code> and <code>Agent</code> part.</p>"},{"location":"Deployment/trial-on-cloud/#step-1-clone-the-repository","title":"Step 1: Clone the repository","text":"<pre><code># switch to a proper directory\ngit clone git@github.com:AI4WA/OpenOmniFramework.git\n</code></pre>"},{"location":"Deployment/trial-on-cloud/#step-2-get-data-sync","title":"Step 2: Get Data Sync","text":"<pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\n\n\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\npython3 storage.py --token your_token_from_step_0 --api_domain https://openomni.ai4wa.com\n</code></pre>"},{"location":"Deployment/trial-on-cloud/#step-3-collect-audio-and-video-data","title":"Step 3: Collect Audio and Video Data","text":"<pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\n\nsource venv/bin/activate\n\n\n# run video acquire\npython3 videos_acquire.py --token your_token_from_step_0 --api_domain https://openomni.ai4wa.com\n</code></pre> <p>You should be able to see something like this: </p> <p>Then open a new terminal</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run audio acquire\npython3 audios_acquire.py --token your_token_from_step_0 --track_cluster CLUSTER_GPT_4O_ETE_CONVERSATION  --api_domain https://openomni.ai4wa.com\n# you can change the cluster to the one your need\n</code></pre> <p>You will see something like this: </p> <p>If everything works, you should be able to check the newly create <code>Data Audios</code>, <code>Data Videos</code> and <code>Speech2Text</code> <code>Tasks</code> in API Admin page. Something like below:  </p>"},{"location":"Deployment/trial-on-cloud/#step-4-run-agent-models","title":"Step 4: Run Agent models","text":"<p>Now we need to start Agent module to consume the <code>Tasks</code>.</p> <p>Same as above, we will need to first run the storage sync.</p> <pre><code>cd ./OpenOmniFramework\ncd ./Agent\n\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\npython3 storage.py --token your_token_from_step_0 --api_domain https://openomni.ai4wa.com\n</code></pre> <p>Before we start the Agent module, there are some pre configurations we need to do.</p> <p>As provided functionalities within Agent modules support OpenAI call, HuggingFace call, and there is also our provided emotion detection module.</p> <p>We need to get them setup first.</p> <p>Setup OpenAI and HuggingFace Environment Variable</p> <p>Create a <code>.env</code> file in <code>./Agent</code> folder, and add the following content:</p> <pre><code>HF_TOKEN=Your_HuggingFace_Token\nOPENAI_API_KEY=Your_OpenAI_API_KEY\n</code></pre> <p>Otherwise, you can run</p> <pre><code>export HF_TOKEN=Your_HuggingFace_Token\nexport OPENAI_API_KEY=Your_OpenAI_API_KEY\n</code></pre> <p>For the model part, if you want to get our emotion detection model running, you will need to download the model from download link</p> <p>And put it in the folder: <code>./Agent/data/models/emotion_detection/model_data</code>. It should be like this</p> <p></p> <p>Then you should be ready to run the Agent module.</p> <pre><code># run the Agent module\npython3 main.py --token your_token_from_step_3\n</code></pre> <p>You can also skip the steps to install the requirements, directly run the Agent module with docker.</p> <pre><code>TOKEN=XXX docker compose up\n</code></pre> <p>This will allow you to utilise the GPU resources on your machine if you have one.</p> <p></p> <p>Until now, you will have the client side to feed the video/audio data to the API, and the Agent module to consume the data.</p>"},{"location":"Deployment/trial-on-cloud/#step-5-play-speech-audio-in-client-side","title":"Step 5: Play speech audio in client side","text":"<pre><code>cd ./OpenOmniFramework\ncd ./Client/Responder\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run the audio player\n\npython3 play_speech.py --token your_token_from_step_3\n</code></pre> <p>You will see something like this:</p> <p></p> <p>Until now, you should have the whole pipeline running on your local machine.</p> <p>You should see new tasks created as expected in the <code>Tasks</code> page in the API admin page. As shown below:</p> <p></p> <p>And in the Detailed Latency Benchmark page, you should be able to see the latency of each round of conversation.</p> <p></p>"},{"location":"Deployment/your-cloud/","title":"Your Cloud","text":"<p>This will be similar to the Trail on Cloud section, only differences is that the API end is on your cloud server.</p> <p>Under this mode, your storage solution will be s3, you will need to</p> <ul> <li>create a s3 bucket, and replace it to the S3_BUCKET setting in Agent/API/Client</li> <li>create an access key and secret key, set it properly for both Agent, API and Client, refer to AWS documentation for more   details</li> </ul> <p>After this, the first step you will need to do is deploying it to your cloud server.</p> <p>We will assume it is a Linux Machine.</p>"},{"location":"Deployment/your-cloud/#step-1-deploy-the-api-on-cloud","title":"Step 1: Deploy the API on Cloud","text":"<p>You will need to have a cloud server, it can be AWS EC2, Azure Compute Engine or any VPS server you can access. It will need to have a public IP address. The demonstration about how to deploy it to a cloud server is in our CI/CD process.</p> <p>You will need to access the server and install <code>docker</code> first. Test out the command <code>docker</code> and <code>docker compose</code> to verify the installation.</p> <p>And then you can fork our repo, and replace the IP in the <code>.github/workflows/deploy.yml</code> file with the public IP of your server, also remember to set the <code>Actions -&gt; Secrets</code>, add a secret with the name <code>SERVER_PASSWORD</code> and the value as your server password.</p> <p>In this way, you can continuously deploy the API to your server when code changes, and merge to the <code>develop</code> branch.</p> <p>If you want to manually to do so, it is also simple, just follow the steps in the <code>deploy.yml</code> file. Pull the code to your server and mainly run the command in last step:</p> <pre><code>cd /root \nrm -rf omni\nmkdir omni\ntar xopf omni.tar -C omni\ncd /root/omni/API\nexport STORAGE_SOLUTION=s3\ndocker compose -f docker-compose.yml down\ndocker compose -f docker-compose.yml up --build -d\n</code></pre> <p>Configuration of Nginx will be like this:</p> <pre><code>server {\n    server_name openomni.ai4wa.com; # replace with your domain\n    client_max_body_size 100M;\n    location / {\n        proxy_pass http://localhost:8000;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Then run</p> <pre><code>sudo service nginx restart\n</code></pre> <p>Add a DNS A record for this sever for your domain, and you should be able to access the API at <code>http://your.domain.com</code>.</p> <p>Then you can follow the steps in the <code>Trail on Cloud</code> section to get the Agent and Client running.</p>"},{"location":"Modules/","title":"Table of Content","text":"<p>We will describe how each module is designed, so you can understand how it works further for future development.</p> <ul> <li>Client:<ul> <li>Listener</li> <li>Responder</li> </ul> </li> <li>API</li> <li>Agent</li> </ul> <p>If you want to check the details, you can either check the <code>Source</code> section or our GitHub repo code.</p>"},{"location":"Modules/Agent/","title":"Agent","text":"<p>The Agent component is the core of the system, which will be in charge of:</p> <ul> <li>Running the ML or AI models distributed in the system<ul> <li>Running the models which require intensive computation</li> <li>LLM models</li> <li>Text2Speech models</li> <li>Emotion Recognition models</li> <li>etc.</li> </ul> </li> </ul> <p>It is writen in Python, and it is a pretty standard Python project.</p> <p>Each different task will have a subfolder within the <code>modules</code> folder</p>"},{"location":"Modules/Agent/#latency-logger","title":"Latency Logger","text":"<p>Key thing to notice is that we create two classes to log the time point and duration to profile the latency performance of the models.</p> <ul> <li><code>Agent/utils/time_logger.py</code>: log time point</li> <li><code>Agent/utils/time_tracker.py</code>: track duration</li> </ul>"},{"location":"Modules/Agent/#docker-setup","title":"Docker setup","text":"<p>We also setup the docker for the Agent component, which is in the <code>Dockerfile</code> and <code>docker-compose.yml</code> file.</p>"},{"location":"Modules/Agent/#storage-solution","title":"Storage solution","text":"<p>How we handle the different storage solution is inside the <code>storage.py</code> file.</p>"},{"location":"Modules/Agent/#data","title":"Data","text":"<p>As we mentioned in the introduction, models will be need to be downloaded to the <code>data/models</code> folder, it is normally automatically.</p> <p>Unless you want to run our emotion detection model, if you want to do that, refer to our introduction page.</p>"},{"location":"Modules/API/main/","title":"API","text":"<p>API will provide the central logic control and orchestration for the whole system. It is written in Django and Django Rest Framework. The database is PostgresSQL.</p> <p>The apps in the API are [continue to be developed]:</p> <ul> <li>authenticate:<ul> <li>User authentication</li> <li>JWT token generation</li> <li>API token generation</li> </ul> </li> <li>hardware:<ul> <li>Hardware management</li> <li>Store audio and video data</li> <li>Store the artifacts of the pipeline</li> </ul> </li> <li>llm:<ul> <li>Manage the configuration of the LLM models</li> </ul> </li> <li>orchestrator:<ul> <li>Manage the pipeline</li> <li>Queue the tasks</li> <li>Manage the pipeline hooks</li> </ul> </li> </ul> <p>Currently, it will provide the following functionalities:</p> <ul> <li>admin interface: http://localhost:8000/</li> <li>API docs: http://localhost:8000/redoc</li> </ul> <p>If you want to add any new functionalities, it is quite easy, you just need to know how to use Django.</p>"},{"location":"Modules/API/main/#data-storage","title":"Data Storage","text":"<ul> <li>We have a relational database, which is PostgresSQL.</li> <li>For the audio and video data, we will store them in the file system.</li> <li>We also include the Neo4j for future development of GraphRAG.</li> </ul>"},{"location":"Modules/Client/Listener/","title":"Listener","text":"<p>This is to collect the audio and video data from any devices with a camera and microphone. It can be your laptop, it also can be your Raspberry Pi 4.</p> <p>Collect the video is easy, just keep in the background, and record the video when needed, upload it to the API.</p> <p>However, collect the audio is a bit tricky, which can be further enhanced.</p>"},{"location":"Modules/Client/Listener/#audio","title":"Audio","text":"<p>Our solution for the audio is using the whisper model to detect when user stop talking, your can specific the energy threshold or timeout milliseconds to determine when to stop and save this round of sound.</p> <p>This will get the API receive the audio in a \"conversation\" way, speaker stop, Agent process and act, then speaker speak again.</p> <p>However, there are several situations are limited by current solution:</p> <ul> <li>multiple speakers: if we add another module to detect the speaker, then the latency will increase again</li> <li>interrupt: if the speaker interrupt the AI, then the AI should stop and listen to the speaker again, or interrupt the   speaker, which GPT-4o is capable of doing</li> <li>streaming: on the other end, this means the audio data should be streamed to the API, which is not supported by the   current solution</li> </ul> <p>But it does can handle the basic conversation for research purpose.</p> <p>There are several parameters you can specify when you start the audio listener:</p> <ul> <li><code>--api_domain</code>: the API domain, default is <code>http://localhost:8000</code>, which is within the full local setup</li> <li><code>--token</code>: the token you get from the API side</li> <li><code>--home_id</code>: If you use cloud mode, you can have multiple homes to upload video and audio data, as one of the most   common user case for this could be home intelligent assistant. The home do not limit to an actual home, can be a   hospital room, etc.</li> <li><code>--energy_threshold</code>: the energy threshold to determine when to stop the audio recording, default is <code>5000</code></li> <li><code>--timeout</code>: the timeout milliseconds to determine when to stop the audio recording, default is <code>30000</code> in   milliseconds</li> <li><code>default_microphone</code>: which microphone to use if there are multiple microphones, default is <code>pulse</code></li> <li><code>track_cluster</code>: the cluster you want to track, default is <code>CLUSTER_GPT_4O_ETE_CONVERSATION</code></li> </ul>"},{"location":"Modules/Client/Listener/#video","title":"Video","text":"<p>Video also in theory should be streaming to a model, however, currently most models do not have the capability to take streaming input.</p> <p>At the same time, most model is taking the images to the model.</p> <p>So how we design it now is:</p> <ul> <li>every 20 seconds or duration you specify, we will record a video for reference purpose.</li> <li>every second, take a frame, save an image, this will be the main input for the model.</li> </ul> <p>This is not the best solution, but it is the most practical solution for now.</p> <p>There are several parameters you can specify when you start the video listener:</p> <ul> <li><code>--api_domain</code>: the API domain, default is <code>http://localhost:8000</code>, which is within the full local setup</li> <li><code>--token</code>: the token you get from the API side</li> <li><code>--home_id</code>: If you use cloud mode, you can have multiple homes to upload video and audio data, as one of the most   common user case for this could be home intelligent assistant. The home do not limit to an actual home, can be a   hospital room, etc.</li> </ul> <p>Then that's all, other setting if you want to customize, you can PR or change it by your own.</p>"},{"location":"Modules/Client/Listener/#audiovideoimage-file-sync","title":"Audio/Video/Image File Sync","text":"<p>We have described the STORAGE_SOLUTION in our Deployment Options</p> <p>The fastest way is definitely on the same machine for all modules, which actually is not practical in production. So next option will be local network or cloud.</p> <ul> <li>local network, sync data to a center sever within home network</li> <li>cloud, upload to s3 or other cloud storage, then trigger the serverless function on cloud to download the file on a   EFS, then Agent and API should both mount to the EFS, this will reduce the </li> </ul>"},{"location":"Modules/Client/Responder/","title":"Responder","text":"<p>All it does is pulling the API end to figure out whether there is any audio have not been played, if not, use the url to play it.</p> <p>So the code is very simple and straight forward, it is just a loop to check the API, and play the audio.</p> <p>code have a <code>play_speech.py</code>, all other files are some extent utilities functions.</p> <p>For the hardware part, it only requires a speaker, so it can be running on a laptop, or working with a Raspberry Pi.</p>"},{"location":"Source/","title":"Code sources","text":"<ul> <li>Client<ul> <li>Listener<ul> <li>audios_acquire.py</li> <li>videos_acquire.py</li> <li>storage.py</li> <li>api.py</li> <li>constants.py</li> <li>utils.py</li> <li>mockup</li> </ul> </li> <li>Responder<ul> <li>play_speech.py</li> <li>api.py</li> <li>constants.py</li> <li>utils.py</li> </ul> </li> </ul> </li> <li> <p>API</p> <ul> <li>manage.md</li> <li>api<ul> <li>asgi.md</li> <li>settings.md</li> <li>urls.md</li> <li>wsgi.md</li> </ul> </li> <li>authenticate<ul> <li>admin.md</li> <li>apps.md</li> <li>models.md</li> <li>serializers.md</li> <li>tests.md</li> <li>urls.md</li> <li>views.md</li> <li>migrations<ul> <li>0001_init.md</li> </ul> </li> <li>utils<ul> <li>fire_and_forget.md</li> <li>get_logger.md</li> <li>timer.md</li> </ul> </li> </ul> </li> <li> <p>hardware</p> <ul> <li>admin.md</li> <li>apps.md</li> <li>forms.md</li> <li>models.md</li> <li>serializers.md</li> <li>signals.md</li> <li>tests.md</li> <li>urls.md</li> <li>views.md</li> </ul> </li> <li> <p>llm</p> <ul> <li>admin.md</li> <li>apps.md</li> <li>models.md</li> <li>serializers.md</li> <li>tests.md</li> <li>urls.md</li> <li>views.md</li> <li>llm<ul> <li>config.md</li> </ul> </li> <li>management<ul> <li>commands<ul> <li>check_models.md</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>orchestrator</p> <ul> <li>admin.md</li> <li>apps.md</li> <li>models.md</li> <li>serializers.md</li> <li>tests.md</li> <li>urls.md</li> <li>views.md</li> <li>chain<ul> <li>clusters.md</li> <li>completed_emotion_detection.md</li> <li>completed_hf_llm.md</li> <li>completed_openai_gpt_35.md</li> <li>completed_openai_gpt_4o_text_and_image.md</li> <li>completed_openai_gpt_4o_text_only.md</li> <li>completed_openai_speech2text.md</li> <li>completed_openai_text2speech.md</li> <li>completed_quantization_llm.md</li> <li>completed_rag.md</li> <li>completed_speech2text.md</li> <li>completed_task.md</li> <li>completed_text2speech.md</li> <li>created_data_text.md</li> <li>manager.md</li> <li>models.md</li> <li>signals.md</li> <li>utils.md</li> </ul> </li> <li>metrics<ul> <li>accuracy_benchmark.md</li> <li>latency_benchmark.md</li> <li>utils.md</li> </ul> </li> </ul> </li> <li> <p>AI</p> <ul> <li>setup.md<ul> <li>storage.md</li> <li>Models<ul> <li>parameters.md</li> <li>results.md</li> <li>task.md</li> <li>track_type.md</li> </ul> </li> <li>Modules<ul> <li>EmotionDetection<ul> <li>features_extraction.md</li> <li>handler.md</li> <li>sentiment.md</li> </ul> </li> <li>GeneralML<ul> <li>handler.md</li> <li>ml_models.md</li> </ul> </li> <li>HFLLM<ul> <li>handler.md</li> </ul> </li> <li>OpenAI<ul> <li>handler.md</li> </ul> </li> <li>QuantizationLLM<ul> <li>adaptor_worker.md</li> <li>handler.md</li> <li>models.md</li> </ul> </li> <li>RAG<ul> <li>handler.md</li> <li>neo4j_connector.md</li> <li>postgresql_connector.md</li> </ul> </li> <li>SpeechToText<ul> <li>speech2text.md</li> </ul> </li> <li>TextToSpeech<ul> <li>text2speech.md</li> </ul> </li> </ul> </li> <li>Utils<ul> <li>api.md</li> <li>aws.md</li> <li>constants.md</li> <li>get_logger.md</li> <li>time_logger.md</li> <li>time_tracker.md</li> <li>timer.md</li> <li>Storage<ul> <li>api_sync_handler.md</li> <li>local_sync_handler.md</li> <li>s3_sync_handler.md</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Sources/API/manage/","title":"manage.md","text":"<p>Django's command-line utility for administrative tasks.</p>"},{"location":"Sources/API/manage/#API.manage.main","title":"<code>main()</code>","text":"<p>Run administrative tasks.</p> Source code in <code>API/manage.py</code> <pre><code>def main():\n    \"\"\"Run administrative tasks.\"\"\"\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"api.settings\")\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc\n    execute_from_command_line(sys.argv)\n</code></pre>"},{"location":"Sources/API/api/asgi/","title":"ASGI","text":"<p>ASGI config for api project.</p> <p>It exposes the ASGI callable as a module-level variable named <code>application</code>.</p> <p>For more information on this file, see https://docs.djangoproject.com/en/5.0/howto/deployment/asgi/</p>"},{"location":"Sources/API/api/settings/","title":"Settings","text":"<p>Django settings for api project.</p> <p>Generated by 'django-admin startproject' using Django 5.0.2.</p> <p>For more information on this file, see https://docs.djangoproject.com/en/5.0/topics/settings/</p> <p>For the full list of settings and their values, see https://docs.djangoproject.com/en/5.0/ref/settings/</p>"},{"location":"Sources/API/api/urls/","title":"URLs","text":"<p>URL configuration for api project.</p> <p>The <code>urlpatterns</code> list routes URLs to views. For more information please see:     https://docs.djangoproject.com/en/5.0/topics/http/urls/ Examples: Function views     1. Add an import:  from my_app import views     2. Add a URL to urlpatterns:  path('', views.home, name='home') Class-based views     1. Add an import:  from other_app.views import Home     2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home') Including another URLconf     1. Import the include() function: from django.urls import include, path     2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))</p>"},{"location":"Sources/API/api/wsgi/","title":"WSGI","text":"<p>WSGI config for api project.</p> <p>It exposes the WSGI callable as a module-level variable named <code>application</code>.</p> <p>For more information on this file, see https://docs.djangoproject.com/en/5.0/howto/deployment/wsgi/</p>"},{"location":"Sources/API/authenticate/admin/","title":"Admin","text":""},{"location":"Sources/API/authenticate/apps/","title":"Apps","text":""},{"location":"Sources/API/authenticate/models/","title":"Models","text":""},{"location":"Sources/API/authenticate/serializers/","title":"Serializers","text":""},{"location":"Sources/API/authenticate/tests/","title":"Tests","text":""},{"location":"Sources/API/authenticate/urls/","title":"URLs","text":""},{"location":"Sources/API/authenticate/views/","title":"Views","text":""},{"location":"Sources/API/authenticate/views/#API.authenticate.views.APITokenObtainPairView","title":"<code>APITokenObtainPairView</code>","text":"<p>               Bases: <code>TokenObtainPairView</code></p> Source code in <code>API/authenticate/views.py</code> <pre><code>class APITokenObtainPairView(TokenObtainPairView):\n    serializer_class = APITokenObtainPairSerializer\n\n    @swagger_auto_schema(\n        operation_description=\"Obtain JSON Web Token pair for user\",\n        responses={200: APIReturnTokenSerializer},\n    )\n    def post(self, request, *args, **kwargs):\n        \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n        return super().post(request, *args, **kwargs)\n</code></pre>"},{"location":"Sources/API/authenticate/views/#API.authenticate.views.APITokenObtainPairView.post","title":"<code>post(request, *args, **kwargs)</code>","text":"<p>Override the post method to add custom swagger documentation.</p> Source code in <code>API/authenticate/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_description=\"Obtain JSON Web Token pair for user\",\n    responses={200: APIReturnTokenSerializer},\n)\ndef post(self, request, *args, **kwargs):\n    \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n    return super().post(request, *args, **kwargs)\n</code></pre>"},{"location":"Sources/API/authenticate/migrations/0001_init/","title":"0001 init","text":""},{"location":"Sources/API/authenticate/utils/fire_and_forget/","title":"FireAndForget","text":""},{"location":"Sources/API/authenticate/utils/fire_and_forget/#API.authenticate.utils.fire_and_forget.fire_and_forget","title":"<code>fire_and_forget(f)</code>","text":"<p>run it and forget it</p> Source code in <code>API/authenticate/utils/fire_and_forget.py</code> <pre><code>def fire_and_forget(f):\n    \"\"\"run it and forget it\"\"\"\n\n    def wrapped(*args, **kwargs):\n        loop = asyncio.new_event_loop()\n        loop.run_in_executor(None, f, *args, *kwargs)\n        loop.close()\n\n    return wrapped\n</code></pre>"},{"location":"Sources/API/authenticate/utils/get_logger/","title":"GetLogger","text":""},{"location":"Sources/API/authenticate/utils/timer/","title":"Timer","text":""},{"location":"Sources/API/authenticate/utils/timer/#API.authenticate.utils.timer.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>API/authenticate/utils/timer.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Parameters\n        ----------\n        logger: Logger\n            logger to write the logs\n        message: str\n            message to log, like start xxx\n        \"\"\"\n        self.message = message\n        self.logger = logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enter to start write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(self, context, value, traceback):\n        \"\"\"\n        context exit will write this\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/API/authenticate/utils/timer/#API.authenticate.utils.timer.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enter to start write this</p> Source code in <code>API/authenticate/utils/timer.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enter to start write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"Sources/API/authenticate/utils/timer/#API.authenticate.utils.timer.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> Source code in <code>API/authenticate/utils/timer.py</code> <pre><code>def __exit__(self, context, value, traceback):\n    \"\"\"\n    context exit will write this\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/API/authenticate/utils/timer/#API.authenticate.utils.timer.timer.__init__","title":"<code>__init__(logger, message)</code>","text":"<p>init the timer</p>"},{"location":"Sources/API/authenticate/utils/timer/#API.authenticate.utils.timer.timer.__init__--parameters","title":"Parameters","text":"<p>logger: Logger     logger to write the logs message: str     message to log, like start xxx</p> Source code in <code>API/authenticate/utils/timer.py</code> <pre><code>def __init__(self, logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Parameters\n    ----------\n    logger: Logger\n        logger to write the logs\n    message: str\n        message to log, like start xxx\n    \"\"\"\n    self.message = message\n    self.logger = logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"},{"location":"Sources/API/hardware/admin/","title":"Admin","text":""},{"location":"Sources/API/hardware/admin/#API.hardware.admin.DataMultiModalConversationFKAdmin","title":"<code>DataMultiModalConversationFKAdmin</code>","text":"<p>               Bases: <code>ImportExportModelAdmin</code></p> <p>All the obj above will be self.multi_modal_conversation</p> Source code in <code>API/hardware/admin.py</code> <pre><code>class DataMultiModalConversationFKAdmin(ImportExportModelAdmin):\n    \"\"\"\n    All the obj above will be self.multi_modal_conversation\n    \"\"\"\n\n    def audio__time_range(self, obj):\n        # format it \"%Y-%m-%d %H:%M:%S\"\n        if obj.multi_modal_conversation.audio is None:\n            return \"No Audio\"\n        start_time_str = obj.multi_modal_conversation.audio.start_time.strftime(\n            \"%Y-%m-%d %H:%M:%S\"\n        )\n        end_time_str = obj.multi_modal_conversation.audio.end_time.strftime(\n            \"%Y-%m-%d %H:%M:%S\"\n        )\n        return f\"{start_time_str} - {end_time_str}\"\n\n    audio__time_range.short_description = \"Time Range: Audio\"\n\n    def video__time_range(self, obj):\n        if len(obj.multi_modal_conversation.video.all()) == 0:\n            return \"No Video\"\n        videos = obj.multi_modal_conversation.video.all().order_by(\"start_time\")\n        # get the first video start time and the last video end time\n        start_time_str = videos.first().start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        end_time_str = videos.last().end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"{start_time_str} - {end_time_str}\"\n\n    video__time_range.short_description = \"Time Range: Video\"\n\n    def play_audio(self, obj):\n        if obj.multi_modal_conversation.audio is None:\n            return \"No Audio\"\n\n        return mark_safe(\n            f'&lt;audio controls name=\"media\"&gt;'\n            f'&lt;source src=\"{obj.multi_modal_conversation.audio.url()}\" type=\"audio/mpeg\"&gt;&lt;/audio&gt;'\n        )\n\n    def play_video(self, obj):\n        if (\n            obj.multi_modal_conversation.video is None\n            or len(obj.multi_modal_conversation.video.all()) == 0\n        ):\n            return \"No Video\"\n        return mark_safe(\n            f'&lt;video width=\"320\" height=\"240\" controls&gt;'\n            f'&lt;source src=\"{obj.multi_modal_conversation.video_url()}\" type=\"video/mp4\"&gt;&lt;/video&gt;'\n        )\n\n    def play_res_speech(self, obj):\n        if obj.multi_modal_conversation.res_speech is None:\n            return \"No Response Speech\"\n        return mark_safe(\n            f'&lt;audio controls name=\"media\"&gt;'\n            f'&lt;source src=\"{obj.multi_modal_conversation.res_speech.url()}\" type=\"audio/mpeg\"&gt;&lt;/audio&gt;'\n        )\n\n    def speech_to_text(self, obj):\n        if obj.multi_modal_conversation.text is None:\n            return \"No Text\"\n        return obj.multi_modal_conversation.text.text\n\n    def response_text(self, obj):\n        if obj.multi_modal_conversation.res_text is None:\n            return \"No Response Text\"\n        return obj.multi_modal_conversation.res_text.text\n\n    def annotation_records(self, obj):\n        annotations = obj.annotations\n        if not annotations:\n            return \"No Annotations\"\n        \"\"\"\n        Get this presentation into a html like this:\n\n        User: {username}\n        Annotation Overall: {annotation_overall}\n        Annotation Text Modality: {annotation_text_modality}\n        Annotation Audio Modality: {annotation_audio_modality}\n        ----\n        User: {username}\n        ....\n\n        \"\"\"\n\n        return_html = \"&lt;div&gt;\"\n        return_html += f\"&lt;h5&gt;Total Annotator: {len(annotations.items())} &lt;/h5&gt;\"\n        return_html += \"&lt;hr&gt;\"\n        for user_id, annotation in annotations.items():\n            user = User.objects.get(pk=user_id)\n            return_html += f\"&lt;h6&gt;User: {user.username}&lt;/h6&gt;\"\n            return_html += \"&lt;ul&gt;\"\n            for annotation_key, annotation_value in annotation.items():\n                return_html += f\"&lt;li&gt;{annotation_key}: {annotation_value}&lt;/li&gt;\"\n            return_html += \"&lt;/ul&gt;\"\n            return_html += \"&lt;hr&gt;\"\n        return_html += \"&lt;/div&gt;\"\n        return mark_safe(return_html)\n\n    list_display = (\n        \"id\",\n        \"audio__time_range\",\n        \"video__time_range\",\n    )\n    exclude = (\n        \"audio\",\n        \"video\",\n        \"text\",\n        \"annotations\",\n    )\n    search_fields = (\n        \"text__text\",\n        \"res_text__text\",\n        \"track_id\",\n        \"text\",\n    )\n    readonly_fields = (\n        # \"track_id\",\n        \"play_audio\",\n        \"audio__time_range\",\n        \"speech_to_text\",\n        \"play_video\",\n        \"video__time_range\",\n        \"response_text\",\n        \"play_res_speech\",\n        \"created_at\",\n        \"updated_at\",\n        \"annotation_records\",\n    )\n    list_filter = (\"created_at\", ClusterFilter)\n\n    change_form_template = \"admin/hardware/conversation/change_form.html\"\n\n    def response_change(self, request, obj):\n        if \"_saveandnext\" in request.POST:\n            next_obj = self.get_next_obj(obj)\n            if next_obj:\n                return HttpResponseRedirect(\n                    reverse(\n                        \"admin:%s_%s_change\"\n                        % (\n                            obj._meta.app_label,\n                            obj._meta.model_name,\n                        ),\n                        args=[next_obj.pk],\n                    )\n                )\n        return super().response_change(request, obj)\n\n    def get_next_obj(self, obj):\n        # Define your logic to get the next object\n        # use self model to get the next object\n        obj_model = obj.__class__\n        next_obj = obj_model.objects.filter(pk__gt=obj.pk).order_by(\"pk\").first()\n        return next_obj\n\n    def change_view(self, request, object_id, form_url=\"\", extra_context=None):\n        if extra_context is None:\n            extra_context = {}\n\n        extra_context[\"additional_save_buttons\"] = [\n            {\"name\": \"_saveandnext\", \"value\": \"Save and Next\"}\n        ]\n\n        return super().change_view(request, object_id, form_url, extra_context)\n\n    def get_form(self, request, *args, **kwargs):\n        form = super().get_form(request, *args, **kwargs)\n        form.current_user = request.user\n        return form\n\n    def save_model(self, request, obj, form, change):\n        annotation_data = {}\n        for key, value in form.cleaned_data.items():\n            if key.startswith(\"annotation_\"):\n                annotation_data[key] = value\n\n        if not obj.annotations:\n            obj.annotations = {}\n        current_annotations = obj.annotations.get(request.user.id, {})\n        obj.annotations[request.user.id] = {\n            **annotation_data,\n            **current_annotations,\n        }\n\n        super().save_model(request, obj, form, change)\n</code></pre>"},{"location":"Sources/API/hardware/apps/","title":"Apps","text":""},{"location":"Sources/API/hardware/forms/","title":"Forms","text":""},{"location":"Sources/API/hardware/models/","title":"Models","text":""},{"location":"Sources/API/hardware/models/#API.hardware.models.DataAudio","title":"<code>DataAudio</code>","text":"<p>               Bases: <code>Model</code></p> <p>Link to home and hardware device, and the audio data will be stored in the database It will be created by the endpoint from client side when audio data is acquired</p> Source code in <code>API/hardware/models.py</code> <pre><code>class DataAudio(models.Model):\n    \"\"\"\n    Link to home and hardware device, and the audio data will be stored in the database\n    It will be created by the endpoint from client side when audio data is acquired\n    \"\"\"\n\n    home = models.ForeignKey(\n        Home, on_delete=models.CASCADE, related_name=\"audio\", null=True, blank=True\n    )\n    hardware_device_mac_address = models.CharField(\n        max_length=100,\n        help_text=\"The mac address of the hardware device\",\n        null=True,\n        blank=True,\n    )\n    uid = models.CharField(\n        max_length=100,\n        help_text=\"the uid of the audio acquire session, can be treated as scenario id\",\n    )\n    sequence_index = models.IntegerField(help_text=\"The sequence index of the audio\")\n    audio_file = models.CharField(max_length=100, help_text=\"The audio file\")\n    start_time = models.DateTimeField(help_text=\"The start time of the audio\")\n    end_time = models.DateTimeField(help_text=\"The end time of the audio\")\n\n    created_at = models.DateTimeField(\n        auto_now_add=True, help_text=\"The created time of the audio\"\n    )\n    updated_at = models.DateTimeField(\n        auto_now=True, help_text=\"The updated time of the audio\"\n    )\n    track_id = models.CharField(\n        max_length=100,\n        help_text=\"The track id of the multimodal conversation\",\n        null=True,\n        blank=True,\n    )\n\n    @classmethod\n    def create_obj(\n        cls,\n        home: Home,\n        uid: str,\n        hardware_device_mac_address: str,\n        sequence_index: int,\n        audio_file: str,\n        start_time: datetime,\n        end_time: datetime,\n        track_id: str = None,\n    ):\n        \"\"\"\n        Create an audio data object\n        \"\"\"\n        return cls.objects.create(\n            home=home,\n            hardware_device_mac_address=hardware_device_mac_address,\n            uid=uid,\n            sequence_index=sequence_index,\n            audio_file=audio_file,\n            start_time=start_time,\n            end_time=end_time,\n            track_id=track_id,\n        )\n\n    def __str__(self):\n        return f\"{self.uid} - {self.audio_file}\"\n\n    def url(self):\n        \"\"\"\n        get the file, and create media url\n        Returns:\n\n        \"\"\"\n        return f\"/hardware/client_audio/{self.id}\"\n\n    class Meta:\n        verbose_name = \"Data Audio\"\n        verbose_name_plural = \"Data Audios\"\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.DataAudio.create_obj","title":"<code>create_obj(home, uid, hardware_device_mac_address, sequence_index, audio_file, start_time, end_time, track_id=None)</code>  <code>classmethod</code>","text":"<p>Create an audio data object</p> Source code in <code>API/hardware/models.py</code> <pre><code>@classmethod\ndef create_obj(\n    cls,\n    home: Home,\n    uid: str,\n    hardware_device_mac_address: str,\n    sequence_index: int,\n    audio_file: str,\n    start_time: datetime,\n    end_time: datetime,\n    track_id: str = None,\n):\n    \"\"\"\n    Create an audio data object\n    \"\"\"\n    return cls.objects.create(\n        home=home,\n        hardware_device_mac_address=hardware_device_mac_address,\n        uid=uid,\n        sequence_index=sequence_index,\n        audio_file=audio_file,\n        start_time=start_time,\n        end_time=end_time,\n        track_id=track_id,\n    )\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.DataAudio.url","title":"<code>url()</code>","text":"<p>get the file, and create media url Returns:</p> Source code in <code>API/hardware/models.py</code> <pre><code>def url(self):\n    \"\"\"\n    get the file, and create media url\n    Returns:\n\n    \"\"\"\n    return f\"/hardware/client_audio/{self.id}\"\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.DataMultiModalConversation","title":"<code>DataMultiModalConversation</code>","text":"<p>               Bases: <code>Model</code></p> <p>It will be created when a audio is created Then video will be added when emotion detection is triggered, or other task require video Text will be added when speech2text is done ResText will be added when the text is processed by the language model ResSpeech will be added when the text is processed by the text2speech</p> Source code in <code>API/hardware/models.py</code> <pre><code>class DataMultiModalConversation(models.Model):\n    \"\"\"\n    It will be created when a audio is created\n    Then video will be added when emotion detection is triggered, or other task require video\n    Text will be added when speech2text is done\n    ResText will be added when the text is processed by the language model\n    ResSpeech will be added when the text is processed by the text2speech\n    \"\"\"\n\n    audio = models.OneToOneField(\n        DataAudio,\n        on_delete=models.SET_NULL,\n        related_name=\"multi_modal_conversation\",\n        null=True,\n        blank=True,\n    )\n    # video should be an array field\n    video = models.ManyToManyField(\n        DataVideo, related_name=\"multi_modal_conversation\", blank=True\n    )\n    text = models.OneToOneField(\n        DataText,\n        on_delete=models.SET_NULL,\n        related_name=\"multi_modal_conversation\",\n        null=True,\n        blank=True,\n    )\n\n    res_text = models.OneToOneField(\n        ResText,\n        on_delete=models.SET_NULL,\n        related_name=\"multi_modal_conversation\",\n        null=True,\n        blank=True,\n    )\n    res_speech = models.OneToOneField(\n        ResSpeech,\n        on_delete=models.SET_NULL,\n        related_name=\"multi_modal_conversation\",\n        null=True,\n        blank=True,\n    )\n    created_at = models.DateTimeField(\n        auto_now_add=True, help_text=\"The created time of the multi-modal conversation\"\n    )\n    updated_at = models.DateTimeField(\n        auto_now=True, help_text=\"The updated time of the multi-modal conversation\"\n    )\n\n    track_id = models.CharField(\n        max_length=100,\n        help_text=\"The track id of the multimodal conversation\",\n        null=True,\n        blank=True,\n    )\n    annotations = models.JSONField(\n        help_text=\"The annotations of the emotion detection\",\n        null=True,\n        blank=True,\n        default=dict,\n    )\n\n    multi_turns_annotations = models.JSONField(\n        help_text=\"The annotations of the multi-turns\",\n        null=True,\n        blank=True,\n        default=dict,\n    )\n    tags = TaggableManager(blank=True)\n\n    def __str__(self):\n        return f\"{self.id}\"\n\n    def video_url(self):\n        if len(self.video.all()) == 0:\n            return \"No Video\"\n        return f\"/hardware/client_video/{self.id}\"\n\n    class Meta:\n        verbose_name = \"Conversation\"\n        verbose_name_plural = \"Conversations\"\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.DataText","title":"<code>DataText</code>","text":"<p>               Bases: <code>Model</code></p> <p>The text data will be stored in the database It will be created after speech2text is done</p> Source code in <code>API/hardware/models.py</code> <pre><code>class DataText(models.Model):\n    \"\"\"\n    The text data will be stored in the database\n    It will be created after speech2text is done\n    \"\"\"\n\n    # foreign key to the audio\n    audio = models.ForeignKey(\n        DataAudio,\n        on_delete=models.CASCADE,\n        related_name=\"text\",\n        help_text=\"The audio data\",\n    )\n    text = models.TextField(help_text=\"The text of the audio\")\n\n    created_at = models.DateTimeField(\n        auto_now_add=True, help_text=\"The created time of the text\"\n    )\n    updated_at = models.DateTimeField(\n        auto_now=True, help_text=\"The updated time of the text\"\n    )\n    model_name = models.CharField(\n        max_length=100,\n        help_text=\"The name of the model\",\n        null=True,\n        blank=True,\n        default=\"whisper\",\n    )\n\n    def __str__(self):\n        return self.text\n\n    class Meta:\n        verbose_name = \"Data Text\"\n        verbose_name_plural = \"Data Texts\"\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.DataVideo","title":"<code>DataVideo</code>","text":"<p>               Bases: <code>Model</code></p> <p>Link to home and hardware device, and the video data will be stored in the database It will be created by the endpoint from client side when video data is acquired Same as the audio data, the video data will be stored in the database It will not be directly connected to the audio data Audio data and video data will be connected by the time range softly</p> Source code in <code>API/hardware/models.py</code> <pre><code>class DataVideo(models.Model):\n    \"\"\"\n    Link to home and hardware device, and the video data will be stored in the database\n    It will be created by the endpoint from client side when video data is acquired\n    Same as the audio data, the video data will be stored in the database\n    It will not be directly connected to the audio data\n    Audio data and video data will be connected by the time range softly\n    \"\"\"\n\n    home = models.ForeignKey(\n        Home, on_delete=models.CASCADE, related_name=\"video\", null=True, blank=True\n    )\n    uid = models.CharField(\n        max_length=100,\n        help_text=\"the uid of the video acquire session, link back to client logs\",\n    )\n    hardware_device_mac_address = models.CharField(\n        max_length=100,\n        help_text=\"The mac address of the hardware device\",\n        null=True,\n        blank=True,\n    )\n    # TODO: add start and end time?\n    video_file = models.CharField(max_length=100, help_text=\"The video file\")\n    start_time = models.DateTimeField(help_text=\"The start time of the video\")\n    end_time = models.DateTimeField(help_text=\"The end time of the video\")\n\n    created_at = models.DateTimeField(\n        auto_now_add=True, help_text=\"The created time of the video\"\n    )\n    updated_at = models.DateTimeField(\n        auto_now=True, help_text=\"The updated time of the video\"\n    )\n\n    def __str__(self):\n        return f\"{self.uid} - {self.video_file}\"\n\n    class Meta:\n        verbose_name = \"Data Video\"\n        verbose_name_plural = \"Data Videos\"\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.HardWareDevice","title":"<code>HardWareDevice</code>","text":"<p>               Bases: <code>Model</code></p> <p>One home can have multiple hardware devices, and the hardware device can be used to acquire the audio and video data</p> Source code in <code>API/hardware/models.py</code> <pre><code>class HardWareDevice(models.Model):\n    \"\"\"\n    One home can have multiple hardware devices, and the hardware device can be used to acquire the audio and video data\n\n    \"\"\"\n\n    home = models.ForeignKey(\n        Home,\n        on_delete=models.CASCADE,\n        related_name=\"hardware_devices\",\n        null=True,\n        blank=True,\n    )\n    mac_address = models.CharField(\n        max_length=100, help_text=\"The mac address of the hardware device\", unique=True\n    )\n    device_name = models.CharField(\n        max_length=100,\n        help_text=\"The name of the hardware device\",\n        null=True,\n        blank=True,\n    )\n    device_type = models.CharField(\n        max_length=100,\n        help_text=\"The type of the hardware device\",\n        null=True,\n        blank=True,\n    )\n    description = models.TextField(\n        help_text=\"The description of the hardware device\", null=True, blank=True\n    )\n    created_at = models.DateTimeField(\n        auto_now_add=True, help_text=\"The created time of the hardware device\"\n    )\n    updated_at = models.DateTimeField(\n        auto_now=True, help_text=\"The updated time of the hardware device\"\n    )\n\n    class Meta:\n        verbose_name = \"Hardware Device\"\n        verbose_name_plural = \"Hardware Devices\"\n</code></pre>"},{"location":"Sources/API/hardware/models/#API.hardware.models.Home","title":"<code>Home</code>","text":"<p>               Bases: <code>Model</code></p> <p>Created by setup manually, and the client side can specify the home, so all data will be connected to this.</p> Source code in <code>API/hardware/models.py</code> <pre><code>class Home(models.Model):\n    \"\"\"\n    Created by setup manually, and the client side can specify the home, so all data will be connected to this.\n    \"\"\"\n\n    user = models.ForeignKey(User, on_delete=models.CASCADE)\n    name = models.CharField(\n        max_length=100, help_text=\"The name of the home\", default=\"Blue Boat House\"\n    )\n    address = models.CharField(\n        max_length=100,\n        help_text=\"The address of the home\",\n        default=\"1 Kings Park Ave, Crawley WA 6009\",\n    )\n\n    def __str__(self):\n        return f\"{self.name}\"\n</code></pre>"},{"location":"Sources/API/hardware/serializers/","title":"Serializers","text":""},{"location":"Sources/API/hardware/signals/","title":"Signals","text":""},{"location":"Sources/API/hardware/signals/#API.hardware.signals.add_data_multimodal_conversation_entry","title":"<code>add_data_multimodal_conversation_entry(sender, instance, created, **kwargs)</code>","text":"<p>Add data multimodal conversation</p> Source code in <code>API/hardware/signals.py</code> <pre><code>@receiver(post_save, sender=DataAudio)\ndef add_data_multimodal_conversation_entry(sender, instance, created, **kwargs):\n    \"\"\"\n    Add data multimodal conversation\n    \"\"\"\n    if created:\n        DataMultiModalConversation.objects.create(\n            audio=instance, track_id=instance.track_id\n        )\n</code></pre>"},{"location":"Sources/API/hardware/tests/","title":"Tests","text":""},{"location":"Sources/API/hardware/urls/","title":"URLs","text":""},{"location":"Sources/API/hardware/views/","title":"Views","text":""},{"location":"Sources/API/hardware/views/#API.hardware.views.AudioDataViewSet","title":"<code>AudioDataViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> Source code in <code>API/hardware/views.py</code> <pre><code>class AudioDataViewSet(viewsets.ModelViewSet):\n    queryset = DataAudio.objects.all()\n    serializer_class = AudioDataSerializer\n\n    @swagger_auto_schema(\n        operation_summary=\"Get an audio data s3 url\",\n        operation_description=\"Get an audio data\",\n        responses={200: \"The audio data\"},\n        tags=[\"hardware\"],\n    )\n    @action(\n        detail=False,\n        methods=[\"post\"],\n        url_path=\"get_audio_data\",\n        url_name=\"get_audio_data\",\n    )\n    def get_audio_data(self, request):\n        \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n        audio_id = request.data.get(\"audio_id\", None)\n        if audio_id is None:\n            return Response(\n                {\"message\": \"audio_id is required.\"},\n                status=status.HTTP_400_BAD_REQUEST,\n            )\n        audio_obj = DataAudio.objects.filter(id=audio_id).first()\n        if audio_obj is None:\n            return Response(\n                {\"message\": \"No audio data found.\"},\n                status=status.HTTP_404_NOT_FOUND,\n            )\n\n        s3_client = settings.BOTO3_SESSION.client(\"s3\")\n        try:\n            response = s3_client.generate_presigned_url(\n                \"get_object\",\n                Params={\n                    \"Bucket\": settings.S3_BUCKET,\n                    \"Key\": f\"Listener/audio/{audio_obj.uid}/audio/{audio_obj.audio_file}\",\n                },\n                ExpiresIn=3600,\n            )\n\n            return Response({\"audio_url\": response}, status=status.HTTP_200_OK)\n        except Exception as e:\n            logger.error(e)\n            return Response(\n                {\"message\": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR\n            )\n</code></pre>"},{"location":"Sources/API/hardware/views/#API.hardware.views.AudioDataViewSet.get_audio_data","title":"<code>get_audio_data(request)</code>","text":"<p>Override the post method to add custom swagger documentation.</p> Source code in <code>API/hardware/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"Get an audio data s3 url\",\n    operation_description=\"Get an audio data\",\n    responses={200: \"The audio data\"},\n    tags=[\"hardware\"],\n)\n@action(\n    detail=False,\n    methods=[\"post\"],\n    url_path=\"get_audio_data\",\n    url_name=\"get_audio_data\",\n)\ndef get_audio_data(self, request):\n    \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n    audio_id = request.data.get(\"audio_id\", None)\n    if audio_id is None:\n        return Response(\n            {\"message\": \"audio_id is required.\"},\n            status=status.HTTP_400_BAD_REQUEST,\n        )\n    audio_obj = DataAudio.objects.filter(id=audio_id).first()\n    if audio_obj is None:\n        return Response(\n            {\"message\": \"No audio data found.\"},\n            status=status.HTTP_404_NOT_FOUND,\n        )\n\n    s3_client = settings.BOTO3_SESSION.client(\"s3\")\n    try:\n        response = s3_client.generate_presigned_url(\n            \"get_object\",\n            Params={\n                \"Bucket\": settings.S3_BUCKET,\n                \"Key\": f\"Listener/audio/{audio_obj.uid}/audio/{audio_obj.audio_file}\",\n            },\n            ExpiresIn=3600,\n        )\n\n        return Response({\"audio_url\": response}, status=status.HTTP_200_OK)\n    except Exception as e:\n        logger.error(e)\n        return Response(\n            {\"message\": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR\n        )\n</code></pre>"},{"location":"Sources/API/hardware/views/#API.hardware.views.Text2SpeechViewSet","title":"<code>Text2SpeechViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> Source code in <code>API/hardware/views.py</code> <pre><code>class Text2SpeechViewSet(viewsets.ModelViewSet):\n    queryset = ResSpeech.objects.all()\n    serializer_class = ResSpeechSerializer\n\n    # retrieve it based on the mac address\n    def get_queryset(self):\n        queryset = ResSpeech.objects.filter(\n            played=False, text2speech_file__isnull=False\n        )\n        home_id = self.request.query_params.get(\"home_id\", None)\n        logger.info(f\"Home id: {home_id}\")\n        if (home_id is not None) and (home_id != \"None\"):\n            home = Home.objects.filter(id=home_id).first()\n            if not home:\n                return None\n            queryset = queryset.filter(\n                home=home, played=False, text2speech_file__isnull=False\n            )\n\n        queryset = queryset.order_by(\"created_at\")\n        item = queryset.first()\n        if item:\n            item.played = True\n            item.save()\n        if item:\n            return [item]\n        else:\n            return None\n\n    def list(self, request, *args, **kwargs):\n        queryset = self.get_queryset()\n        if queryset is None:\n            return Response(\n                {\"message\": \"No text to speech found.\"},\n                status=status.HTTP_404_NOT_FOUND,\n            )\n\n        item = queryset[0]\n\n        s3_url = None\n        if item.text2speech_file is not None:\n            local_file = settings.AI_MEDIA_ROOT / item.text2speech_file.split(\"/\")[-1]\n            logger.info(local_file)\n            if local_file.exists() and (\n                settings.STORAGE_SOLUTION == settings.STORAGE_SOLUTION_VOLUME\n                or settings.STORAGE_SOLUTION == settings.STORAGE_SOLUTION_LOCAL\n            ):\n                s3_client = settings.BOTO3_SESSION.client(\"s3\")\n                s3_key = f\"Responder/tts/{item.text2speech_file.split('/')[-1]}\"\n                try:\n                    s3_client.upload_file(\n                        local_file,\n                        settings.S3_BUCKET,\n                        s3_key,\n                    )\n                except Exception as e:\n                    logger.error(e)\n                    logger.exception(e)\n                    # response with the HttpResponse\n            try:\n                s3_client = settings.BOTO3_SESSION.client(\"s3\")\n                response = s3_client.generate_presigned_url(\n                    \"get_object\",\n                    Params={\n                        \"Bucket\": settings.S3_BUCKET,\n                        \"Key\": f\"Responder/tts/{item.text2speech_file}\",\n                    },\n                    ExpiresIn=3600,\n                )\n                s3_url = response\n            except Exception as e:\n                logger.error(e)\n        data = ResSpeechSerializer(item).data\n        data[\"tts_url\"] = s3_url\n        logger.info(s3_url)\n        return Response(data, status=status.HTTP_200_OK)\n\n    @swagger_auto_schema(\n        operation_summary=\"Get speech audio s3 url\",\n        operation_description=\"Get the text to speech audio s3 url\",\n        responses={200: \"The text to speech\"},\n        tags=[\"hardware\"],\n    )\n    @action(\n        detail=False,\n        methods=[\"post\"],\n        url_path=\"get_text_to_speech\",\n        url_name=\"get_text_to_speech\",\n    )\n    def get_text_to_speech(self, request):\n        \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n        text2speech_id = request.data.get(\"text2speech_id\", None)\n        if text2speech_id is None:\n            return Response(\n                {\"message\": \"text2speech_id is required.\"},\n                status=status.HTTP_400_BAD_REQUEST,\n            )\n        text2speech_obj = ResSpeech.objects.filter(id=text2speech_id).first()\n        if text2speech_obj is None:\n            return Response(\n                {\"message\": \"No text to speech found.\"},\n                status=status.HTTP_404_NOT_FOUND,\n            )\n\n        s3_client = settings.BOTO3_SESSION.client(\"s3\")\n        try:\n            response = s3_client.generate_presigned_url(\n                \"get_object\",\n                Params={\n                    \"Bucket\": settings.S3_BUCKET,\n                    \"Key\": f\"tts/{text2speech_obj.text2speech_file}\",\n                },\n                ExpiresIn=3600,\n            )\n\n            return Response({\"tts_url\": response}, status=status.HTTP_200_OK)\n        except Exception as e:\n            logger.error(e)\n            return Response(\n                {\"message\": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR\n            )\n</code></pre>"},{"location":"Sources/API/hardware/views/#API.hardware.views.Text2SpeechViewSet.get_text_to_speech","title":"<code>get_text_to_speech(request)</code>","text":"<p>Override the post method to add custom swagger documentation.</p> Source code in <code>API/hardware/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"Get speech audio s3 url\",\n    operation_description=\"Get the text to speech audio s3 url\",\n    responses={200: \"The text to speech\"},\n    tags=[\"hardware\"],\n)\n@action(\n    detail=False,\n    methods=[\"post\"],\n    url_path=\"get_text_to_speech\",\n    url_name=\"get_text_to_speech\",\n)\ndef get_text_to_speech(self, request):\n    \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n    text2speech_id = request.data.get(\"text2speech_id\", None)\n    if text2speech_id is None:\n        return Response(\n            {\"message\": \"text2speech_id is required.\"},\n            status=status.HTTP_400_BAD_REQUEST,\n        )\n    text2speech_obj = ResSpeech.objects.filter(id=text2speech_id).first()\n    if text2speech_obj is None:\n        return Response(\n            {\"message\": \"No text to speech found.\"},\n            status=status.HTTP_404_NOT_FOUND,\n        )\n\n    s3_client = settings.BOTO3_SESSION.client(\"s3\")\n    try:\n        response = s3_client.generate_presigned_url(\n            \"get_object\",\n            Params={\n                \"Bucket\": settings.S3_BUCKET,\n                \"Key\": f\"tts/{text2speech_obj.text2speech_file}\",\n            },\n            ExpiresIn=3600,\n        )\n\n        return Response({\"tts_url\": response}, status=status.HTTP_200_OK)\n    except Exception as e:\n        logger.error(e)\n        return Response(\n            {\"message\": str(e)}, status=status.HTTP_500_INTERNAL_SERVER_ERROR\n        )\n</code></pre>"},{"location":"Sources/API/hardware/views/#API.hardware.views.list_files","title":"<code>list_files(request)</code>","text":"<p>List all the files in the S3 bucket</p> Source code in <code>API/hardware/views.py</code> <pre><code>@api_view([\"GET\"])\n@permission_classes([IsAuthenticated])\ndef list_files(request):\n    \"\"\"\n    List all the files in the S3 bucket\n    \"\"\"\n    from_time = request.data.get(\"from_time\", None)\n    logger.info(f\"From time: {from_time}\")\n    if from_time is None:\n        # default to 100 day ago\n        from_time = datetime.now() - timedelta(days=100)\n    else:\n        # get the from_time from the timestamp\n        from_time = datetime.fromtimestamp(float(from_time))\n\n    audio_files = DataAudio.objects.filter(created_at__gte=from_time)\n    video_files = DataVideo.objects.filter(created_at__gte=from_time)\n    audio_list_json = AudioDataSerializer(audio_files, many=True).data\n    video_list_json = VideoDataSerializer(video_files, many=True).data\n    return Response(\n        {\"audio_files\": audio_list_json, \"video_files\": video_list_json},\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"Sources/API/hardware/views/#API.hardware.views.upload_file","title":"<code>upload_file(request)</code>","text":"<p>This is for temporarily solution, as we host the centre server, and will not provide the S3 access to the general user</p> <p>So to testout our system, you can use this endpoint to upload files to S3 Focus on client and AI side</p> Source code in <code>API/hardware/views.py</code> <pre><code>@api_view([\"POST\"])\n@permission_classes([IsAuthenticated])\ndef upload_file(request):\n    \"\"\"\n    This is for temporarily solution, as we host the centre server,\n    and will not provide the S3 access to the general user\n\n    So to testout our system, you can use this endpoint to upload files to S3\n    Focus on client and AI side\n\n    \"\"\"\n    file = request.FILES.get(\"file\")\n    if file is None:\n        return Response(\n            {\"message\": \"No file found.\"},\n            status=status.HTTP_400_BAD_REQUEST,\n        )\n    s3_client = settings.BOTO3_SESSION.client(\"s3\")\n    dest_path = request.data.get(\"dest_path\", None)\n    if dest_path is None:\n        return Response(\n            {\"message\": \"dest_path is required.\"},\n            status=status.HTTP_400_BAD_REQUEST,\n        )\n    try:\n        s3_client.upload_fileobj(\n            file,\n            settings.S3_BUCKET,\n            dest_path,\n        )\n        return Response(\n            {\"message\": \"File uploaded successfully.\"},\n            status=status.HTTP_200_OK,\n        )\n    except Exception as e:\n        logger.error(e)\n        return Response(\n            {\"message\": str(e)},\n            status=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        )\n</code></pre>"},{"location":"Sources/API/hardware/migrations/0001_init/","title":"0001 init","text":""},{"location":"Sources/API/hardware/migrations/0002_add_rag/","title":"0002 add rag","text":""},{"location":"Sources/API/llm/admin/","title":"Admin","text":""},{"location":"Sources/API/llm/apps/","title":"Apps","text":""},{"location":"Sources/API/llm/models/","title":"Models","text":""},{"location":"Sources/API/llm/models/#API.llm.models.LLMConfigRecords","title":"<code>LLMConfigRecords</code>","text":"<p>               Bases: <code>Model</code></p> Source code in <code>API/llm/models.py</code> <pre><code>class LLMConfigRecords(models.Model):\n    model_name = models.CharField(max_length=100)\n    model_size = models.CharField(max_length=100)\n    model_family = models.CharField(max_length=100)\n    model_type = models.CharField(\n        max_length=100,\n        choices=[\n            (\"hf\", \"HuggingFace\"),\n            (\"api\", \"API\"),\n            (\"llama.cpp\", \"llama.cpp\"),\n            (\"chatglm.cpp\", \"chatglm.cpp\"),\n        ],\n        default=\"hf\",\n    )\n    repo = models.CharField(max_length=100, blank=True, null=True)\n    filename = models.CharField(max_length=100, blank=True, null=True)\n    file_size = models.FloatField(blank=True, null=True)\n    available = models.BooleanField(default=False)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    def __str__(self):\n        return f\"{self.model_name} - {self.created_at.strftime('%Y-%m-%d %H:%M:%S')}\"\n\n    @property\n    def model_path(self):\n        return Path(\n            settings.BASE_DIR\n            / \"llm\"\n            / \"llm_call\"\n            / \"models\"\n            / self.model_family\n            / self.filename\n        )\n\n    def download_model(self):\n        \"\"\"\n        Download the model from the model_details\n        :return:\n        \"\"\"\n        download_url = hf_hub_url(repo_id=self.repo, filename=self.filename)\n        logger.critical(f\"Downloading model from {download_url}\")\n\n        model_general_folder = Path(\n            settings.BASE_DIR / \"llm\" / \"llm_call\" / \"models\" / self.model_family\n        )\n        logger.critical(f\"Model folder {model_general_folder}\")\n        model_general_folder.mkdir(parents=True, exist_ok=True)\n        filename = model_general_folder / self.filename\n\n        response = requests.get(download_url, stream=True)\n\n        # Total size in bytes.\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024  # 1 Kilobyte\n        logger.critical(f\"Downloading {self.filename} to {model_general_folder}\")\n        logger.critical(f\"Total size: {total_size}\")\n        progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                progress_bar.update(len(data))\n                file.write(data)\n        progress_bar.close()\n\n        if total_size != 0 and progress_bar.n != total_size:\n            logger.error(\"ERROR, something went wrong\")\n            return False\n        return True\n\n    class Meta:\n        verbose_name = \"LLM Config Record\"\n        verbose_name_plural = \"LLM Config Records\"\n</code></pre>"},{"location":"Sources/API/llm/models/#API.llm.models.LLMConfigRecords.download_model","title":"<code>download_model()</code>","text":"<p>Download the model from the model_details :return:</p> Source code in <code>API/llm/models.py</code> <pre><code>def download_model(self):\n    \"\"\"\n    Download the model from the model_details\n    :return:\n    \"\"\"\n    download_url = hf_hub_url(repo_id=self.repo, filename=self.filename)\n    logger.critical(f\"Downloading model from {download_url}\")\n\n    model_general_folder = Path(\n        settings.BASE_DIR / \"llm\" / \"llm_call\" / \"models\" / self.model_family\n    )\n    logger.critical(f\"Model folder {model_general_folder}\")\n    model_general_folder.mkdir(parents=True, exist_ok=True)\n    filename = model_general_folder / self.filename\n\n    response = requests.get(download_url, stream=True)\n\n    # Total size in bytes.\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kilobyte\n    logger.critical(f\"Downloading {self.filename} to {model_general_folder}\")\n    logger.critical(f\"Total size: {total_size}\")\n    progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n    with open(filename, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n\n    if total_size != 0 and progress_bar.n != total_size:\n        logger.error(\"ERROR, something went wrong\")\n        return False\n    return True\n</code></pre>"},{"location":"Sources/API/llm/serializers/","title":"Serializers","text":""},{"location":"Sources/API/llm/tests/","title":"Tests","text":""},{"location":"Sources/API/llm/urls/","title":"URLs","text":""},{"location":"Sources/API/llm/views/","title":"Views","text":""},{"location":"Sources/API/llm/views/#API.llm.views.LLMConfigViewSet","title":"<code>LLMConfigViewSet</code>","text":"<p>               Bases: <code>ModelViewSet</code></p> Source code in <code>API/llm/views.py</code> <pre><code>class LLMConfigViewSet(viewsets.ModelViewSet):\n    permission_classes = [IsAuthenticated]\n    serializer_class = LLMConfigRecordsSerializer\n    \"\"\"\n    List all available llm config records\n    \"\"\"\n    queryset = LLMConfigRecords.objects.all()\n\n    @swagger_auto_schema(\n        operation_summary=\"List LLM Model\",\n        operation_description=\"Obtain the list of available LLM models and their status, need to have a token\",\n        responses={200: LLMConfigRecordsSerializer(many=True)},\n        tags=[\"llm\"],\n    )\n    @csrf_exempt\n    def list(self, request, *args, **kwargs):\n        \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n        return super().list(request, *args, **kwargs)\n</code></pre>"},{"location":"Sources/API/llm/views/#API.llm.views.LLMConfigViewSet.serializer_class","title":"<code>serializer_class = LLMConfigRecordsSerializer</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List all available llm config records</p>"},{"location":"Sources/API/llm/views/#API.llm.views.LLMConfigViewSet.list","title":"<code>list(request, *args, **kwargs)</code>","text":"<p>Override the post method to add custom swagger documentation.</p> Source code in <code>API/llm/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"List LLM Model\",\n    operation_description=\"Obtain the list of available LLM models and their status, need to have a token\",\n    responses={200: LLMConfigRecordsSerializer(many=True)},\n    tags=[\"llm\"],\n)\n@csrf_exempt\ndef list(self, request, *args, **kwargs):\n    \"\"\"Override the post method to add custom swagger documentation.\"\"\"\n    return super().list(request, *args, **kwargs)\n</code></pre>"},{"location":"Sources/API/llm/llm/config/","title":"Config","text":""},{"location":"Sources/API/llm/management/commands/check_models/","title":"CheckModels","text":""},{"location":"Sources/API/llm/management/commands/check_models/#API.llm.management.commands.check_models.Command","title":"<code>Command</code>","text":"<p>               Bases: <code>BaseCommand</code></p> Source code in <code>API/llm/management/commands/check_models.py</code> <pre><code>class Command(BaseCommand):\n    help = \"Check models and update the database\"\n\n    def handle(self, *args, **options):\n        \"\"\"\n        Loop through the MODELS dictionary and check if the model is in the database. If it is not, add it.\n        :param args:\n        :param options:\n        :return:\n        \"\"\"\n\n        for model_families in MODELS:\n            model_family = model_families[\"name\"]\n            model_type = model_families[\"model_type\"]\n            for model_info in model_families[\"models\"]:\n                if not LLMConfigRecords.objects.filter(\n                    model_name=model_info[\"name\"]\n                ).exists():\n                    record = LLMConfigRecords(\n                        model_name=model_info[\"name\"],\n                        model_size=model_info[\"size\"],\n                        model_family=model_family,\n                        model_type=model_type,\n                        repo=model_info[\"repo\"],\n                        filename=model_info[\"filename\"],\n                        available=False,\n                    )\n                    record.save()\n                    logger.critical(f\"Added {model_info['name']} to the database\")\n                else:\n                    logger.critical(f\"{model_info['name']} is already in the database\")\n</code></pre>"},{"location":"Sources/API/llm/management/commands/check_models/#API.llm.management.commands.check_models.Command.handle","title":"<code>handle(*args, **options)</code>","text":"<p>Loop through the MODELS dictionary and check if the model is in the database. If it is not, add it. :param args: :param options: :return:</p> Source code in <code>API/llm/management/commands/check_models.py</code> <pre><code>def handle(self, *args, **options):\n    \"\"\"\n    Loop through the MODELS dictionary and check if the model is in the database. If it is not, add it.\n    :param args:\n    :param options:\n    :return:\n    \"\"\"\n\n    for model_families in MODELS:\n        model_family = model_families[\"name\"]\n        model_type = model_families[\"model_type\"]\n        for model_info in model_families[\"models\"]:\n            if not LLMConfigRecords.objects.filter(\n                model_name=model_info[\"name\"]\n            ).exists():\n                record = LLMConfigRecords(\n                    model_name=model_info[\"name\"],\n                    model_size=model_info[\"size\"],\n                    model_family=model_family,\n                    model_type=model_type,\n                    repo=model_info[\"repo\"],\n                    filename=model_info[\"filename\"],\n                    available=False,\n                )\n                record.save()\n                logger.critical(f\"Added {model_info['name']} to the database\")\n            else:\n                logger.critical(f\"{model_info['name']} is already in the database\")\n</code></pre>"},{"location":"Sources/API/llm/migrations/0001_init/","title":"0001 init","text":""},{"location":"Sources/API/orchestrator/admin/","title":"Admin","text":""},{"location":"Sources/API/orchestrator/apps/","title":"Apps","text":""},{"location":"Sources/API/orchestrator/models/","title":"Models","text":""},{"location":"Sources/API/orchestrator/models/#API.orchestrator.models.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>Model</code></p> Source code in <code>API/orchestrator/models.py</code> <pre><code>class Task(models.Model):\n    user = models.ForeignKey(\n        User,\n        on_delete=models.SET_NULL,\n        null=True,\n        blank=True,\n        related_name=\"tasks\",\n        help_text=\"Select the user\",\n    )\n    name = models.CharField(\n        max_length=100, help_text=\"A unique name to track the cluster of tasks\"\n    )\n\n    task_name = models.CharField(\n        max_length=100,\n        help_text=\"The name of the task\",\n    )\n    parameters = models.JSONField(\n        default=dict,\n        blank=True,\n        null=True,\n        help_text=\"Enter the parameters for the task\",\n    )\n    result_status = models.CharField(\n        max_length=100,\n        choices=[\n            (\"pending\", \"Pending\"),\n            (\"completed\", \"Completed\"),\n            (\"failed\", \"Failed\"),\n            (\"started\", \"Started\"),\n            (\"cancelled\", \"Cancelled\"),\n        ],\n        default=\"pending\",\n    )\n    result_json = models.JSONField(\n        default=dict,\n        blank=True,\n        null=True,\n        help_text=\"The result of the task\",\n    )\n    description = models.TextField(blank=True, null=True)\n    track_id = models.CharField(\n        max_length=100,\n        blank=True,\n        null=True,\n        help_text=\"The tracking ID of the task, will start with T-{cluster_name}-{id}\",\n    )\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    def __str__(self):\n        return self.name\n\n    @classmethod\n    def create_task(\n        cls,\n        user: Optional[User],\n        name: str,\n        task_name: str,\n        parameters: dict,\n        description: str = \"\",\n        track_id: Optional[str] = None,\n    ):\n        \"\"\"\n        Create a task\n        Args:\n            user (User): The user who created the task\n            name (str): The name of the task\n            task_name (str): The name of the task\n            parameters (dict): The parameters for the task\n            description (str): The description of the task\n            track_id (str): The tracking ID of the task, will start with T-{cluster_name}-{id}\n\n        Returns:\n\n        \"\"\"\n        task = cls(\n            user=user,\n            name=name,\n            task_name=task_name,\n            parameters=parameters,\n            description=description,\n            track_id=track_id,\n        )\n        task.save()\n        return task\n\n    @staticmethod\n    def init_track_id(name: str) -&gt; str:\n        \"\"\"\n        Initialize the track ID\n        Args:\n            name (str): The name of the task\n\n        Returns:\n            str: The track ID\n        \"\"\"\n        uid = str(uuid4())\n        # replace the - with \"\"\n        uid = uid.replace(\"-\", \"\")\n        return f\"T-{name}-{uid}\"\n\n    # override the save method, to call the chain\n    def save(self, *args, **kwargs):\n        # if it is updated, then we need to call the chain\n        if self.result_status == \"completed\":\n            completed_task.send(sender=self, data=self.__dict__)\n        super().save(*args, **kwargs)\n\n    @staticmethod\n    def get_task_name_choices():\n        \"\"\"\n        Get dynamic task name choices\n        Returns:\n            list: List of tuples containing task name choices\n        \"\"\"\n        # Here you can fetch the choices from an external source or database\n        return [\n            (\"quantization_llm\", \"Quantization LLM\"),\n            (\"hf_llm\", \"HF LLM\"),\n            (\"emotion_detection\", \"Emotion Detection\"),\n            (\"speech2text\", \"Speech2Text\"),\n            (\"text2speech\", \"Text2Speech\"),\n            (\"general_ml\", \"General ML\"),\n            (\"openai_speech2text\", \"OpenAI Speech2Text\"),\n            (\"openai_gpt_4o\", \"OpenAI GPT4o\"),\n            (\"openai_gpt_35\", \"OpenAI GPT3.5\"),\n            (\"openai_text2speech\", \"OpenAI Text2Speech\"),\n            (\"openai_gpt_4o_text_and_image\", \"OpenAI GPT4o Text and Image\"),\n            (\"openai_gpt_4o_text_only\", \"OpenAI GPT4o Text\"),\n            (\"rag\", \"RAG\"),\n        ]\n\n    @staticmethod\n    def task_ml_task_mapping() -&gt; dict:\n        return {\n            \"quantization_llm\": \"text_generation\",\n            \"hf_llm\": \"text_generation\",\n            \"emotion_detection\": \"emotion_detection\",\n            \"speech2text\": \"speech2text\",\n            \"text2speech\": \"text2speech\",\n            \"general_ml\": \"general_ml\",\n            \"openai_speech2text\": \"speech2text\",\n            \"openai_gpt_4o\": \"text_generation\",\n            \"openai_gpt_35\": \"text_generation\",\n            \"openai_text2speech\": \"text2speech\",\n            \"openai_gpt_4o_text_and_image\": \"text_generation\",\n            \"openai_gpt_4o_text_only\": \"text_generation\",\n            \"rag\": \"rag\",\n        }\n</code></pre>"},{"location":"Sources/API/orchestrator/models/#API.orchestrator.models.Task.create_task","title":"<code>create_task(user, name, task_name, parameters, description='', track_id=None)</code>  <code>classmethod</code>","text":"<p>Create a task Args:     user (User): The user who created the task     name (str): The name of the task     task_name (str): The name of the task     parameters (dict): The parameters for the task     description (str): The description of the task     track_id (str): The tracking ID of the task, will start with T-{cluster_name}-{id}</p> <p>Returns:</p> Source code in <code>API/orchestrator/models.py</code> <pre><code>@classmethod\ndef create_task(\n    cls,\n    user: Optional[User],\n    name: str,\n    task_name: str,\n    parameters: dict,\n    description: str = \"\",\n    track_id: Optional[str] = None,\n):\n    \"\"\"\n    Create a task\n    Args:\n        user (User): The user who created the task\n        name (str): The name of the task\n        task_name (str): The name of the task\n        parameters (dict): The parameters for the task\n        description (str): The description of the task\n        track_id (str): The tracking ID of the task, will start with T-{cluster_name}-{id}\n\n    Returns:\n\n    \"\"\"\n    task = cls(\n        user=user,\n        name=name,\n        task_name=task_name,\n        parameters=parameters,\n        description=description,\n        track_id=track_id,\n    )\n    task.save()\n    return task\n</code></pre>"},{"location":"Sources/API/orchestrator/models/#API.orchestrator.models.Task.get_task_name_choices","title":"<code>get_task_name_choices()</code>  <code>staticmethod</code>","text":"<p>Get dynamic task name choices Returns:     list: List of tuples containing task name choices</p> Source code in <code>API/orchestrator/models.py</code> <pre><code>@staticmethod\ndef get_task_name_choices():\n    \"\"\"\n    Get dynamic task name choices\n    Returns:\n        list: List of tuples containing task name choices\n    \"\"\"\n    # Here you can fetch the choices from an external source or database\n    return [\n        (\"quantization_llm\", \"Quantization LLM\"),\n        (\"hf_llm\", \"HF LLM\"),\n        (\"emotion_detection\", \"Emotion Detection\"),\n        (\"speech2text\", \"Speech2Text\"),\n        (\"text2speech\", \"Text2Speech\"),\n        (\"general_ml\", \"General ML\"),\n        (\"openai_speech2text\", \"OpenAI Speech2Text\"),\n        (\"openai_gpt_4o\", \"OpenAI GPT4o\"),\n        (\"openai_gpt_35\", \"OpenAI GPT3.5\"),\n        (\"openai_text2speech\", \"OpenAI Text2Speech\"),\n        (\"openai_gpt_4o_text_and_image\", \"OpenAI GPT4o Text and Image\"),\n        (\"openai_gpt_4o_text_only\", \"OpenAI GPT4o Text\"),\n        (\"rag\", \"RAG\"),\n    ]\n</code></pre>"},{"location":"Sources/API/orchestrator/models/#API.orchestrator.models.Task.init_track_id","title":"<code>init_track_id(name)</code>  <code>staticmethod</code>","text":"<p>Initialize the track ID Args:     name (str): The name of the task</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The track ID</p> Source code in <code>API/orchestrator/models.py</code> <pre><code>@staticmethod\ndef init_track_id(name: str) -&gt; str:\n    \"\"\"\n    Initialize the track ID\n    Args:\n        name (str): The name of the task\n\n    Returns:\n        str: The track ID\n    \"\"\"\n    uid = str(uuid4())\n    # replace the - with \"\"\n    uid = uid.replace(\"-\", \"\")\n    return f\"T-{name}-{uid}\"\n</code></pre>"},{"location":"Sources/API/orchestrator/serializers/","title":"Serializers","text":""},{"location":"Sources/API/orchestrator/tests/","title":"Tests","text":""},{"location":"Sources/API/orchestrator/urls/","title":"URLs","text":""},{"location":"Sources/API/orchestrator/views/","title":"Views","text":""},{"location":"Sources/API/orchestrator/views/#API.orchestrator.views.QueueTaskViewSet","title":"<code>QueueTaskViewSet</code>","text":"<p>               Bases: <code>ViewSet</code></p> <p>A ViewSet for queuing AI tasks generally</p> Source code in <code>API/orchestrator/views.py</code> <pre><code>class QueueTaskViewSet(viewsets.ViewSet):\n    \"\"\"\n    A ViewSet for queuing AI tasks generally\n\n    \"\"\"\n\n    # This ensures that only authenticated users can access these endpoints\n    permission_classes = [IsAuthenticated]\n\n    @swagger_auto_schema(\n        operation_summary=\"Queue an AI task\",\n        operation_description=\"This will include LLM, STT, and other AI tasks\",\n        request_body=TaskSerializer,\n        responses={200: \"Task queued successfully\"},\n    )\n    @action(detail=False, methods=[\"post\"], permission_classes=[IsAuthenticated])\n    def ai_task(self, request):\n        \"\"\"\n        Endpoint to queue tasks for AI Client side to run\n        \"\"\"\n        data = request.data\n        serializer = TaskSerializer(data=data)\n\n        try:\n            serializer.is_valid(raise_exception=True)\n        except Exception as e:\n            logger.error(f\"Error validating task request: {e}\")\n            return Response(\n                {\"error\": f\"Error validating task request: {e}\"},\n                status=status.HTTP_400_BAD_REQUEST,\n            )\n\n        # if track id not set, set up the track id\n        track_id = data.get(\"track_id\", None)\n        logger.info(f\"Track ID: {track_id}\")\n        # if track_id is not provided, then we need to generate it\n        if track_id is None:\n            track_id = Task.init_track_id(CLUSTER_Q_ETE_CONVERSATION_NAME)\n            logger.info(f\"Generated track ID: {track_id}\")\n            serializer.validated_data[\"track_id\"] = track_id\n\n        # based on the track cluster name, determine what to do next\n        task_id = ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"init\",\n            next_component_params=serializer.validated_data[\"parameters\"],\n            name=data.get(\"name\", None),\n            user=request.user,\n        )\n\n        return Response(\n            {\"message\": \"LLM task queued successfully\", \"task_id\": task_id},\n            status=status.HTTP_200_OK,\n        )\n\n    @swagger_auto_schema(\n        operation_summary=\"Worker: Get Task\",\n        operation_description=\"Get the task\",\n        responses={200: \"Task retrieved successfully\"},\n    )\n    @action(\n        detail=False,\n        methods=[\"get\"],\n        permission_classes=[IsAuthenticated],\n        url_path=\"task/(?P&lt;task_name&gt;.+)\",\n        url_name=\"task\",\n    )\n    def task(self, request, task_name=\"all\"):\n        \"\"\"\n        Endpoint to get the task for AI\n        \"\"\"\n        cool_down_task = 10  # 10 second\n        cool_down_time = datetime.now() - timedelta(seconds=cool_down_task)\n        try:\n            if task_name == \"all\":\n                task = Task.objects.filter(\n                    result_status=\"pending\", created_at__lte=cool_down_time\n                ).first()\n            else:\n                task = Task.objects.filter(\n                    task_name=task_name,\n                    result_status=\"pending\",\n                    created_at__lte=cool_down_time,\n                ).first()\n            if task is None:\n                return Response(\n                    {\"error\": f\"No pending {task_name} tasks found\"},\n                    status=status.HTTP_404_NOT_FOUND,\n                )\n            task.result_status = \"started\"\n            task.save()\n            task_serializer = TaskSerializer(task)\n            logger.critical(f\"Task {task.id} retrieved successfully\")\n            return Response(data=task_serializer.data, status=status.HTTP_200_OK)\n        except Task.DoesNotExist:\n            return Response(\n                {\"error\": f\"No pending {task_name} tasks found\"},\n                status=status.HTTP_404_NOT_FOUND,\n            )\n\n    # add an endpoint to update the task result\n    @swagger_auto_schema(\n        operation_summary=\"Worker: Result Update\",\n        operation_description=\"Update the task result\",\n        request_body=TaskSerializer,\n        responses={200: \"Task result updated successfully\"},\n    )\n    @action(\n        detail=True,\n        methods=[\"post\"],\n        permission_classes=[IsAuthenticated],\n        url_path=\"update_result\",\n        url_name=\"update_result\",\n    )\n    def update_result(self, request, pk=None):\n        \"\"\"\n        Endpoint to update the result of a task.\n        \"\"\"\n        try:\n            data = request.data\n            task = Task.objects.filter(id=pk).first()\n            if task is None:\n                return Response(\n                    {\"error\": f\"Task with ID {pk} does not exist\"},\n                    status=status.HTTP_404_NOT_FOUND,\n                )\n\n            serializer = TaskSerializer(data=data, instance=task, partial=True)\n            serializer.is_valid(raise_exception=True)\n            serializer.save()\n            return Response(\n                {\"message\": f\"Task {task.id} updated successfully\"},\n                status=status.HTTP_200_OK,\n            )\n        except Exception as e:\n            logger.error(f\"Error updating task result: {e}\")\n            logger.exception(e)\n            return Response(\n                {\"error\": f\"Error updating task result: {e}\"},\n                status=status.HTTP_400_BAD_REQUEST,\n            )\n\n    @swagger_auto_schema(\n        operation_summary=\"Worker: Register\",\n        operation_description=\"Register a worker\",\n        responses={200: \"Worker registered or updated successfully\"},\n        request_body=TaskWorkerSerializer,\n    )\n    @action(detail=False, methods=[\"post\"], permission_classes=[IsAuthenticated])\n    def worker(self, request):\n        \"\"\"\n        Endpoint to register a GPU worker.\n        \"\"\"\n        data = request.data\n        serializer = TaskWorkerSerializer(data=data)\n        serializer.is_valid(raise_exception=True)\n\n        uuid = data.get(\"uuid\")\n        mac_address = data.get(\"mac_address\")\n        ip_address = data.get(\"ip_address\")\n        task_name = data.get(\"task_name\")\n\n        worker, created = TaskWorker.objects.get_or_create(\n            uuid=uuid,\n            defaults={\n                \"mac_address\": mac_address,\n                \"ip_address\": ip_address,\n                \"task_name\": task_name,\n            },\n        )\n        if not created:\n            worker.mac_address = mac_address\n            worker.ip_address = ip_address\n            worker.save()\n\n        return Response(\n            {\"message\": f\"Worker {uuid} registered or updated successfully\"},\n            status=status.HTTP_200_OK,\n        )\n</code></pre>"},{"location":"Sources/API/orchestrator/views/#API.orchestrator.views.QueueTaskViewSet.ai_task","title":"<code>ai_task(request)</code>","text":"<p>Endpoint to queue tasks for AI Client side to run</p> Source code in <code>API/orchestrator/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"Queue an AI task\",\n    operation_description=\"This will include LLM, STT, and other AI tasks\",\n    request_body=TaskSerializer,\n    responses={200: \"Task queued successfully\"},\n)\n@action(detail=False, methods=[\"post\"], permission_classes=[IsAuthenticated])\ndef ai_task(self, request):\n    \"\"\"\n    Endpoint to queue tasks for AI Client side to run\n    \"\"\"\n    data = request.data\n    serializer = TaskSerializer(data=data)\n\n    try:\n        serializer.is_valid(raise_exception=True)\n    except Exception as e:\n        logger.error(f\"Error validating task request: {e}\")\n        return Response(\n            {\"error\": f\"Error validating task request: {e}\"},\n            status=status.HTTP_400_BAD_REQUEST,\n        )\n\n    # if track id not set, set up the track id\n    track_id = data.get(\"track_id\", None)\n    logger.info(f\"Track ID: {track_id}\")\n    # if track_id is not provided, then we need to generate it\n    if track_id is None:\n        track_id = Task.init_track_id(CLUSTER_Q_ETE_CONVERSATION_NAME)\n        logger.info(f\"Generated track ID: {track_id}\")\n        serializer.validated_data[\"track_id\"] = track_id\n\n    # based on the track cluster name, determine what to do next\n    task_id = ClusterManager.chain_next(\n        track_id=track_id,\n        current_component=\"init\",\n        next_component_params=serializer.validated_data[\"parameters\"],\n        name=data.get(\"name\", None),\n        user=request.user,\n    )\n\n    return Response(\n        {\"message\": \"LLM task queued successfully\", \"task_id\": task_id},\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"Sources/API/orchestrator/views/#API.orchestrator.views.QueueTaskViewSet.task","title":"<code>task(request, task_name='all')</code>","text":"<p>Endpoint to get the task for AI</p> Source code in <code>API/orchestrator/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"Worker: Get Task\",\n    operation_description=\"Get the task\",\n    responses={200: \"Task retrieved successfully\"},\n)\n@action(\n    detail=False,\n    methods=[\"get\"],\n    permission_classes=[IsAuthenticated],\n    url_path=\"task/(?P&lt;task_name&gt;.+)\",\n    url_name=\"task\",\n)\ndef task(self, request, task_name=\"all\"):\n    \"\"\"\n    Endpoint to get the task for AI\n    \"\"\"\n    cool_down_task = 10  # 10 second\n    cool_down_time = datetime.now() - timedelta(seconds=cool_down_task)\n    try:\n        if task_name == \"all\":\n            task = Task.objects.filter(\n                result_status=\"pending\", created_at__lte=cool_down_time\n            ).first()\n        else:\n            task = Task.objects.filter(\n                task_name=task_name,\n                result_status=\"pending\",\n                created_at__lte=cool_down_time,\n            ).first()\n        if task is None:\n            return Response(\n                {\"error\": f\"No pending {task_name} tasks found\"},\n                status=status.HTTP_404_NOT_FOUND,\n            )\n        task.result_status = \"started\"\n        task.save()\n        task_serializer = TaskSerializer(task)\n        logger.critical(f\"Task {task.id} retrieved successfully\")\n        return Response(data=task_serializer.data, status=status.HTTP_200_OK)\n    except Task.DoesNotExist:\n        return Response(\n            {\"error\": f\"No pending {task_name} tasks found\"},\n            status=status.HTTP_404_NOT_FOUND,\n        )\n</code></pre>"},{"location":"Sources/API/orchestrator/views/#API.orchestrator.views.QueueTaskViewSet.update_result","title":"<code>update_result(request, pk=None)</code>","text":"<p>Endpoint to update the result of a task.</p> Source code in <code>API/orchestrator/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"Worker: Result Update\",\n    operation_description=\"Update the task result\",\n    request_body=TaskSerializer,\n    responses={200: \"Task result updated successfully\"},\n)\n@action(\n    detail=True,\n    methods=[\"post\"],\n    permission_classes=[IsAuthenticated],\n    url_path=\"update_result\",\n    url_name=\"update_result\",\n)\ndef update_result(self, request, pk=None):\n    \"\"\"\n    Endpoint to update the result of a task.\n    \"\"\"\n    try:\n        data = request.data\n        task = Task.objects.filter(id=pk).first()\n        if task is None:\n            return Response(\n                {\"error\": f\"Task with ID {pk} does not exist\"},\n                status=status.HTTP_404_NOT_FOUND,\n            )\n\n        serializer = TaskSerializer(data=data, instance=task, partial=True)\n        serializer.is_valid(raise_exception=True)\n        serializer.save()\n        return Response(\n            {\"message\": f\"Task {task.id} updated successfully\"},\n            status=status.HTTP_200_OK,\n        )\n    except Exception as e:\n        logger.error(f\"Error updating task result: {e}\")\n        logger.exception(e)\n        return Response(\n            {\"error\": f\"Error updating task result: {e}\"},\n            status=status.HTTP_400_BAD_REQUEST,\n        )\n</code></pre>"},{"location":"Sources/API/orchestrator/views/#API.orchestrator.views.QueueTaskViewSet.worker","title":"<code>worker(request)</code>","text":"<p>Endpoint to register a GPU worker.</p> Source code in <code>API/orchestrator/views.py</code> <pre><code>@swagger_auto_schema(\n    operation_summary=\"Worker: Register\",\n    operation_description=\"Register a worker\",\n    responses={200: \"Worker registered or updated successfully\"},\n    request_body=TaskWorkerSerializer,\n)\n@action(detail=False, methods=[\"post\"], permission_classes=[IsAuthenticated])\ndef worker(self, request):\n    \"\"\"\n    Endpoint to register a GPU worker.\n    \"\"\"\n    data = request.data\n    serializer = TaskWorkerSerializer(data=data)\n    serializer.is_valid(raise_exception=True)\n\n    uuid = data.get(\"uuid\")\n    mac_address = data.get(\"mac_address\")\n    ip_address = data.get(\"ip_address\")\n    task_name = data.get(\"task_name\")\n\n    worker, created = TaskWorker.objects.get_or_create(\n        uuid=uuid,\n        defaults={\n            \"mac_address\": mac_address,\n            \"ip_address\": ip_address,\n            \"task_name\": task_name,\n        },\n    )\n    if not created:\n        worker.mac_address = mac_address\n        worker.ip_address = ip_address\n        worker.save()\n\n    return Response(\n        {\"message\": f\"Worker {uuid} registered or updated successfully\"},\n        status=status.HTTP_200_OK,\n    )\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/clusters/","title":"Clusters","text":""},{"location":"Sources/API/orchestrator/chain/clusters/#API.orchestrator.chain.clusters.CLUSTER_GPT_35_ETE_CONVERSATION","title":"<code>CLUSTER_GPT_35_ETE_CONVERSATION = {'openai_speech2text': {'order': 0, 'extra_params': {}, 'component_type': 'task', 'task_name': 'openai_speech2text'}, 'completed_openai_speech2text': {'order': 1, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'created_data_text': {'order': 2, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'completed_openai_gpt_35': {'order': 3, 'extra_params': {'sample_ratio': 10, 'prompt_template': '{text}'}, 'component_type': 'task', 'task_name': 'openai_gpt_35'}, 'completed_openai_text2speech': {'order': 4, 'extra_params': {}, 'component_type': 'task', 'task_name': 'openai_text2speech'}}</code>  <code>module-attribute</code>","text":"<p>Cluster for gpt3.5 model and gpt3.5 with RAG</p>"},{"location":"Sources/API/orchestrator/chain/clusters/#API.orchestrator.chain.clusters.CLUSTER_GPT_4O_TEXT_ETE_CONVERSATION","title":"<code>CLUSTER_GPT_4O_TEXT_ETE_CONVERSATION = {'openai_speech2text': {'order': 0, 'extra_params': {}, 'component_type': 'task', 'task_name': 'openai_speech2text'}, 'completed_openai_speech2text': {'order': 1, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'created_data_text': {'order': 2, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'completed_openai_gpt_4o_text_only': {'order': 2, 'extra_params': {'sample_ratio': 10, 'prompt_template': '\\n            You are a robot, and you are talking to a human.\\n\\n            Your task is to generate a response to the human based on the text\\n\\n            You response will be directly send to end user.\\n\\n            The text is: {text}\\n            '}, 'component_type': 'task', 'task_name': 'openai_gpt_4o_text_only'}, 'completed_openai_text2speech': {'order': 3, 'extra_params': {}, 'component_type': 'task', 'task_name': 'openai_text2speech'}}</code>  <code>module-attribute</code>","text":"<p>Cluster for gpt3.5 model and gpt3.5 with RAG</p>"},{"location":"Sources/API/orchestrator/chain/clusters/#API.orchestrator.chain.clusters.CLUSTER_HF_ETE_CONVERSATION","title":"<code>CLUSTER_HF_ETE_CONVERSATION = {'speech2text': {'order': 0, 'extra_params': {}, 'component_type': 'task', 'task_name': 'speech2text'}, 'completed_speech2text': {'order': 1, 'extra_params': {}, 'component_type': 'signal', 'task_name': 'None'}, 'created_data_text': {'order': 2, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'completed_emotion_detection': {'order': 3, 'extra_params': {}, 'component_type': 'task', 'task_name': 'emotion_detection'}, 'completed_hf_llm': {'order': 4, 'extra_params': {'hf_model_name': 'Qwen/Qwen2-7B-Instruct'}, 'component_type': 'task', 'task_name': 'hf_llm'}, 'completed_text2speech': {'order': 5, 'extra_params': {}, 'component_type': 'task', 'task_name': 'text2speech'}}</code>  <code>module-attribute</code>","text":"<p>Create one to use the full GPT-4o models.</p> <p>In theory, it should takes the audio and video in, and then output audio.</p> <p>However, until now, the API for audio is not yet available.</p> <p>So we will use the walk around by using the speech to text model first, and then call GPT-4o</p>"},{"location":"Sources/API/orchestrator/chain/clusters/#API.orchestrator.chain.clusters.CLUSTER_Q_ETE_CONVERSATION","title":"<code>CLUSTER_Q_ETE_CONVERSATION = {'speech2text': {'order': 0, 'extra_params': {}, 'component_type': 'task', 'task_name': 'speech2text'}, 'completed_speech2text': {'order': 1, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'created_data_text': {'order': 2, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'completed_emotion_detection': {'order': 3, 'extra_params': {}, 'component_type': 'task', 'task_name': 'emotion_detection'}, 'completed_quantization_llm': {'order': 4, 'extra_params': {'llm_model_name': 'SOLAR-10'}, 'component_type': 'task', 'task_name': 'quantization_llm'}, 'completed_text2speech': {'order': 5, 'extra_params': {}, 'component_type': 'task', 'task_name': 'text2speech'}}</code>  <code>module-attribute</code>","text":"<p>Get rid of the emotion detection model</p>"},{"location":"Sources/API/orchestrator/chain/clusters/#API.orchestrator.chain.clusters.CLUSTER_Q_NO_EMOTION_ETE_CONVERSATION","title":"<code>CLUSTER_Q_NO_EMOTION_ETE_CONVERSATION = {'speech2text': {'order': 0, 'extra_params': {}, 'component_type': 'task', 'task_name': 'speech2text'}, 'completed_speech2text': {'order': 1, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'created_data_text': {'order': 2, 'extra_params': {}, 'component_type': 'signal', 'task_name': None}, 'completed_quantization_llm': {'order': 4, 'extra_params': {'llm_model_name': 'SOLAR-10'}, 'component_type': 'task', 'task_name': 'quantization_llm'}, 'completed_text2speech': {'order': 5, 'extra_params': {}, 'component_type': 'task', 'task_name': 'text2speech'}}</code>  <code>module-attribute</code>","text":"<p>This is the pipeline using the HF LLM model for the ETE conversation</p>"},{"location":"Sources/API/orchestrator/chain/clusters/#API.orchestrator.chain.clusters.logger","title":"<code>logger = get_logger(__name__)</code>  <code>module-attribute</code>","text":"<p>This is for the quantization LLM model for the ETE conversation</p>"},{"location":"Sources/API/orchestrator/chain/completed_emotion_detection/","title":"CompletedEmotionDetection","text":""},{"location":"Sources/API/orchestrator/chain/completed_emotion_detection/#API.orchestrator.chain.completed_emotion_detection.trigger_completed_emotion_detection","title":"<code>trigger_completed_emotion_detection(sender, **kwargs)</code>","text":"<p>This will create a task to do the quantization LLM inference</p> Source code in <code>API/orchestrator/chain/completed_emotion_detection.py</code> <pre><code>@receiver(completed_emotion_detection)\ndef trigger_completed_emotion_detection(sender, **kwargs):\n    \"\"\"\n    This will create a task to do the quantization LLM inference\n\n    \"\"\"\n    try:\n        logger.info(\"Emotion detection completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n\n        # get the text and emotion from the result\n        text = task_data.parameters[\"text\"]\n        emotion = task_data.result_json[\"result_profile\"].get(\"multi_modal_output\", {})\n        data_multimodal_conversation_log_context_emotion_detection(\n            task_data=task_data, result=emotion\n        )\n        emotion_text = (\n            \"Emotion value is from -1 to 1, -1 means negative, 1 means positive\\n\"\n        )\n\n        for key, value in emotion.items():\n            if key == \"A\":\n                emotion_text += f\"Audio emotion: {value}\\n\"\n            if key == \"T\":\n                emotion_text += f\"Text emotion: {value}\\n\"\n            if key == \"V\":\n                emotion_text += f\"Video emotion: {value}\\n\"\n            if key == \"M\":\n                emotion_text += f\"Overall emotion: {value}\\n\"\n        prompt = f\"\"\"\n            You are a conversational AI.\n            Your friend said: {text}.\n            And his emotion is detected like this:\n            {emotion_text}\n\n            Respond to him.\n            Your response will directly send to him.\n\n            \"\"\"\n\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_emotion_detection\",\n            next_component_params={\"text\": prompt, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_hf_llm/","title":"CompletedHFLLM","text":""},{"location":"Sources/API/orchestrator/chain/completed_hf_llm/#API.orchestrator.chain.completed_hf_llm.trigger_completed_hf_llm","title":"<code>trigger_completed_hf_llm(sender, **kwargs)</code>","text":"<p>This will create the response, which will be a text 2 text task We will create the ResText here</p> Source code in <code>API/orchestrator/chain/completed_hf_llm.py</code> <pre><code>@receiver(completed_hf_llm)\ndef trigger_completed_hf_llm(sender, **kwargs):  # noqa\n    \"\"\"\n    This will create the response, which will be a text 2 text task\n    We will create the ResText here\n    \"\"\"\n    try:\n        logger.info(\"HF LLM completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n\n        text = task_data.result_json[\"result_profile\"][\"text\"]\n        # grab the multi-modal conversation\n        data_multimodal_conversation_log_res_text(\n            task_data=task_data,\n            text=text,\n        )\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_hf_llm\",\n            next_component_params={\"text\": text, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_openai_gpt_35/","title":"CompletedOpenAIGPT35","text":""},{"location":"Sources/API/orchestrator/chain/completed_openai_gpt_35/#API.orchestrator.chain.completed_openai_gpt_35.trigger_completed_openai_gpt_35","title":"<code>trigger_completed_openai_gpt_35(sender, **kwargs)</code>","text":"<p>This will create the response, which will be a text 2 text task</p> Source code in <code>API/orchestrator/chain/completed_openai_gpt_35.py</code> <pre><code>@receiver(completed_openai_gpt_35)\ndef trigger_completed_openai_gpt_35(sender, **kwargs):  # noqa\n    \"\"\"\n    This will create the response, which will be a text 2 text task\n    \"\"\"\n    try:\n        logger.info(\"OpenAI GPT 35 LLM completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n\n        text = task_data.result_json[\"result_profile\"][\"text\"]\n        logger.info(text)\n        data_multimodal_conversation_log_res_text(\n            task_data=task_data,\n            text=text,\n        )\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_openai_gpt_35\",\n            next_component_params={\"text\": text, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_openai_gpt_4o_text_and_image/","title":"CompletedOpenAIGPT4oTextAndImage","text":""},{"location":"Sources/API/orchestrator/chain/completed_openai_gpt_4o_text_and_image/#API.orchestrator.chain.completed_openai_gpt_4o_text_and_image.trigger_completed_openai_gpt_4o_text_and_image","title":"<code>trigger_completed_openai_gpt_4o_text_and_image(sender, **kwargs)</code>","text":"<p>This will create the response, which will be a text 2 text task</p> Source code in <code>API/orchestrator/chain/completed_openai_gpt_4o_text_and_image.py</code> <pre><code>@receiver(completed_openai_gpt_4o_text_and_image)\ndef trigger_completed_openai_gpt_4o_text_and_image(sender, **kwargs):  # noqa\n    \"\"\"\n    This will create the response, which will be a text 2 text task\n    \"\"\"\n    try:\n        logger.info(\"OpenAI GPT 4o LLM completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n\n        text = task_data.result_json[\"result_profile\"][\"text\"]\n        logger.critical(text)\n        # grab the multi-modal conversation\n        data_multimodal_conversation_log_res_text(\n            task_data=task_data,\n            text=text,\n        )\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_openai_gpt_4o_text_and_image\",\n            next_component_params={\"text\": text, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_openai_gpt_4o_text_only/","title":"CompletedOpenAIGPT4oTextOnly","text":""},{"location":"Sources/API/orchestrator/chain/completed_openai_gpt_4o_text_only/#API.orchestrator.chain.completed_openai_gpt_4o_text_only.trigger_completed_openai_gpt_4o_text_only","title":"<code>trigger_completed_openai_gpt_4o_text_only(sender, **kwargs)</code>","text":"<p>This will create the response, which will be a text 2 text task</p> Source code in <code>API/orchestrator/chain/completed_openai_gpt_4o_text_only.py</code> <pre><code>@receiver(completed_openai_gpt_4o_text_only)\ndef trigger_completed_openai_gpt_4o_text_only(sender, **kwargs):  # noqa\n    \"\"\"\n    This will create the response, which will be a text 2 text task\n    \"\"\"\n    try:\n        logger.info(\"OpenAI GPT 4o LLM completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n\n        text = task_data.result_json[\"result_profile\"][\"text\"]\n        logger.info(text)\n        data_multimodal_conversation_log_res_text(\n            task_data=task_data,\n            text=text,\n        )\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_openai_gpt_4o_text_only\",\n            next_component_params={\"text\": text, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_openai_speech2text/","title":"CompletedOpenAISpeech2Text","text":""},{"location":"Sources/API/orchestrator/chain/completed_openai_speech2text/#API.orchestrator.chain.completed_openai_speech2text.trigger_completed_openai_speech2text","title":"<code>trigger_completed_openai_speech2text(sender, **kwargs)</code>","text":"<p>We will need to gather the text, and then grab the video to the next step</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <p>The sender of the signal</p> required <code>**kwargs</code> <p>The data passed to the signal</p> <code>{}</code> <p>Returns:</p> Source code in <code>API/orchestrator/chain/completed_openai_speech2text.py</code> <pre><code>@receiver(completed_openai_speech2text)\ndef trigger_completed_openai_speech2text(sender, **kwargs):\n    \"\"\"\n    We will need to gather the text, and then grab the video to the next step\n\n    Args:\n        sender: The sender of the signal\n        **kwargs: The data passed to the signal\n\n    Returns:\n\n    \"\"\"\n    logger.info(\"OpenAI Speech2Text completed triggerred\")\n    data = kwargs.get(\"data\", {})\n    track_id = kwargs.get(\"track_id\", None)\n    task_data = TaskData(**data)\n    params = task_data.parameters\n    logger.info(track_id)\n\n    # get the text\n    result_json = task_data.result_json\n    result_profile = result_json.get(\"result_profile\", {})\n    text = result_profile.get(\"text\", \"\")\n    logger.info(text)\n\n    # Currently GPT-4o can only take images, so we will try to locate the relevant images\n    uid = params.get(\"uid\")\n    home_id = params.get(\"home_id\")\n    audio_index = params.get(\"audio_index\")\n\n    audio = DataAudio.objects.filter(\n        home_id=home_id, uid=uid, sequence_index=audio_index\n    )\n\n    if not audio:\n        logger.error(\"Audio not found\")\n        return\n    if len(audio) &gt; 1:\n        logger.error(\"Multiple audio found\")\n        return\n    audio_obj = audio.first()\n\n    data_text_obj = DataText.objects.filter(audio=audio_obj).first()\n    if data_text_obj:\n        data_text_obj.text = text\n        data_text_obj.save()\n    else:\n        data_text_obj = DataText(\n            audio=audio_obj,\n            text=text,\n        )\n        data_text_obj.save()\n\n    if not hasattr(audio_obj, \"multi_modal_conversation\"):\n        DataMultiModalConversation.objects.create(\n            audio=audio_obj,\n        )\n    audio_obj.multi_modal_conversation.text = data_text_obj\n    audio_obj.multi_modal_conversation.save()\n\n    ClusterManager.chain_next(\n        track_id=track_id,\n        current_component=\"completed_openai_speech2text\",\n        next_component_params={\"sender\": data_text_obj, \"data\": data_text_obj.__dict__},\n        user=sender.user,\n    )\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_openai_text2speech/","title":"CompletedOpenAIText2Speech","text":""},{"location":"Sources/API/orchestrator/chain/completed_openai_text2speech/#API.orchestrator.chain.completed_openai_text2speech.trigger_completed_openai_text2speech","title":"<code>trigger_completed_openai_text2speech(sender, **kwargs)</code>","text":"<p>After the text2speech is done, save it to the database</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <p>The sender of the signal</p> required <code>kwargs</code> <p>The data passed to the signal</p> <code>{}</code> Source code in <code>API/orchestrator/chain/completed_openai_text2speech.py</code> <pre><code>@receiver(completed_openai_text2speech)\ndef trigger_completed_openai_text2speech(sender, **kwargs):\n    \"\"\"\n    After the text2speech is done, save it to the database\n\n    Args:\n        sender: The sender of the signal\n        kwargs: The data passed to the signal\n    \"\"\"\n    logger.info(\"OpenAI Text2Speech completed triggerred\")\n    try:\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n        # get the speech2text task based on the track_id\n        speech2text_task = (\n            Task.objects.filter(track_id=track_id, task_name=\"openai_text2speech\")\n            .order_by(\"-created_at\")\n            .first()\n        )\n        if speech2text_task is None:\n            logger.error(\"No speech2text task found\")\n            return\n        logger.info(speech2text_task.parameters)\n        text2speech_file = task_data.result_json[\"result_profile\"].get(\n            \"audio_file_path\", \"\"\n        )\n        ResSpeech.objects.create(text2speech_file=text2speech_file)\n\n        # this is the end of the chain\n        data_multimodal_conversation_log_res_speech(\n            task_data=task_data,\n            speech_file_path=text2speech_file,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_quantization_llm/","title":"CompletedQuantizationLLM","text":""},{"location":"Sources/API/orchestrator/chain/completed_quantization_llm/#API.orchestrator.chain.completed_quantization_llm.trigger_completed_quantization_llm","title":"<code>trigger_completed_quantization_llm(sender, **kwargs)</code>","text":"<p>This will create the response, which will be a text 2 text task And we will need to log this ResText</p> Source code in <code>API/orchestrator/chain/completed_quantization_llm.py</code> <pre><code>@receiver(completed_quantization_llm)\ndef trigger_completed_quantization_llm(sender, **kwargs):  # noqa\n    \"\"\"\n    This will create the response, which will be a text 2 text task\n    And we will need to log this ResText\n    \"\"\"\n    try:\n        logger.info(\"Quantization LLM completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n\n        text = task_data.result_json[\"result_profile\"][\"text\"]\n        # then we need to locate the conversation task\n        data_multimodal_conversation_log_res_text(\n            task_data=task_data,\n            text=text,\n        )\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_quantization_llm\",\n            next_component_params={\"text\": text, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_rag/","title":"CompletedRAG","text":""},{"location":"Sources/API/orchestrator/chain/completed_rag/#API.orchestrator.chain.completed_rag.trigger_completed_rag","title":"<code>trigger_completed_rag(sender, **kwargs)</code>","text":"<p>This will create the response, which will be a text 2 text task And we will need to log this ResText</p> Source code in <code>API/orchestrator/chain/completed_rag.py</code> <pre><code>@receiver(completed_rag)\ndef trigger_completed_rag(sender, **kwargs):  # noqa\n    \"\"\"\n    This will create the response, which will be a text 2 text task\n    And we will need to log this ResText\n    \"\"\"\n    try:\n        logger.info(\"RAG completed triggerred\")\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n\n        text = task_data.result_json[\"result_profile\"][\"text\"]\n        # then we need to locate the conversation task\n        data_multimodal_conversation_log_context_rag(\n            task_data=task_data,\n            result=task_data.result_json[\"result_profile\"],\n        )\n        data_text_id = task_data.parameters.get(\"data_text_id\", None)\n        ClusterManager.chain_next(\n            track_id=track_id,\n            current_component=\"completed_rag\",\n            next_component_params={\"text\": text, \"data_text_id\": data_text_id},\n            user=sender.user,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_speech2text/","title":"CompletedSpeech2Text","text":""},{"location":"Sources/API/orchestrator/chain/completed_speech2text/#API.orchestrator.chain.completed_speech2text.trigger_completed_speech2text","title":"<code>trigger_completed_speech2text(sender, **kwargs)</code>","text":"<p>After the speech2text is done, save it to the database</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <p>The sender of the signal</p> required <code>kwargs</code> <p>The data passed to the signal</p> <code>{}</code> Source code in <code>API/orchestrator/chain/completed_speech2text.py</code> <pre><code>@receiver(completed_speech2text)\ndef trigger_completed_speech2text(sender, **kwargs):\n    \"\"\"\n    After the speech2text is done, save it to the database\n\n    Args:\n        sender: The sender of the signal\n        kwargs: The data passed to the signal\n    \"\"\"\n    logger.info(\"Speech2Text completed triggerred\")\n    data = kwargs.get(\"data\", {})\n    track_id = kwargs.get(\"track_id\", None)\n    task_data = TaskData(**data)\n    params = task_data.parameters\n    logger.info(track_id)\n\n    uid = params.get(\"uid\")\n    home_id = params.get(\"home_id\")\n    audio_index = params.get(\"audio_index\")\n\n    audio = DataAudio.objects.filter(\n        home_id=home_id, uid=uid, sequence_index=audio_index\n    )\n    logger.debug(audio)\n    if not audio:\n        logger.error(\"Audio not found\")\n        return\n    if len(audio) &gt; 1:\n        logger.error(\"Multiple audio found\")\n        return\n    audio_obj = audio.first()\n\n    # save the data to the database\n    result_json = task_data.result_json\n    result_profile = result_json.get(\"result_profile\", {})\n    text = result_profile.get(\"text\", \"\")\n    logger.debug(result_json)\n    data_text_obj = DataText.objects.filter(audio=audio_obj).first()\n    if data_text_obj:\n        data_text_obj.text = text\n        data_text_obj.save()\n    else:\n        data_text_obj = DataText(\n            audio=audio_obj,\n            text=text,\n        )\n        data_text_obj.save()\n\n    audio_obj.multi_modal_conversation.text = data_text_obj\n    audio_obj.multi_modal_conversation.save()\n\n    ClusterManager.chain_next(\n        track_id=track_id,\n        current_component=\"completed_speech2text\",\n        next_component_params={\"sender\": data_text_obj, \"data\": data_text_obj.__dict__},\n        user=sender.user,\n    )\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_task/","title":"CompletedTask","text":""},{"location":"Sources/API/orchestrator/chain/completed_task/#API.orchestrator.chain.completed_task.trigger_completed_task","title":"<code>trigger_completed_task(sender, **kwargs)</code>","text":"<p>Trigger the multi-modal emotion detection.</p> Source code in <code>API/orchestrator/chain/completed_task.py</code> <pre><code>@receiver(completed_task)\ndef trigger_completed_task(sender, **kwargs):\n    \"\"\"\n    Trigger the multi-modal emotion detection.\n    \"\"\"\n    data = kwargs.get(\"data\", {})\n    task_data = TaskData(**data)\n\n    if task_data.task_name == \"speech2text\":\n        return completed_speech2text.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"emotion_detection\":\n        return completed_emotion_detection.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"quantization_llm\":\n        return completed_quantization_llm.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"text2speech\":\n        logger.info(\"Text2Speech task completed\")\n        return completed_text2speech.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"hf_llm\":\n        logger.info(\"HF LLM task completed\")\n        return completed_hf_llm.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"openai_speech2text\":\n        logger.info(\"OpenAI Speech2Text task completed\")\n        return completed_openai_speech2text.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"openai_gpt_4o_text_and_image\":\n        logger.info(\"OpenAI GPT4O task completed\")\n        return completed_openai_gpt_4o_text_and_image.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n    if task_data.task_name == \"openai_gpt_35\":\n        logger.info(\"OpenAI GPT3.5 task completed\")\n        return completed_openai_gpt_35.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"openai_gpt_4o_text_only\":\n        logger.info(\"OpenAI GPT4O Text Only task completed\")\n        return completed_openai_gpt_4o_text_only.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n    if task_data.task_name == \"rag\":\n        logger.info(\"RAG task completed\")\n        return completed_rag.send(sender=sender, data=data, track_id=task_data.track_id)\n\n    if task_data.task_name == \"openai_text2speech\":\n        logger.info(\"OpenAI Text2Speech task completed\")\n        return completed_openai_text2speech.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    task_name_choices = Task.get_task_name_choices()\n    task_name_choices_list = [task[0] for task in task_name_choices]\n    if task_data.task_name not in task_name_choices_list:\n        logger.error(\"Task name not found is not in the choices list\")\n        return\n    logger.critical(f\"{task_data.task_name} task completed, however, no action taken.\")\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/completed_text2speech/","title":"CompletedText2Speech","text":""},{"location":"Sources/API/orchestrator/chain/completed_text2speech/#API.orchestrator.chain.completed_text2speech.trigger_completed_text2speech","title":"<code>trigger_completed_text2speech(sender, **kwargs)</code>","text":"<p>After the text2speech is done, save it to the database</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <p>The sender of the signal</p> required <code>kwargs</code> <p>The data passed to the signal</p> <code>{}</code> Source code in <code>API/orchestrator/chain/completed_text2speech.py</code> <pre><code>@receiver(completed_text2speech)\ndef trigger_completed_text2speech(sender, **kwargs):\n    \"\"\"\n    After the text2speech is done, save it to the database\n\n    Args:\n        sender: The sender of the signal\n        kwargs: The data passed to the signal\n    \"\"\"\n    logger.info(\"Text2Speech completed triggerred\")\n    try:\n        data = kwargs.get(\"data\", {})\n        track_id = kwargs.get(\"track_id\", None)\n        logger.info(data)\n        task_data = TaskData(**data)\n\n        if track_id is None:\n            logger.error(\"No track_id found\")\n            return\n        # get the speech2text task based on the track_id\n        speech2text_task = (\n            Task.objects.filter(track_id=track_id, task_name=\"speech2text\")\n            .order_by(\"-created_at\")\n            .first()\n        )\n        if speech2text_task is None:\n            logger.error(\"No speech2text task found\")\n            return\n        text = speech2text_task.result_json[\"result_profile\"].get(\"text\", \"\")\n        logger.info(text)\n        logger.info(speech2text_task.parameters)\n        text2speech_file = task_data.result_json[\"result_profile\"].get(\n            \"audio_file_path\", \"\"\n        )\n        data_multimodal_conversation_log_res_speech(\n            task_data=task_data,\n            speech_file_path=text2speech_file,\n        )\n\n    except Exception as e:\n        logger.exception(e)\n        return\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/created_data_text/","title":"CreatedDataText","text":""},{"location":"Sources/API/orchestrator/chain/created_data_text/#API.orchestrator.chain.created_data_text.trigger_created_data_text","title":"<code>trigger_created_data_text(sender, **kwargs)</code>","text":"<p>This function will trigger the emotion detection model with the latest data</p> <p>It will first look for the latest data_text, and then get the audio and image data based on the time range of the audio data</p> <p>Parameters:</p> Name Type Description Default <code>sender</code> <p>The sender of the signal</p> required <code>kwargs</code> <p>The data passed to the signal</p> <code>{}</code> <p>Returns:</p> Source code in <code>API/orchestrator/chain/created_data_text.py</code> <pre><code>@receiver(created_data_text)\ndef trigger_created_data_text(sender, **kwargs):\n    \"\"\"\n    This function will trigger the emotion detection model with the latest data\n\n    It will first look for the latest data_text,\n    and then get the audio and image data based on the time range of the audio data\n\n    Args:\n        sender: The sender of the signal\n        kwargs: The data passed to the signal\n    Returns:\n\n    \"\"\"\n    data = kwargs.get(\"data\", {})\n    track_id = kwargs.get(\"track_id\", None)\n    data_text_id = data.get(\"id\", None)\n    logger.debug(track_id)\n    # get the audio data, which have not been process and have the text information\n    if data_text_id:\n        data_text = DataText.objects.get(id=data_text_id)\n    else:\n        logger.error(\"No data_text_id found\")\n        return None, None, None, None\n\n    text = data_text.text\n    data_audio = data_text.audio\n\n    audio_file = f\"audio/{data_audio.uid}/{data_audio.audio_file}\"\n\n    # get the image data based on the audio data time range\n    # TODO: this will be changed rapidly\n    start_time = data_audio.start_time\n    end_time = data_audio.end_time\n    # round the start to the minute level down\n    start_time = start_time.replace(second=0, microsecond=0)\n    # round the end to the minute level up\n    end_time = end_time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n    logger.info(f\"Start time: {start_time}, End time: {end_time}\")\n    logger.info(data_audio)\n    # we will assume it comes from the same device\n    # list all videos has overlap with [start_time, end_time]\n    # get start_time and end_time has overlap with [start_time, end_time]\n    videos_data = DataVideo.objects.filter(\n        Q(start_time__lt=end_time, end_time__gt=start_time)\n    )\n\n    data_audio.multi_modal_conversation.video.add(*videos_data)\n    data_audio.multi_modal_conversation.save()\n    images_path = []\n    for video_data in videos_data:\n        image_folder_name = video_data.video_file.split(\".\")[0].rsplit(\"-\", 1)[0]\n        images_path.append(f\"{video_data.uid}/frames/{image_folder_name}\")\n\n    # I need to read image files into List[np.ndarray]\n    images_path_list = []\n    for image_path in images_path:\n        # loop the path, get all images\n        folder = f\"videos/{image_path}\"\n        images_path_list.append(folder)\n\n    # trigger the model\n    logger.info(f\"Text: {text}, Audio: {audio_file}, Images: {len(images_path_list)}\")\n\n    task_params = {\n        \"text\": text,\n        \"audio_file\": audio_file,\n        \"images_path_list\": images_path_list,\n        \"data_text_id\": data_text.id,\n    }\n\n    user = kwargs.get(\"user\", None)\n    ClusterManager.chain_next(\n        track_id=track_id,\n        current_component=\"created_data_text\",\n        next_component_params=task_params,\n        user=user,\n    )\n\n    return text, [audio_file], images_path_list, data_text\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/manager/","title":"Manager","text":"<p>Here will define a list of clusters</p> <p>Each cluster will have a list of chain components</p> <p>For example, end-to-end conversation chain will have the following components:</p> <ul> <li>completed_speech2text</li> <li>created_data_text</li> <li>completed_emotion_detection</li> <li>completed_quantization_llm</li> <li>completed_text2speech</li> </ul>"},{"location":"Sources/API/orchestrator/chain/manager/#API.orchestrator.chain.manager.ClusterManager","title":"<code>ClusterManager</code>","text":"Source code in <code>API/orchestrator/chain/manager.py</code> <pre><code>class ClusterManager:\n\n    @staticmethod\n    def get_cluster(cluster_name: str):\n        \"\"\"\n        Get the cluster\n\n        Args:\n            cluster_name (str): The cluster name\n        \"\"\"\n        if cluster_name in CLUSTERS:\n            return CLUSTERS[cluster_name]\n        return None\n\n    @staticmethod\n    def get_next_chain_component(\n        cluster: dict, current_component: str\n    ) -&gt; Tuple[Optional[str], Optional[dict]]:\n        \"\"\"\n        Get the next chain\n\n        Args:\n            cluster (dict): The cluster\n            current_component (str): The current component\n\n        Return:\n            Tuple[Optional[str], Optional[dict]]: The next component and its parameters if exists, otherwise None\n        \"\"\"\n        chain = []\n        for key, value in cluster.items():\n            chain.append(key)\n        chain.sort(key=lambda x: cluster[x][\"order\"])\n        if current_component == \"init\":\n            \"\"\"\n            If this is the start of the chain, then return the first component\n            \"\"\"\n            return chain[0], cluster[chain[0]]\n        # index of the current component\n        current_component_index = chain.index(current_component)\n        next_index = current_component_index + 1\n        if next_index &gt;= len(chain):\n            return None, None\n        return chain[next_index], cluster[chain[next_index]]\n\n    @classmethod\n    def get_next(cls, cluster_name: str, current_component: str):\n        \"\"\"\n        Get the next component\n\n        Args:\n            cluster_name (str): The cluster name\n            current_component (str): The current component\n        \"\"\"\n        cluster = cls.get_cluster(cluster_name)\n        if cluster is None:\n            return None\n        return ClusterManager.get_next_chain_component(cluster, current_component)\n\n    @classmethod\n    def chain_next(\n        cls,\n        track_id: Optional[str],\n        current_component: str,\n        next_component_params: dict,\n        name: str = None,\n        user=None,\n    ):\n        \"\"\"\n        Chain to the next component\n\n        Args:\n            current_component (str): The current component\n            track_id (str): The track ID\n            next_component_params (dict): The next component parameters\n            name (str): The task name, it will be used to aggregate the task\n            user (None): The user\n        \"\"\"\n        logger.info(f\"Current component: {current_component}\")\n        logger.info(f\"Next component params: {next_component_params}\")\n        cluster_name = track_id.split(\"-\")[1]\n        next_component_name, next_component = cls.get_next(\n            cluster_name, current_component\n        )\n        logger.info(f\"Next component: {next_component_name}\")\n\n        if next_component_name is None:\n            return\n        # do something with the next component\n        # It can be a task or a signal\n        next_parameters = {\n            **next_component_params,\n            **next_component.get(\"extra_params\", {}),\n        }\n        logger.info(next_parameters)\n        logger.info(next_component_name)\n\n        if next_component[\"component_type\"] == \"task\":\n            task = Task.create_task(\n                user=user,\n                name=name or next_component[\"task_name\"],\n                task_name=next_component[\"task_name\"],\n                parameters=next_parameters,\n                track_id=track_id,\n            )\n            logger.info(f\"Task {task.id} created for {next_component['task_name']}\")\n            return task.id\n        elif next_component[\"component_type\"] == \"signal\":\n            if next_component_name == \"created_data_text\":\n                created_data_text.send(\n                    sender=next_component_params.get(\"sender\"),\n                    data=next_component_params.get(\"data\"),\n                    track_id=track_id,\n                    user=user,\n                )\n        return None\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/manager/#API.orchestrator.chain.manager.ClusterManager.chain_next","title":"<code>chain_next(track_id, current_component, next_component_params, name=None, user=None)</code>  <code>classmethod</code>","text":"<p>Chain to the next component</p> <p>Parameters:</p> Name Type Description Default <code>current_component</code> <code>str</code> <p>The current component</p> required <code>track_id</code> <code>str</code> <p>The track ID</p> required <code>next_component_params</code> <code>dict</code> <p>The next component parameters</p> required <code>name</code> <code>str</code> <p>The task name, it will be used to aggregate the task</p> <code>None</code> <code>user</code> <code>None</code> <p>The user</p> <code>None</code> Source code in <code>API/orchestrator/chain/manager.py</code> <pre><code>@classmethod\ndef chain_next(\n    cls,\n    track_id: Optional[str],\n    current_component: str,\n    next_component_params: dict,\n    name: str = None,\n    user=None,\n):\n    \"\"\"\n    Chain to the next component\n\n    Args:\n        current_component (str): The current component\n        track_id (str): The track ID\n        next_component_params (dict): The next component parameters\n        name (str): The task name, it will be used to aggregate the task\n        user (None): The user\n    \"\"\"\n    logger.info(f\"Current component: {current_component}\")\n    logger.info(f\"Next component params: {next_component_params}\")\n    cluster_name = track_id.split(\"-\")[1]\n    next_component_name, next_component = cls.get_next(\n        cluster_name, current_component\n    )\n    logger.info(f\"Next component: {next_component_name}\")\n\n    if next_component_name is None:\n        return\n    # do something with the next component\n    # It can be a task or a signal\n    next_parameters = {\n        **next_component_params,\n        **next_component.get(\"extra_params\", {}),\n    }\n    logger.info(next_parameters)\n    logger.info(next_component_name)\n\n    if next_component[\"component_type\"] == \"task\":\n        task = Task.create_task(\n            user=user,\n            name=name or next_component[\"task_name\"],\n            task_name=next_component[\"task_name\"],\n            parameters=next_parameters,\n            track_id=track_id,\n        )\n        logger.info(f\"Task {task.id} created for {next_component['task_name']}\")\n        return task.id\n    elif next_component[\"component_type\"] == \"signal\":\n        if next_component_name == \"created_data_text\":\n            created_data_text.send(\n                sender=next_component_params.get(\"sender\"),\n                data=next_component_params.get(\"data\"),\n                track_id=track_id,\n                user=user,\n            )\n    return None\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/manager/#API.orchestrator.chain.manager.ClusterManager.get_cluster","title":"<code>get_cluster(cluster_name)</code>  <code>staticmethod</code>","text":"<p>Get the cluster</p> <p>Parameters:</p> Name Type Description Default <code>cluster_name</code> <code>str</code> <p>The cluster name</p> required Source code in <code>API/orchestrator/chain/manager.py</code> <pre><code>@staticmethod\ndef get_cluster(cluster_name: str):\n    \"\"\"\n    Get the cluster\n\n    Args:\n        cluster_name (str): The cluster name\n    \"\"\"\n    if cluster_name in CLUSTERS:\n        return CLUSTERS[cluster_name]\n    return None\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/manager/#API.orchestrator.chain.manager.ClusterManager.get_next","title":"<code>get_next(cluster_name, current_component)</code>  <code>classmethod</code>","text":"<p>Get the next component</p> <p>Parameters:</p> Name Type Description Default <code>cluster_name</code> <code>str</code> <p>The cluster name</p> required <code>current_component</code> <code>str</code> <p>The current component</p> required Source code in <code>API/orchestrator/chain/manager.py</code> <pre><code>@classmethod\ndef get_next(cls, cluster_name: str, current_component: str):\n    \"\"\"\n    Get the next component\n\n    Args:\n        cluster_name (str): The cluster name\n        current_component (str): The current component\n    \"\"\"\n    cluster = cls.get_cluster(cluster_name)\n    if cluster is None:\n        return None\n    return ClusterManager.get_next_chain_component(cluster, current_component)\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/manager/#API.orchestrator.chain.manager.ClusterManager.get_next_chain_component","title":"<code>get_next_chain_component(cluster, current_component)</code>  <code>staticmethod</code>","text":"<p>Get the next chain</p> <p>Parameters:</p> Name Type Description Default <code>cluster</code> <code>dict</code> <p>The cluster</p> required <code>current_component</code> <code>str</code> <p>The current component</p> required Return <p>Tuple[Optional[str], Optional[dict]]: The next component and its parameters if exists, otherwise None</p> Source code in <code>API/orchestrator/chain/manager.py</code> <pre><code>@staticmethod\ndef get_next_chain_component(\n    cluster: dict, current_component: str\n) -&gt; Tuple[Optional[str], Optional[dict]]:\n    \"\"\"\n    Get the next chain\n\n    Args:\n        cluster (dict): The cluster\n        current_component (str): The current component\n\n    Return:\n        Tuple[Optional[str], Optional[dict]]: The next component and its parameters if exists, otherwise None\n    \"\"\"\n    chain = []\n    for key, value in cluster.items():\n        chain.append(key)\n    chain.sort(key=lambda x: cluster[x][\"order\"])\n    if current_component == \"init\":\n        \"\"\"\n        If this is the start of the chain, then return the first component\n        \"\"\"\n        return chain[0], cluster[chain[0]]\n    # index of the current component\n    current_component_index = chain.index(current_component)\n    next_index = current_component_index + 1\n    if next_index &gt;= len(chain):\n        return None, None\n    return chain[next_index], cluster[chain[next_index]]\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/models/","title":"Models","text":""},{"location":"Sources/API/orchestrator/chain/signals/","title":"Signals","text":""},{"location":"Sources/API/orchestrator/chain/utils/","title":"Utils","text":""},{"location":"Sources/API/orchestrator/chain/utils/#API.orchestrator.chain.utils.data_multimodal_conversation_log_context_emotion_detection","title":"<code>data_multimodal_conversation_log_context_emotion_detection(task_data, result, logs=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task_data</code> <code>TaskData</code> <p>the task data</p> required <code>result</code> <code>dict</code> <p>the result of the context emotion detection</p> required <code>logs</code> <code>dict</code> <p>the logs of the context emotion detection</p> <code>None</code> <p>Returns:</p> Source code in <code>API/orchestrator/chain/utils.py</code> <pre><code>def data_multimodal_conversation_log_context_emotion_detection(\n    task_data: TaskData, result: dict, logs: dict = None\n):\n    \"\"\"\n\n    Args:\n        task_data (TaskData): the task data\n        result (dict): the result of the context emotion detection\n        logs (dict): the logs of the context emotion detection\n\n    Returns:\n\n    \"\"\"\n    data_text_id = task_data.parameters.get(\"data_text_id\", None)\n    if data_text_id is not None:\n        data_text = DataText.objects.filter(id=data_text_id).first()\n        if data_text is not None and hasattr(data_text, \"multi_modal_conversation\"):\n            emotion = ContextEmotionDetection(\n                multi_modal_conversation=data_text.multi_modal_conversation,\n                result=result,\n                logs=logs,\n            )\n            emotion.save()\n            logger.info(emotion)\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/utils/#API.orchestrator.chain.utils.data_multimodal_conversation_log_context_rag","title":"<code>data_multimodal_conversation_log_context_rag(task_data, result, logs=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task_data</code> <code>TaskData</code> <p>the task data</p> required <code>result</code> <code>dict</code> <p>the result of the context rag</p> required <code>logs</code> <code>dict</code> <p>the logs of the context rag</p> <code>None</code> <p>Returns:</p> Source code in <code>API/orchestrator/chain/utils.py</code> <pre><code>def data_multimodal_conversation_log_context_rag(\n    task_data: TaskData, result: dict, logs: dict = None\n):\n    \"\"\"\n\n    Args:\n        task_data (TaskData): the task data\n        result (dict): the result of the context rag\n        logs (dict): the logs of the context rag\n\n    Returns:\n\n    \"\"\"\n    data_text_id = task_data.parameters.get(\"data_text_id\", None)\n    if data_text_id is not None:\n        data_text = DataText.objects.filter(id=data_text_id).first()\n        if data_text is not None and hasattr(data_text, \"multi_modal_conversation\"):\n            rag = ContextRAG(\n                multi_modal_conversation=data_text.multi_modal_conversation,\n                result=result,\n                logs=logs,\n            )\n            rag.save()\n            logger.info(rag)\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/utils/#API.orchestrator.chain.utils.data_multimodal_conversation_log_res_speech","title":"<code>data_multimodal_conversation_log_res_speech(task_data, speech_file_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task_data</code> <code>TaskData</code> <p>the task data</p> required <code>speech_file_path</code> <code>str</code> <p>the speech file path</p> required <p>Returns:</p> Source code in <code>API/orchestrator/chain/utils.py</code> <pre><code>def data_multimodal_conversation_log_res_speech(\n    task_data: TaskData, speech_file_path: str\n):\n    \"\"\"\n\n    Args:\n        task_data (TaskData): the task data\n        speech_file_path (str): the speech file path\n\n    Returns:\n\n    \"\"\"\n    res_speech = ResSpeech.objects.create(text2speech_file=speech_file_path)\n    data_text_id = task_data.parameters.get(\"data_text_id\", None)\n    if data_text_id is not None:\n        data_text = DataText.objects.filter(id=data_text_id).first()\n        if data_text is not None and hasattr(data_text, \"multi_modal_conversation\"):\n            data_text.multi_modal_conversation.res_speech = res_speech\n            data_text.multi_modal_conversation.save()\n</code></pre>"},{"location":"Sources/API/orchestrator/chain/utils/#API.orchestrator.chain.utils.data_multimodal_conversation_log_res_text","title":"<code>data_multimodal_conversation_log_res_text(task_data, text)</code>","text":"<p>Log the ResText to the DataMultiModalConversation</p> <p>Parameters:</p> Name Type Description Default <code>task_data</code> <code>TaskData</code> <p>The task data</p> required <code>text</code> <code>str</code> <p>The text to log</p> required Source code in <code>API/orchestrator/chain/utils.py</code> <pre><code>def data_multimodal_conversation_log_res_text(task_data: TaskData, text: str):\n    \"\"\"\n    Log the ResText to the DataMultiModalConversation\n\n    Args:\n        task_data (TaskData): The task data\n        text (str): The text to log\n    \"\"\"\n    res_text = ResText.objects.create(text=text)\n    data_text_id = task_data.parameters.get(\"data_text_id\", None)\n    if data_text_id is not None:\n        data_text = DataText.objects.filter(id=data_text_id).first()\n        if data_text is not None and hasattr(data_text, \"multi_modal_conversation\"):\n            data_text.multi_modal_conversation.res_text = res_text\n            data_text.multi_modal_conversation.save()\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/","title":"AccuracyBenchmark","text":""},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark","title":"<code>AccuracyBenchmark</code>","text":"Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>class AccuracyBenchmark:\n    def __init__(self, benchmark_cluster: str = CLUSTER_Q_ETE_CONVERSATION_NAME):\n        \"\"\"\n        Initialize the benchmark\n        Args:\n            benchmark_cluster (str): The benchmark cluster\n        \"\"\"\n        # if it is a specific name, gather this metric, otherwise, report all existing cluster\n        self.benchmark_cluster = benchmark_cluster\n\n    def benchmark_run(self):\n        \"\"\"\n        Run the benchmark\n        \"\"\"\n        logger.info(f\"Running accuracy benchmark for cluster {self.benchmark_cluster}\")\n        # run the benchmark\n        html_content = \"\"\n        if self.benchmark_cluster == \"all\":\n            for cluster_name in CLUSTERS.keys():\n                html_content += \"&lt;hr&gt;\"\n                html_content += self.process_cluster_benchmark(\n                    cluster_name, detailed=False\n                )\n        else:\n            html_content += self.process_cluster_benchmark(\n                self.benchmark_cluster, detailed=False\n            )\n        return html_content\n\n    def process_cluster_benchmark(\n        self, cluster_name: str, detailed: bool = False\n    ) -&gt; str:\n        \"\"\"\n        Process the benchmark for a specific cluster\n\n        For each cluster, we will need to analyse the conversation model\n        And also need to understand what's the else model we need to analyse, for example the emotion_detection\n        Args:\n             cluster_name (str): The cluster name\n            detailed (bool): The detailed flag\n\n        Returns:\n            str: The HTML content\n        \"\"\"\n        task_groups, required_tasks_count, tasks = extract_task_group(cluster_name)\n\n        required_annotation_task = self.extract_required_annotation_models(cluster_name)\n        logger.info(\n            f\"Cluster: {cluster_name}, Required annotation tasks: {required_annotation_task}\"\n        )\n        conversations = DataMultiModalConversation.objects.filter(\n            track_id__startswith=f\"T-{cluster_name}-\"\n        ).order_by(\"-created_at\")\n\n        html_content = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n        html_content += (\n            f\"&lt;p&gt;Required tasks each group: {required_tasks_count} \uff5c \"\n            f\"Annotation task groups: {len(conversations)}&lt;/p&gt;\"\n        )\n\n        # the emotion and other context results also will be pulled from this one\n        # then we will according to this to load the annotation results\n        # track id and annotation =&gt; flatten the results\n        annotations = []\n        annotation_expected_keys = MultiModalAnnotationForm.declared_fields.keys()\n        annotation_pending_default = {\n            key: \"pending\" for key in annotation_expected_keys\n        }\n\n        for conversation in conversations:\n            conversation_annotation = conversation.annotations\n            annotated = False\n            for user_id, annotation in conversation_annotation.items():\n                annotations.append(\n                    {\n                        \"track_id\": conversation.track_id,\n                        \"user_id\": user_id,\n                        \"predict_text\": conversation.text.text,\n                        **annotation_pending_default,\n                        **annotation,\n                    }\n                )\n                annotated = True\n            if not annotated:\n                annotations.append(\n                    {\n                        \"track_id\": conversation.track_id,\n                        \"user_id\": \"missing\",\n                        \"predict_text\": \"\",\n                        **annotation_pending_default,\n                    }\n                )\n\n        conversation_annotation_df = pd.DataFrame(annotations)\n        if len(conversation_annotation_df) == 0:\n            return html_content + \"&lt;p&gt;No conversation annotation found&lt;/p&gt;\"\n        # transform the track_id to be the last part\n        conversation_annotation_df[\"track_id\"] = (\n            conversation_annotation_df[\"track_id\"].str.split(\"-\").str[-1]\n        )\n        # replace all the column names, remove the annotation prefix\n        conversation_annotation_df.columns = [\n            col.replace(\"annotation_\", \"\") for col in conversation_annotation_df.columns\n        ]\n        # add CER and WER\n        conversation_annotation_df = self.calculate_speech2text_accuracy(\n            conversation_annotation_df\n        )\n\n        if detailed:\n            # then we will present them into multiple tables: speech2text, text_generation, text2speech, overall\n            if \"speech2text\" in required_annotation_task:\n                speech2text_df = conversation_annotation_df[\n                    [\n                        \"track_id\",\n                        \"user_id\",\n                        \"predict_text\",\n                        \"speech2text\",\n                        \"wer\",\n                        \"cer\",\n                        \"speech2text_score\",\n                    ]\n                ].copy(deep=True)\n                html_content += self.plot_table(speech2text_df, \"Speech2Text\")\n            if \"text_generation\" in required_annotation_task:\n                text_generation_df = conversation_annotation_df[\n                    [\"track_id\", \"user_id\", \"text_generation\", \"text_generation_score\"]\n                ].copy(deep=True)\n                html_content += self.plot_table(text_generation_df, \"Text Generation\")\n            if \"text2speech\" in required_annotation_task:\n                text2speech_df = conversation_annotation_df[\n                    [\"track_id\", \"user_id\", \"text2speech_score\"]\n                ].copy(deep=True)\n                html_content += self.plot_table(text2speech_df, \"Text2Speech\")\n\n            overall_conversation_df = conversation_annotation_df[\n                [\"track_id\", \"user_id\", \"overall_comment\", \"overall_score\"]\n            ].copy(deep=True)\n            html_content += self.plot_table(\n                overall_conversation_df, \"Overall Conversation Quality\"\n            )\n        else:\n            #\n            # then we will try to calculate the overall accuracy for each annotation task\n            conversation_annotation_df = self.annotation_average(\n                df=conversation_annotation_df\n            )\n            if \"speech2text\" in required_annotation_task:\n                desc_df = self.summary_df(\n                    conversation_annotation_df[\n                        [\"track_id\", \"wer\", \"cer\", \"speech2text_score\"]\n                    ].copy(deep=True)\n                )\n                html_content += self.plot_table(desc_df, \"Speech2Text Overall Quality\")\n                html_content += self.plot_distribution(\n                    conversation_annotation_df[\n                        [\"track_id\", \"wer\", \"cer\", \"speech2text_score\"]\n                    ].copy(deep=True),\n                    \"Speech2Text\",\n                )\n            if \"text_generation\" in required_annotation_task:\n                desc_df = self.summary_df(\n                    conversation_annotation_df[\n                        [\"track_id\", \"text_generation_score\"]\n                    ].copy(deep=True)\n                )\n                html_content += self.plot_table(\n                    desc_df, \"Text Generation Overall Quality\"\n                )\n                html_content += self.plot_distribution(\n                    conversation_annotation_df[\n                        [\"track_id\", \"text_generation_score\"]\n                    ].copy(deep=True),\n                    \"Text Generation\",\n                )\n\n            if \"text2speech\" in required_annotation_task:\n                desc_df = self.summary_df(\n                    conversation_annotation_df[[\"track_id\", \"text2speech_score\"]].copy(\n                        deep=True\n                    )\n                )\n                html_content += self.plot_table(desc_df, \"Text2Speech Overall Quality\")\n                html_content += self.plot_distribution(\n                    conversation_annotation_df[[\"track_id\", \"text2speech_score\"]].copy(\n                        deep=True\n                    ),\n                    \"Text2Speech\",\n                )\n\n        # summary the emotion detection task\n        if \"emotion_detection\" in required_annotation_task:\n            # load the emotion detection results\n            emotion_detection_results = ContextEmotionDetection.objects.filter(\n                multi_modal_conversation__in=conversations\n            ).order_by(\"-created_at\")\n            if len(emotion_detection_results) == 0:\n                return html_content + \"&lt;h4&gt;No emotion detection results found&lt;/h4&gt;\"\n\n            emotion_detection_expected_keys = (\n                MultiModalFKEmotionDetectionAnnotationForm.declared_fields.keys()\n            )\n            emotion_detection_pending_default = {\n                key: \"pending\" for key in emotion_detection_expected_keys\n            }\n            emotion_detection_annotations = []\n            for emotion_detection in emotion_detection_results:\n                emotion_detection_annotation = emotion_detection.annotations\n                annotated = False\n                for user_id, annotation in emotion_detection_annotation.items():\n                    emotion_detection_annotations.append(\n                        {\n                            \"track_id\": emotion_detection.multi_modal_conversation.track_id,\n                            \"user_id\": user_id,\n                            **emotion_detection_pending_default,\n                            **annotation,\n                        }\n                    )\n                    annotated = True\n                if not annotated:\n                    emotion_detection_annotations.append(\n                        {\n                            \"track_id\": emotion_detection.multi_modal_conversation.track_id,\n                            \"user_id\": \"missing\",\n                            **emotion_detection_pending_default,\n                        }\n                    )\n\n            emotion_detection_df = pd.DataFrame(emotion_detection_annotations)\n            logger.info(emotion_detection_df)\n            emotion_detection_df[\"track_id\"] = (\n                emotion_detection_df[\"track_id\"].str.split(\"-\").str[-1]\n            )\n            emotion_detection_df.columns = [\n                col.replace(\"annotation_\", \"\") for col in emotion_detection_df.columns\n            ]\n            if detailed:\n                html_content += self.plot_table(\n                    emotion_detection_df, \"Emotion Detection\"\n                )\n\n            else:\n                emotion_detection_df = self.annotation_average(emotion_detection_df)\n                desc_df = self.summary_df(emotion_detection_df)\n                # logger.info(desc_df)\n                html_content += self.plot_table(desc_df, \"Emotion Detection\")\n                html_content += self.plot_distribution(\n                    emotion_detection_df, \"Emotion Detection\"\n                )\n        return html_content\n\n    @staticmethod\n    def plot_table(df: pd.DataFrame, title: str = \"\") -&gt; str:\n        \"\"\"\n        Plot the table\n        Args:\n            df (pd.DataFrame): The dataframe\n            title (str): The title\n\n        Returns:\n            str: The plot in HTML\n        \"\"\"\n        colors = []\n        for col in df.columns:\n            col_colors = []\n            for val in df[col]:\n                if isinstance(val, float) or isinstance(val, int):\n                    col_colors.append(\"lavender\")\n                else:\n                    if val == \"missing\":\n                        col_colors.append(\"lightcoral\")\n                    elif val == \"started\":\n                        col_colors.append(\"lightyellow\")\n                    elif val == \"failed\":\n                        col_colors.append(\"lightcoral\")\n                    elif val == \"pending\":\n                        col_colors.append(\"lightblue\")\n                    elif val == \"incomplete\":\n                        col_colors.append(\"lightgrey\")\n                    else:\n                        col_colors.append(\"lightgreen\")\n            colors.append(col_colors)\n        # Create a Plotly table\n        fig = go.Figure(\n            data=[\n                go.Table(\n                    header=dict(\n                        values=[\n                            (\n                                [f\"&lt;b&gt;{c.upper()}&lt;/b&gt;\" for c in col]\n                                if isinstance(col, tuple)\n                                else f\"&lt;b&gt;{col.upper()}&lt;/b&gt;\"\n                            )\n                            for col in df.columns\n                        ],\n                        fill_color=\"paleturquoise\",\n                        align=\"left\",\n                    ),\n                    cells=dict(\n                        values=[df[col] for col in df.columns],\n                        fill_color=colors,\n                        align=\"left\",\n                    ),\n                )\n            ]\n        )\n        fig.update_layout(\n            title={\n                \"text\": f\"Accuracy: {title}\",\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"yanchor\": \"top\",\n            },\n            #     update margin to be 0\n            margin=dict(l=10, r=10, b=0),\n            # get the height to be whatever it requires\n            height=max((len(df) * 35), 400),\n        )\n        # Update layout for better appearance\n        desc_html = fig.to_html(full_html=False)\n        return desc_html\n\n    @staticmethod\n    def plot_distribution(df: pd.DataFrame, title: str = \"\") -&gt; str:\n        \"\"\"\n        Plot the distribution of the latency\n        Args:\n            df (pd.DataFrame): The dataframe\n            title (str): The title\n\n        Returns:\n            str: The plot in HTML\n        \"\"\"\n        # plot the distribution for each column\n        # Calculate mean and max for each latency column\n\n        mean_accuracies = df[df.columns[1:]].mean()\n        max_accuracies = df[df.columns[1:]].max()\n        min_accuracies = df[df.columns[1:]].min()\n\n        # Create a Plotly figure\n        fig = go.Figure()\n        # Add min latencies to the figure\n        fig.add_trace(\n            go.Bar(x=min_accuracies.index, y=min_accuracies.values, name=\"Min Accuracy\")\n        )\n        # Add mean latencies to the figure\n        fig.add_trace(\n            go.Bar(\n                x=mean_accuracies.index, y=mean_accuracies.values, name=\"Mean Accuracy\"\n            )\n        )\n\n        # Add max latencies to the figure\n        fig.add_trace(\n            go.Bar(x=max_accuracies.index, y=max_accuracies.values, name=\"Max Accuracy\")\n        )\n\n        # Customize the layout\n        fig.update_layout(\n            title={\n                \"text\": \"Accuracy Distribution\" + title,\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"yanchor\": \"top\",\n            },\n            xaxis_title=\"Evaluation Metrics\",\n            yaxis_title=\"Accuracies\",\n            barmode=\"group\",\n            margin=dict(l=10, r=10, b=0),\n        )\n\n        # Convert Plotly figure to HTML\n        plot_html = fig.to_html(full_html=False)\n        return plot_html\n\n    @staticmethod\n    def extract_required_annotation_models(cluster_name: str) -&gt; List[str]:\n        \"\"\"\n        Extract the required annotation models\n        Args:\n            cluster_name (str): The cluster name\n        \"\"\"\n        cluster = CLUSTERS.get(cluster_name, None)\n        if cluster is None:\n            raise ValueError(f\"Cluster {cluster_name} not found\")\n\n        # candidate included: speech2text, text_generation, text2speech, this normally is required\n        # other include emotion_detection now\n        required_annotation_task = []\n        for item in cluster.values():\n            if item[\"component_type\"] == \"task\":\n                task_name = item[\"task_name\"]\n                required_annotation_task.append(\n                    Task.task_ml_task_mapping().get(task_name, None)\n                )\n\n        # filter out None\n        required_annotation_task = list(filter(None, required_annotation_task))\n        # remove the duplicate\n        return list(set(required_annotation_task))\n\n    @staticmethod\n    def calculate_speech2text_accuracy(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the speech2text accuracy\n        Args:\n            df (pd.DataFrame): The dataframe\n\n        Returns:\n            float: The accuracy\n        \"\"\"\n        # both predict_text and speech2text can be null\n        # if the predict_text is null, then we will consider it as 0\n        # if the speech2text is null, then we will consider it as 0\n        df[\"speech2text\"] = df[\"speech2text\"].fillna(\"\")\n        df[\"predict_text\"] = df[\"predict_text\"].fillna(\"\")\n        # calculate the accuracy\n        df[\"wer\"] = df.apply(\n            lambda x: (\n                round(\n                    jiwer.wer(\n                        x[\"speech2text\"],\n                        x[\"predict_text\"],\n                    ),\n                    2,\n                )\n                if len(x[\"speech2text\"]) &gt; 0\n                else 0\n            ),\n            axis=1,\n        )\n\n        df[\"cer\"] = df.apply(\n            lambda x: (\n                round(\n                    jiwer.cer(\n                        x[\"speech2text\"],\n                        x[\"predict_text\"],\n                    ),\n                    2,\n                )\n                if len(x[\"speech2text\"]) &gt; 0\n                else 0\n            ),\n            axis=1,\n        )\n\n        return df\n\n    @staticmethod\n    def annotation_average(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the average of the annotation\n        Args:\n            df (pd.DataFrame): The dataframe\n\n        Returns:\n            pd.DataFrame: The dataframe\n        \"\"\"\n        # for each row, if the value is missing or pending, remove the row\n        # then calculate the average for each track_id\n        df = df.replace(\"missing\", pd.NA)\n        df = df.replace(\"pending\", pd.NA)\n        df = df.dropna(subset=df.columns[2:], how=\"any\")\n        # try to get all columns to float, if not possible, then keep it as it is\n        # loop the columns, try to get it to float\n        for col in df.columns[2:]:\n            try:\n                df[col] = df[col].astype(float)\n            except ValueError:\n                pass\n        numeric_columns = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n        df_mean = df.groupby(\"track_id\")[numeric_columns].mean().reset_index()\n        return df_mean\n\n    @staticmethod\n    def summary_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Summary the given dataframe\n\n        Args:\n            df (pd.DataFrame): The dataframe\n\n        Returns:\n            str: The HTML content\n        \"\"\"\n        # for the same track_id, aggregate the results into one, and use the mean as the final result\n        # df = df.apply(pd.to_numeric, errors='coerce')\n\n        # Group by 'track_id' and calculate the mean for each group\n        # df = df.groupby(\"track_id\").mean().reset_index()\n        desc_df = df.describe().transpose()\n        desc_df = desc_df.reset_index()\n        desc_df.rename(columns={\"index\": \"metric\"}, inplace=True)\n        desc_df = desc_df.round(4)\n        return desc_df\n\n    def detail_run(self):\n        logger.info(f\"Running accuracy benchmark for cluster {self.benchmark_cluster}\")\n        # run the benchmark\n        html_content = \"\"\n        if self.benchmark_cluster == \"all\":\n            for cluster_name in CLUSTERS.keys():\n                html_content += \"&lt;hr&gt;\"\n                html_content += self.process_cluster_benchmark(\n                    cluster_name, detailed=True\n                )\n        else:\n            html_content += self.process_cluster_benchmark(\n                self.benchmark_cluster, detailed=True\n            )\n        return html_content\n\n    def multi_turn_benchmark_run(self):\n        \"\"\"\n        Run the multi-turn benchmark\n        Returns:\n\n        \"\"\"\n        logger.info(\n            f\"Running multi-turn benchmark for cluster {self.benchmark_cluster}\"\n        )\n        # run the benchmark\n        html_content = \"\"\n        if self.benchmark_cluster == \"all\":\n            for cluster_name in CLUSTERS.keys():\n                html_content += \"&lt;hr&gt;\"\n                html_content += self.process_multi_turn_benchmark(cluster_name)\n        else:\n            html_content += self.process_multi_turn_benchmark(self.benchmark_cluster)\n        return html_content\n\n    def process_multi_turn_benchmark(self, cluster_name: str) -&gt; str:\n        \"\"\"\n        Process the multi-turn benchmark\n\n        First we will need to get all tag with this cluster name, and grab the last one within each tag\n        Args:\n            cluster_name (str): The cluster name\n        Returns:\n\n        \"\"\"\n        conversations = DataMultiModalConversation.objects.filter(\n            track_id__startswith=f\"T-{cluster_name}-\"\n        )\n\n        # grab all tags\n\n        tags = []\n        for conversation in conversations:\n            for tag in conversation.tags.all():\n                tags.append(tag.name)\n\n        tags = list(set(tags))\n\n        tag_last_conversations = []\n        for tag in tags:\n            last_conversation = (\n                conversations.filter(tags__name=tag).order_by(\"-created_at\").first()\n            )\n            tag_last_conversations.append(last_conversation)\n\n        html_content = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n        html_content += (\n            f\"&lt;p&gt;Multi-Turn Conversation Count: {len(tag_last_conversations)}&lt;/p&gt;\"\n        )\n        # then we will need to analyse the conversation model\n        multi_turn_annotations = []\n        # get all possible keys from the annotation form\n        multi_turn_annotations_expected_keys = []\n        for conversation in tag_last_conversations:\n            conversation_annotation = conversation.multi_turns_annotations\n            for user_id, annotation in conversation_annotation.items():\n                multi_turn_annotations_expected_keys.extend(annotation.keys())\n        multi_turn_annotations_expected_keys = list(\n            set(multi_turn_annotations_expected_keys)\n        )\n\n        if \"multi_turn_annotation_overall\" not in multi_turn_annotations_expected_keys:\n            multi_turn_annotations_expected_keys.append(\"multi_turn_annotation_overall\")\n        if (\n            \"multi_turn_annotation_overall_comment\"\n            not in multi_turn_annotations_expected_keys\n        ):\n            multi_turn_annotations_expected_keys.append(\n                \"multi_turn_annotation_overall_comment\"\n            )\n\n        multi_turn_annotations_pending_default = {\n            key: \"pending\" for key in multi_turn_annotations_expected_keys\n        }\n\n        for conversation in tag_last_conversations:\n            conversation_annotation = conversation.multi_turns_annotations\n            annotated = False\n            for user_id, annotation in conversation_annotation.items():\n                multi_turn_annotations.append(\n                    {\n                        \"track_id\": conversation.track_id,\n                        \"user_id\": user_id,\n                        **multi_turn_annotations_pending_default,\n                        **annotation,\n                    }\n                )\n                annotated = True\n            if not annotated:\n                multi_turn_annotations.append(\n                    {\n                        \"track_id\": conversation.track_id,\n                        \"user_id\": \"missing\",\n                        **multi_turn_annotations_pending_default,\n                    }\n                )\n\n        multi_turn_annotation_df = pd.DataFrame(multi_turn_annotations)\n\n        if len(multi_turn_annotation_df) == 0:\n            return html_content + \"&lt;p&gt;No multi-turn conversation annotation found&lt;/p&gt;\"\n        # transform the track_id to be the last part\n        multi_turn_annotation_df[\"track_id\"] = (\n            multi_turn_annotation_df[\"track_id\"].str.split(\"-\").str[-1]\n        )\n        # replace all the column names, remove the annotation prefix\n        multi_turn_annotation_df.columns = [\n            col.replace(\"multi_turn_annotation_\", \"\")\n            for col in multi_turn_annotation_df.columns\n        ]\n\n        html_content += self.plot_table(\n            multi_turn_annotation_df, \"Multi-Turn Conversation\"\n        )\n        return html_content\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.__init__","title":"<code>__init__(benchmark_cluster=CLUSTER_Q_ETE_CONVERSATION_NAME)</code>","text":"<p>Initialize the benchmark Args:     benchmark_cluster (str): The benchmark cluster</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>def __init__(self, benchmark_cluster: str = CLUSTER_Q_ETE_CONVERSATION_NAME):\n    \"\"\"\n    Initialize the benchmark\n    Args:\n        benchmark_cluster (str): The benchmark cluster\n    \"\"\"\n    # if it is a specific name, gather this metric, otherwise, report all existing cluster\n    self.benchmark_cluster = benchmark_cluster\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.annotation_average","title":"<code>annotation_average(df)</code>  <code>staticmethod</code>","text":"<p>Calculate the average of the annotation Args:     df (pd.DataFrame): The dataframe</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The dataframe</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>@staticmethod\ndef annotation_average(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the average of the annotation\n    Args:\n        df (pd.DataFrame): The dataframe\n\n    Returns:\n        pd.DataFrame: The dataframe\n    \"\"\"\n    # for each row, if the value is missing or pending, remove the row\n    # then calculate the average for each track_id\n    df = df.replace(\"missing\", pd.NA)\n    df = df.replace(\"pending\", pd.NA)\n    df = df.dropna(subset=df.columns[2:], how=\"any\")\n    # try to get all columns to float, if not possible, then keep it as it is\n    # loop the columns, try to get it to float\n    for col in df.columns[2:]:\n        try:\n            df[col] = df[col].astype(float)\n        except ValueError:\n            pass\n    numeric_columns = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n    df_mean = df.groupby(\"track_id\")[numeric_columns].mean().reset_index()\n    return df_mean\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.benchmark_run","title":"<code>benchmark_run()</code>","text":"<p>Run the benchmark</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>def benchmark_run(self):\n    \"\"\"\n    Run the benchmark\n    \"\"\"\n    logger.info(f\"Running accuracy benchmark for cluster {self.benchmark_cluster}\")\n    # run the benchmark\n    html_content = \"\"\n    if self.benchmark_cluster == \"all\":\n        for cluster_name in CLUSTERS.keys():\n            html_content += \"&lt;hr&gt;\"\n            html_content += self.process_cluster_benchmark(\n                cluster_name, detailed=False\n            )\n    else:\n        html_content += self.process_cluster_benchmark(\n            self.benchmark_cluster, detailed=False\n        )\n    return html_content\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.calculate_speech2text_accuracy","title":"<code>calculate_speech2text_accuracy(df)</code>  <code>staticmethod</code>","text":"<p>Calculate the speech2text accuracy Args:     df (pd.DataFrame): The dataframe</p> <p>Returns:</p> Name Type Description <code>float</code> <code>DataFrame</code> <p>The accuracy</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>@staticmethod\ndef calculate_speech2text_accuracy(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the speech2text accuracy\n    Args:\n        df (pd.DataFrame): The dataframe\n\n    Returns:\n        float: The accuracy\n    \"\"\"\n    # both predict_text and speech2text can be null\n    # if the predict_text is null, then we will consider it as 0\n    # if the speech2text is null, then we will consider it as 0\n    df[\"speech2text\"] = df[\"speech2text\"].fillna(\"\")\n    df[\"predict_text\"] = df[\"predict_text\"].fillna(\"\")\n    # calculate the accuracy\n    df[\"wer\"] = df.apply(\n        lambda x: (\n            round(\n                jiwer.wer(\n                    x[\"speech2text\"],\n                    x[\"predict_text\"],\n                ),\n                2,\n            )\n            if len(x[\"speech2text\"]) &gt; 0\n            else 0\n        ),\n        axis=1,\n    )\n\n    df[\"cer\"] = df.apply(\n        lambda x: (\n            round(\n                jiwer.cer(\n                    x[\"speech2text\"],\n                    x[\"predict_text\"],\n                ),\n                2,\n            )\n            if len(x[\"speech2text\"]) &gt; 0\n            else 0\n        ),\n        axis=1,\n    )\n\n    return df\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.extract_required_annotation_models","title":"<code>extract_required_annotation_models(cluster_name)</code>  <code>staticmethod</code>","text":"<p>Extract the required annotation models Args:     cluster_name (str): The cluster name</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>@staticmethod\ndef extract_required_annotation_models(cluster_name: str) -&gt; List[str]:\n    \"\"\"\n    Extract the required annotation models\n    Args:\n        cluster_name (str): The cluster name\n    \"\"\"\n    cluster = CLUSTERS.get(cluster_name, None)\n    if cluster is None:\n        raise ValueError(f\"Cluster {cluster_name} not found\")\n\n    # candidate included: speech2text, text_generation, text2speech, this normally is required\n    # other include emotion_detection now\n    required_annotation_task = []\n    for item in cluster.values():\n        if item[\"component_type\"] == \"task\":\n            task_name = item[\"task_name\"]\n            required_annotation_task.append(\n                Task.task_ml_task_mapping().get(task_name, None)\n            )\n\n    # filter out None\n    required_annotation_task = list(filter(None, required_annotation_task))\n    # remove the duplicate\n    return list(set(required_annotation_task))\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.multi_turn_benchmark_run","title":"<code>multi_turn_benchmark_run()</code>","text":"<p>Run the multi-turn benchmark Returns:</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>def multi_turn_benchmark_run(self):\n    \"\"\"\n    Run the multi-turn benchmark\n    Returns:\n\n    \"\"\"\n    logger.info(\n        f\"Running multi-turn benchmark for cluster {self.benchmark_cluster}\"\n    )\n    # run the benchmark\n    html_content = \"\"\n    if self.benchmark_cluster == \"all\":\n        for cluster_name in CLUSTERS.keys():\n            html_content += \"&lt;hr&gt;\"\n            html_content += self.process_multi_turn_benchmark(cluster_name)\n    else:\n        html_content += self.process_multi_turn_benchmark(self.benchmark_cluster)\n    return html_content\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.plot_distribution","title":"<code>plot_distribution(df, title='')</code>  <code>staticmethod</code>","text":"<p>Plot the distribution of the latency Args:     df (pd.DataFrame): The dataframe     title (str): The title</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plot in HTML</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>@staticmethod\ndef plot_distribution(df: pd.DataFrame, title: str = \"\") -&gt; str:\n    \"\"\"\n    Plot the distribution of the latency\n    Args:\n        df (pd.DataFrame): The dataframe\n        title (str): The title\n\n    Returns:\n        str: The plot in HTML\n    \"\"\"\n    # plot the distribution for each column\n    # Calculate mean and max for each latency column\n\n    mean_accuracies = df[df.columns[1:]].mean()\n    max_accuracies = df[df.columns[1:]].max()\n    min_accuracies = df[df.columns[1:]].min()\n\n    # Create a Plotly figure\n    fig = go.Figure()\n    # Add min latencies to the figure\n    fig.add_trace(\n        go.Bar(x=min_accuracies.index, y=min_accuracies.values, name=\"Min Accuracy\")\n    )\n    # Add mean latencies to the figure\n    fig.add_trace(\n        go.Bar(\n            x=mean_accuracies.index, y=mean_accuracies.values, name=\"Mean Accuracy\"\n        )\n    )\n\n    # Add max latencies to the figure\n    fig.add_trace(\n        go.Bar(x=max_accuracies.index, y=max_accuracies.values, name=\"Max Accuracy\")\n    )\n\n    # Customize the layout\n    fig.update_layout(\n        title={\n            \"text\": \"Accuracy Distribution\" + title,\n            \"x\": 0.5,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis_title=\"Evaluation Metrics\",\n        yaxis_title=\"Accuracies\",\n        barmode=\"group\",\n        margin=dict(l=10, r=10, b=0),\n    )\n\n    # Convert Plotly figure to HTML\n    plot_html = fig.to_html(full_html=False)\n    return plot_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.plot_table","title":"<code>plot_table(df, title='')</code>  <code>staticmethod</code>","text":"<p>Plot the table Args:     df (pd.DataFrame): The dataframe     title (str): The title</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plot in HTML</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>@staticmethod\ndef plot_table(df: pd.DataFrame, title: str = \"\") -&gt; str:\n    \"\"\"\n    Plot the table\n    Args:\n        df (pd.DataFrame): The dataframe\n        title (str): The title\n\n    Returns:\n        str: The plot in HTML\n    \"\"\"\n    colors = []\n    for col in df.columns:\n        col_colors = []\n        for val in df[col]:\n            if isinstance(val, float) or isinstance(val, int):\n                col_colors.append(\"lavender\")\n            else:\n                if val == \"missing\":\n                    col_colors.append(\"lightcoral\")\n                elif val == \"started\":\n                    col_colors.append(\"lightyellow\")\n                elif val == \"failed\":\n                    col_colors.append(\"lightcoral\")\n                elif val == \"pending\":\n                    col_colors.append(\"lightblue\")\n                elif val == \"incomplete\":\n                    col_colors.append(\"lightgrey\")\n                else:\n                    col_colors.append(\"lightgreen\")\n        colors.append(col_colors)\n    # Create a Plotly table\n    fig = go.Figure(\n        data=[\n            go.Table(\n                header=dict(\n                    values=[\n                        (\n                            [f\"&lt;b&gt;{c.upper()}&lt;/b&gt;\" for c in col]\n                            if isinstance(col, tuple)\n                            else f\"&lt;b&gt;{col.upper()}&lt;/b&gt;\"\n                        )\n                        for col in df.columns\n                    ],\n                    fill_color=\"paleturquoise\",\n                    align=\"left\",\n                ),\n                cells=dict(\n                    values=[df[col] for col in df.columns],\n                    fill_color=colors,\n                    align=\"left\",\n                ),\n            )\n        ]\n    )\n    fig.update_layout(\n        title={\n            \"text\": f\"Accuracy: {title}\",\n            \"x\": 0.5,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        #     update margin to be 0\n        margin=dict(l=10, r=10, b=0),\n        # get the height to be whatever it requires\n        height=max((len(df) * 35), 400),\n    )\n    # Update layout for better appearance\n    desc_html = fig.to_html(full_html=False)\n    return desc_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.process_cluster_benchmark","title":"<code>process_cluster_benchmark(cluster_name, detailed=False)</code>","text":"<p>Process the benchmark for a specific cluster</p> <p>For each cluster, we will need to analyse the conversation model And also need to understand what's the else model we need to analyse, for example the emotion_detection Args:      cluster_name (str): The cluster name     detailed (bool): The detailed flag</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The HTML content</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>def process_cluster_benchmark(\n    self, cluster_name: str, detailed: bool = False\n) -&gt; str:\n    \"\"\"\n    Process the benchmark for a specific cluster\n\n    For each cluster, we will need to analyse the conversation model\n    And also need to understand what's the else model we need to analyse, for example the emotion_detection\n    Args:\n         cluster_name (str): The cluster name\n        detailed (bool): The detailed flag\n\n    Returns:\n        str: The HTML content\n    \"\"\"\n    task_groups, required_tasks_count, tasks = extract_task_group(cluster_name)\n\n    required_annotation_task = self.extract_required_annotation_models(cluster_name)\n    logger.info(\n        f\"Cluster: {cluster_name}, Required annotation tasks: {required_annotation_task}\"\n    )\n    conversations = DataMultiModalConversation.objects.filter(\n        track_id__startswith=f\"T-{cluster_name}-\"\n    ).order_by(\"-created_at\")\n\n    html_content = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n    html_content += (\n        f\"&lt;p&gt;Required tasks each group: {required_tasks_count} \uff5c \"\n        f\"Annotation task groups: {len(conversations)}&lt;/p&gt;\"\n    )\n\n    # the emotion and other context results also will be pulled from this one\n    # then we will according to this to load the annotation results\n    # track id and annotation =&gt; flatten the results\n    annotations = []\n    annotation_expected_keys = MultiModalAnnotationForm.declared_fields.keys()\n    annotation_pending_default = {\n        key: \"pending\" for key in annotation_expected_keys\n    }\n\n    for conversation in conversations:\n        conversation_annotation = conversation.annotations\n        annotated = False\n        for user_id, annotation in conversation_annotation.items():\n            annotations.append(\n                {\n                    \"track_id\": conversation.track_id,\n                    \"user_id\": user_id,\n                    \"predict_text\": conversation.text.text,\n                    **annotation_pending_default,\n                    **annotation,\n                }\n            )\n            annotated = True\n        if not annotated:\n            annotations.append(\n                {\n                    \"track_id\": conversation.track_id,\n                    \"user_id\": \"missing\",\n                    \"predict_text\": \"\",\n                    **annotation_pending_default,\n                }\n            )\n\n    conversation_annotation_df = pd.DataFrame(annotations)\n    if len(conversation_annotation_df) == 0:\n        return html_content + \"&lt;p&gt;No conversation annotation found&lt;/p&gt;\"\n    # transform the track_id to be the last part\n    conversation_annotation_df[\"track_id\"] = (\n        conversation_annotation_df[\"track_id\"].str.split(\"-\").str[-1]\n    )\n    # replace all the column names, remove the annotation prefix\n    conversation_annotation_df.columns = [\n        col.replace(\"annotation_\", \"\") for col in conversation_annotation_df.columns\n    ]\n    # add CER and WER\n    conversation_annotation_df = self.calculate_speech2text_accuracy(\n        conversation_annotation_df\n    )\n\n    if detailed:\n        # then we will present them into multiple tables: speech2text, text_generation, text2speech, overall\n        if \"speech2text\" in required_annotation_task:\n            speech2text_df = conversation_annotation_df[\n                [\n                    \"track_id\",\n                    \"user_id\",\n                    \"predict_text\",\n                    \"speech2text\",\n                    \"wer\",\n                    \"cer\",\n                    \"speech2text_score\",\n                ]\n            ].copy(deep=True)\n            html_content += self.plot_table(speech2text_df, \"Speech2Text\")\n        if \"text_generation\" in required_annotation_task:\n            text_generation_df = conversation_annotation_df[\n                [\"track_id\", \"user_id\", \"text_generation\", \"text_generation_score\"]\n            ].copy(deep=True)\n            html_content += self.plot_table(text_generation_df, \"Text Generation\")\n        if \"text2speech\" in required_annotation_task:\n            text2speech_df = conversation_annotation_df[\n                [\"track_id\", \"user_id\", \"text2speech_score\"]\n            ].copy(deep=True)\n            html_content += self.plot_table(text2speech_df, \"Text2Speech\")\n\n        overall_conversation_df = conversation_annotation_df[\n            [\"track_id\", \"user_id\", \"overall_comment\", \"overall_score\"]\n        ].copy(deep=True)\n        html_content += self.plot_table(\n            overall_conversation_df, \"Overall Conversation Quality\"\n        )\n    else:\n        #\n        # then we will try to calculate the overall accuracy for each annotation task\n        conversation_annotation_df = self.annotation_average(\n            df=conversation_annotation_df\n        )\n        if \"speech2text\" in required_annotation_task:\n            desc_df = self.summary_df(\n                conversation_annotation_df[\n                    [\"track_id\", \"wer\", \"cer\", \"speech2text_score\"]\n                ].copy(deep=True)\n            )\n            html_content += self.plot_table(desc_df, \"Speech2Text Overall Quality\")\n            html_content += self.plot_distribution(\n                conversation_annotation_df[\n                    [\"track_id\", \"wer\", \"cer\", \"speech2text_score\"]\n                ].copy(deep=True),\n                \"Speech2Text\",\n            )\n        if \"text_generation\" in required_annotation_task:\n            desc_df = self.summary_df(\n                conversation_annotation_df[\n                    [\"track_id\", \"text_generation_score\"]\n                ].copy(deep=True)\n            )\n            html_content += self.plot_table(\n                desc_df, \"Text Generation Overall Quality\"\n            )\n            html_content += self.plot_distribution(\n                conversation_annotation_df[\n                    [\"track_id\", \"text_generation_score\"]\n                ].copy(deep=True),\n                \"Text Generation\",\n            )\n\n        if \"text2speech\" in required_annotation_task:\n            desc_df = self.summary_df(\n                conversation_annotation_df[[\"track_id\", \"text2speech_score\"]].copy(\n                    deep=True\n                )\n            )\n            html_content += self.plot_table(desc_df, \"Text2Speech Overall Quality\")\n            html_content += self.plot_distribution(\n                conversation_annotation_df[[\"track_id\", \"text2speech_score\"]].copy(\n                    deep=True\n                ),\n                \"Text2Speech\",\n            )\n\n    # summary the emotion detection task\n    if \"emotion_detection\" in required_annotation_task:\n        # load the emotion detection results\n        emotion_detection_results = ContextEmotionDetection.objects.filter(\n            multi_modal_conversation__in=conversations\n        ).order_by(\"-created_at\")\n        if len(emotion_detection_results) == 0:\n            return html_content + \"&lt;h4&gt;No emotion detection results found&lt;/h4&gt;\"\n\n        emotion_detection_expected_keys = (\n            MultiModalFKEmotionDetectionAnnotationForm.declared_fields.keys()\n        )\n        emotion_detection_pending_default = {\n            key: \"pending\" for key in emotion_detection_expected_keys\n        }\n        emotion_detection_annotations = []\n        for emotion_detection in emotion_detection_results:\n            emotion_detection_annotation = emotion_detection.annotations\n            annotated = False\n            for user_id, annotation in emotion_detection_annotation.items():\n                emotion_detection_annotations.append(\n                    {\n                        \"track_id\": emotion_detection.multi_modal_conversation.track_id,\n                        \"user_id\": user_id,\n                        **emotion_detection_pending_default,\n                        **annotation,\n                    }\n                )\n                annotated = True\n            if not annotated:\n                emotion_detection_annotations.append(\n                    {\n                        \"track_id\": emotion_detection.multi_modal_conversation.track_id,\n                        \"user_id\": \"missing\",\n                        **emotion_detection_pending_default,\n                    }\n                )\n\n        emotion_detection_df = pd.DataFrame(emotion_detection_annotations)\n        logger.info(emotion_detection_df)\n        emotion_detection_df[\"track_id\"] = (\n            emotion_detection_df[\"track_id\"].str.split(\"-\").str[-1]\n        )\n        emotion_detection_df.columns = [\n            col.replace(\"annotation_\", \"\") for col in emotion_detection_df.columns\n        ]\n        if detailed:\n            html_content += self.plot_table(\n                emotion_detection_df, \"Emotion Detection\"\n            )\n\n        else:\n            emotion_detection_df = self.annotation_average(emotion_detection_df)\n            desc_df = self.summary_df(emotion_detection_df)\n            # logger.info(desc_df)\n            html_content += self.plot_table(desc_df, \"Emotion Detection\")\n            html_content += self.plot_distribution(\n                emotion_detection_df, \"Emotion Detection\"\n            )\n    return html_content\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.process_multi_turn_benchmark","title":"<code>process_multi_turn_benchmark(cluster_name)</code>","text":"<p>Process the multi-turn benchmark</p> <p>First we will need to get all tag with this cluster name, and grab the last one within each tag Args:     cluster_name (str): The cluster name Returns:</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>def process_multi_turn_benchmark(self, cluster_name: str) -&gt; str:\n    \"\"\"\n    Process the multi-turn benchmark\n\n    First we will need to get all tag with this cluster name, and grab the last one within each tag\n    Args:\n        cluster_name (str): The cluster name\n    Returns:\n\n    \"\"\"\n    conversations = DataMultiModalConversation.objects.filter(\n        track_id__startswith=f\"T-{cluster_name}-\"\n    )\n\n    # grab all tags\n\n    tags = []\n    for conversation in conversations:\n        for tag in conversation.tags.all():\n            tags.append(tag.name)\n\n    tags = list(set(tags))\n\n    tag_last_conversations = []\n    for tag in tags:\n        last_conversation = (\n            conversations.filter(tags__name=tag).order_by(\"-created_at\").first()\n        )\n        tag_last_conversations.append(last_conversation)\n\n    html_content = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n    html_content += (\n        f\"&lt;p&gt;Multi-Turn Conversation Count: {len(tag_last_conversations)}&lt;/p&gt;\"\n    )\n    # then we will need to analyse the conversation model\n    multi_turn_annotations = []\n    # get all possible keys from the annotation form\n    multi_turn_annotations_expected_keys = []\n    for conversation in tag_last_conversations:\n        conversation_annotation = conversation.multi_turns_annotations\n        for user_id, annotation in conversation_annotation.items():\n            multi_turn_annotations_expected_keys.extend(annotation.keys())\n    multi_turn_annotations_expected_keys = list(\n        set(multi_turn_annotations_expected_keys)\n    )\n\n    if \"multi_turn_annotation_overall\" not in multi_turn_annotations_expected_keys:\n        multi_turn_annotations_expected_keys.append(\"multi_turn_annotation_overall\")\n    if (\n        \"multi_turn_annotation_overall_comment\"\n        not in multi_turn_annotations_expected_keys\n    ):\n        multi_turn_annotations_expected_keys.append(\n            \"multi_turn_annotation_overall_comment\"\n        )\n\n    multi_turn_annotations_pending_default = {\n        key: \"pending\" for key in multi_turn_annotations_expected_keys\n    }\n\n    for conversation in tag_last_conversations:\n        conversation_annotation = conversation.multi_turns_annotations\n        annotated = False\n        for user_id, annotation in conversation_annotation.items():\n            multi_turn_annotations.append(\n                {\n                    \"track_id\": conversation.track_id,\n                    \"user_id\": user_id,\n                    **multi_turn_annotations_pending_default,\n                    **annotation,\n                }\n            )\n            annotated = True\n        if not annotated:\n            multi_turn_annotations.append(\n                {\n                    \"track_id\": conversation.track_id,\n                    \"user_id\": \"missing\",\n                    **multi_turn_annotations_pending_default,\n                }\n            )\n\n    multi_turn_annotation_df = pd.DataFrame(multi_turn_annotations)\n\n    if len(multi_turn_annotation_df) == 0:\n        return html_content + \"&lt;p&gt;No multi-turn conversation annotation found&lt;/p&gt;\"\n    # transform the track_id to be the last part\n    multi_turn_annotation_df[\"track_id\"] = (\n        multi_turn_annotation_df[\"track_id\"].str.split(\"-\").str[-1]\n    )\n    # replace all the column names, remove the annotation prefix\n    multi_turn_annotation_df.columns = [\n        col.replace(\"multi_turn_annotation_\", \"\")\n        for col in multi_turn_annotation_df.columns\n    ]\n\n    html_content += self.plot_table(\n        multi_turn_annotation_df, \"Multi-Turn Conversation\"\n    )\n    return html_content\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/accuracy_benchmark/#API.orchestrator.metrics.accuracy_benchmark.AccuracyBenchmark.summary_df","title":"<code>summary_df(df)</code>  <code>staticmethod</code>","text":"<p>Summary the given dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>DataFrame</code> <p>The HTML content</p> Source code in <code>API/orchestrator/metrics/accuracy_benchmark.py</code> <pre><code>@staticmethod\ndef summary_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Summary the given dataframe\n\n    Args:\n        df (pd.DataFrame): The dataframe\n\n    Returns:\n        str: The HTML content\n    \"\"\"\n    # for the same track_id, aggregate the results into one, and use the mean as the final result\n    # df = df.apply(pd.to_numeric, errors='coerce')\n\n    # Group by 'track_id' and calculate the mean for each group\n    # df = df.groupby(\"track_id\").mean().reset_index()\n    desc_df = df.describe().transpose()\n    desc_df = desc_df.reset_index()\n    desc_df.rename(columns={\"index\": \"metric\"}, inplace=True)\n    desc_df = desc_df.round(4)\n    return desc_df\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/","title":"LatencyBenchmark","text":""},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark","title":"<code>LatencyBenchmark</code>","text":"<p>For each component, we will generally have two values: - model_latency: The time taken by the model to process the data - transfer_latency: The time taken to transfer the data to the model - overall_latency: The time taken by the model to process the data and transfer the data to the model</p> <p>The whole pipeline latency will be the sum of - all component start end end ts</p> <p>Another way to output the performance is the Timeline - start will be 0 - and average relative time to 0 for each important time point, plot them in the timeline</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>class LatencyBenchmark:\n    \"\"\"\n    For each component, we will generally have two values:\n    - model_latency: The time taken by the model to process the data\n    - transfer_latency: The time taken to transfer the data to the model\n    - overall_latency: The time taken by the model to process the data and transfer the data to the model\n\n    The whole pipeline latency will be the sum of\n    - all component start end end ts\n\n    Another way to output the performance is the Timeline\n    - start will be 0\n    - and average relative time to 0 for each important time point, plot them in the timeline\n    \"\"\"\n\n    def __init__(self, benchmark_cluster: str = CLUSTER_Q_ETE_CONVERSATION_NAME):\n        \"\"\"\n        Initialize the benchmark\n        Args:\n            benchmark_cluster (str): The benchmark cluster\n        \"\"\"\n        # if it is a specific name, gather this metric, otherwise, report all existing cluster\n        self.benchmark_cluster = benchmark_cluster\n\n    def run(self):\n        \"\"\"\n        Run the benchmark\n        \"\"\"\n        html_content = \"\"\n        if self.benchmark_cluster == \"all\":\n            for cluster_name in CLUSTERS.keys():\n                # add a divider\n                html_content += \"&lt;hr&gt;\"\n                html_content += self.process_cluster(cluster_name)\n        else:\n            if self.benchmark_cluster not in CLUSTERS:\n                raise ValueError(f\"Cluster {self.benchmark_cluster} not found\")\n            html_content += \"&lt;hr&gt;\"\n            html_content += self.process_cluster(self.benchmark_cluster)\n        return html_content\n\n    def run_detail(self) -&gt; str:\n        html_content = \"\"\n        if self.benchmark_cluster == \"all\":\n            for cluster_name in CLUSTERS.keys():\n                # add a divider\n                html_content += \"&lt;hr&gt;\"\n                html_content += self.process_cluster_detail(cluster_name)\n        else:\n            if self.benchmark_cluster not in CLUSTERS:\n                raise ValueError(f\"Cluster {self.benchmark_cluster} not found\")\n            html_content += \"&lt;hr&gt;\"\n            html_content += self.process_cluster_detail(self.benchmark_cluster)\n        return html_content\n\n    def process_cluster(self, cluster_name: str):\n        \"\"\"\n        Process the cluster\n        Args:\n            cluster_name (str): The cluster name\n        \"\"\"\n        task_groups, required_tasks_count, tasks = extract_task_group(cluster_name)\n        general_desc = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n        general_desc += f\"&lt;p&gt;Required tasks: {required_tasks_count} | Total tasks groups: {len(task_groups)}&lt;/p&gt;\"\n        # loop through the task groups, if the success task is not == required_tasks_count, then we will skip\n        success_pipeline = 0\n        cluster_latency = []\n        for track_id, task_group in task_groups.items():\n            success_tasks = [\n                task for task in task_group if task.result_status == \"completed\"\n            ]\n            if len(success_tasks) != required_tasks_count:\n                # the pipeline is not completed, so we will skip\n                continue\n            success_pipeline += 1\n            cluster_latency.append(self.process_task_group(task_group))\n\n        logger.info(\n            f\"\"\"\n                Cluster: {cluster_name}, Success Ratio: {success_pipeline}/{len(task_groups)}\n                Required Components: {required_tasks_count}, Total tasks: {len(tasks)}\n            \"\"\"\n        )\n\n        general_title = f\"Cluster: &lt;b&gt;{cluster_name}&lt;/b&gt;, Completed Ratio: {success_pipeline}/{len(task_groups)}\"\n        # flatten the cluster_latency\n        result_df = pd.DataFrame(cluster_latency)\n        # get the column split with _ from right, and left element is the component name\n\n        if len(result_df) != 0:\n            logger.debug(result_df.describe())\n            # result_df.to_csv(settings.LOG_DIR / f\"{cluster_name}_benchmark.csv\")\n            # to html and return it\n            logger.debug(result_df.describe())\n            desc = result_df.describe().transpose()\n            desc = desc.round(4)\n\n            # add another column\n            # Extract model accuracy from index and add it as a new column\n            desc[\"latency_type\"] = desc.index.str.rsplit(\"_\", n=2).str[1]\n            # then update the index to two columns, first will be component\n            desc.index = desc.index.str.rsplit(\"_\", n=2, expand=True).get_level_values(\n                0\n            )\n            # reset index, get the index to be the column component\n            desc = desc.reset_index()\n            # rename the index to be component\n            desc = desc.rename(columns={\"index\": \"component\"})\n            desc_html = self.plot_table(desc, title=f\" ({general_title})\")\n            plot_html = self.plot_distribution(result_df, title=f\" ({general_title})\")\n\n            return general_desc + desc_html + plot_html\n        return general_desc\n\n    def process_cluster_detail(self, cluster_name: str) -&gt; str:\n        \"\"\"\n        Process the cluster in detail\n        Even if the track is not finished, we will still plot it and stop status\n        Args:\n            cluster_name (str): html content\n\n        Returns:\n\n        \"\"\"\n        task_groups, required_tasks_count, tasks = extract_task_group(cluster_name)\n        general_desc = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n        general_desc += f\"&lt;p&gt;Required tasks: {required_tasks_count} | Total tasks groups: {len(task_groups)}&lt;/p&gt;\"\n        # loop through the task groups, if the success task is not == required_tasks_count, then we will skip\n        success_pipeline = 0\n        cluster_latency = []\n        cluster_ts_latency = []\n        cluster_tp_latency = []\n        for track_id, task_group in task_groups.items():\n            success_tasks = [\n                task for task in task_group if task.result_status == \"completed\"\n            ]\n            if len(success_tasks) == required_tasks_count:\n                # the pipeline is not completed, so we will skip\n                success_pipeline += 1\n            cluster_latency.append(self.process_task_group_detail(task_group))\n            cluster_ts_latency.append(\n                self.process_task_group_detail_timeline(task_group)\n            )\n            cluster_tp_latency.append(\n                self.process_task_group_detail_timeline(task_group, timeline=True)\n            )\n        general_title = f\"Cluster: &lt;b&gt;{cluster_name}&lt;/b&gt;, Completed Ratio: {success_pipeline}/{len(task_groups)}\"\n        result_df = pd.DataFrame(cluster_latency)\n        if len(result_df) == 0:\n            return general_desc\n\n        # only keep the last element in the track_id\n        result_df[\"track_id\"] = result_df[\"track_id\"].str.split(\"-\").str[-1]\n        # get result into multiple level column, which will split current column into multiple level column name\n        # Split the column names into three parts, but only keep the first two\n        split_columns = result_df.columns.str.rsplit(\"_\", n=2, expand=True)\n\n        # we only need the first two level, so we will get the first two level\n        result_df.columns = [\n            split_columns.get_level_values(0),\n            split_columns.get_level_values(1),\n        ]\n        # sort the column\n        track_tasks_html = self.plot_table(result_df, title=f\" ({general_title})\")\n\n        # cluster ts latency\n        result_ts_df = pd.DataFrame(cluster_ts_latency)\n        # result_ts_df.to_csv(settings.LOG_DIR / f\"{cluster_name}_ts_benchmark.csv\")\n        if len(result_ts_df) == 0:\n            return track_tasks_html\n        # we will plot a bar\n        ts_stacked_html = self.plot_stacked_timeline(result_ts_df, title=general_title)\n\n        # grab the time point latency, and try to draw time point html\n        result_tp_df = pd.DataFrame(cluster_tp_latency)\n        # result_tp_df.to_csv(settings.LOG_DIR / f\"{cluster_name}_tp_benchmark.csv\")\n        ts_timepoint_html = self.plot_timestamp_timeline_depth(\n            result_tp_df, title=general_title\n        )\n        return general_desc + track_tasks_html + ts_stacked_html + ts_timepoint_html\n\n    @staticmethod\n    def process_task_group(task_track: List[Task]):\n        \"\"\"\n        This will process each component, and then extract the transfer and model latency total\n\n        Args:\n            task_track (List[Task]): The task track\n\n        Returns:\n            dict: The benchmark result\n        \"\"\"\n        result = {\n            \"track_id\": task_track[0].track_id,\n        }\n        task_names = get_task_names_order(result[\"track_id\"])\n        for task in task_track:\n            latency_profile = task.result_json.get(\"latency_profile\", {})\n            # NOTE: this will require client side do not log overlap durations\n            model_latency = 0\n            transfer_latency = 0\n            logger.debug(latency_profile)\n            task_start_time = None\n            task_end_time = None\n            for key, value in latency_profile.items():\n                if key.startswith(\"model\"):\n                    model_latency += float(value)\n                if key.startswith(\"transfer\"):\n                    transfer_latency += float(value)\n                if key.startswith(\"ts\"):\n                    if key == \"ts_start_task\":\n                        task_start_time = value\n                    if key == \"ts_end_task\":\n                        task_end_time = value\n            result[f\"{task.task_name}_model_latency\"] = model_latency\n            result[f\"{task.task_name}_transfer_latency\"] = transfer_latency\n            # look for the ts_start_task and ts_end_task, and the overall_latency should be that value\n            # process time into datetime object\n            # ts_end_trigger_emotion_model 2024-07-01T14:58:36.419352\n            if task_start_time and task_end_time:\n                task_start_time_dt = str_to_datetime(task_start_time)\n                task_end_time_dt = str_to_datetime(task_end_time)\n                result[f\"{task.task_name}_overall_latency\"] = (  # noqa\n                    task_end_time_dt - task_start_time_dt\n                ).total_seconds()\n\n            else:\n                logger.error(f\"Task {task.task_name} does not have start and end time\")\n                result[f\"{task.task_name}_overall_latency\"] = (\n                    model_latency + transfer_latency\n                )\n        # total_latency should be the sum of all the overall_latency\n        total_latency = 0\n        for key, value in result.items():\n            if key.endswith(\"overall_latency\"):\n                total_latency += value\n        result[\"total_latency\"] = total_latency\n        # loop all value, get it to decimal 4\n        for key, value in result.items():\n            if isinstance(value, float):\n                result[key] = round(value, 4)\n\n        ordered_result = {\n            \"track_id\": result[\"track_id\"],\n        }\n        for task_name in task_names:\n            ordered_result[task_name + \"_model_latency\"] = result[\n                task_name + \"_model_latency\"\n            ]\n            ordered_result[task_name + \"_transfer_latency\"] = result[\n                task_name + \"_transfer_latency\"\n            ]\n            ordered_result[task_name + \"_overall_latency\"] = result[\n                task_name + \"_overall_latency\"\n            ]\n        ordered_result[\"total_latency\"] = result[\"total_latency\"]\n        return ordered_result\n\n    @staticmethod\n    def process_task_group_detail(task_track: List[Task]):\n        \"\"\"\n        This will process each component, and then extract the transfer and model latency total\n\n        Args:\n            task_track (List[Task]): The task track\n\n        Returns:\n            dict: The benchmark result\n        \"\"\"\n        result = {\n            \"track_id\": task_track[0].track_id,\n        }\n        task_names = get_task_names_order(result[\"track_id\"])\n        for task in task_track:\n            if task.result_status != \"completed\":\n                result[f\"{task.task_name}_model_latency\"] = task.result_status\n                result[f\"{task.task_name}_transfer_latency\"] = task.result_status\n                result[f\"{task.task_name}_overall_latency\"] = task.result_status\n                continue\n            latency_profile = task.result_json.get(\"latency_profile\", {})\n            # NOTE: this will require client side do not log overlap durations\n            model_latency = 0\n            transfer_latency = 0\n            logger.debug(latency_profile)\n            task_start_time = None\n            task_end_time = None\n            for key, value in latency_profile.items():\n                if key.startswith(\"model\"):\n                    model_latency += float(value)\n                if key.startswith(\"transfer\"):\n                    transfer_latency += float(value)\n                if key.startswith(\"ts\"):\n                    if key == \"ts_start_task\":\n                        task_start_time = value\n                    if key == \"ts_end_task\":\n                        task_end_time = value\n            result[f\"{task.task_name}_model_latency\"] = model_latency\n            result[f\"{task.task_name}_transfer_latency\"] = transfer_latency\n            # look for the ts_start_task and ts_end_task, and the overall_latency should be that value\n            # process time into datetime object\n            # ts_end_trigger_emotion_model 2024-07-01T14:58:36.419352\n            if task_start_time and task_end_time:\n                task_start_time_dt = str_to_datetime(task_start_time)\n                task_end_time_dt = str_to_datetime(task_end_time)\n                result[f\"{task.task_name}_overall_latency\"] = (  # noqa\n                    task_end_time_dt - task_start_time_dt\n                ).total_seconds()\n\n            else:\n                logger.error(f\"Task {task.task_name} does not have start and end time\")\n                result[f\"{task.task_name}_overall_latency\"] = (\n                    model_latency + transfer_latency\n                )\n\n        # sort the key to be the same as the cluster order, also if missed, fill it with missing\n        for task_name in task_names:\n            if f\"{task_name}_overall_latency\" not in result:\n                result[task_name + \"_model_latency\"] = \"missing\"\n                result[task_name + \"_transfer_latency\"] = \"missing\"\n                result[task_name + \"_overall_latency\"] = \"missing\"\n\n        # total_latency should be the sum of all the overall_latency\n        total_latency = 0\n        for key, value in result.items():\n            if key.endswith(\"overall_latency\") and isinstance(value, float):\n                total_latency += value\n            elif key.endswith(\"overall_latency\") and not isinstance(value, float):\n                total_latency = \"incomplete\"\n                break\n        result[\"total_latency\"] = total_latency\n        # loop all value, get it to decimal 4\n        for key, value in result.items():\n            if isinstance(value, float):\n                result[key] = round(value, 4)\n\n        ordered_result = {\n            \"track_id\": result[\"track_id\"],\n        }\n        for task_name in task_names:\n            ordered_result[task_name + \"_model_latency\"] = result[\n                task_name + \"_model_latency\"\n            ]\n            ordered_result[task_name + \"_transfer_latency\"] = result[\n                task_name + \"_transfer_latency\"\n            ]\n            ordered_result[task_name + \"_overall_latency\"] = result[\n                task_name + \"_overall_latency\"\n            ]\n\n        ordered_result[\"total_latency\"] = result[\"total_latency\"]\n        return ordered_result\n\n    @staticmethod\n    def process_task_group_detail_timeline(\n        task_track: List[Task], timeline: bool = False\n    ):\n        \"\"\"\n        Based on the result_json =&gt; latency_profile\n        We will gather the time point for each, and then change to the relative second value compared to start point\n\n        If timeline is True, we will only grab the timestamp information.\n        Otherwise, we will calculate the relative time to the start point\n\n        In the end, we will grab the\n        Args:\n            task_track (List[Task]): The task track\n            timeline (bool): If we want to plot the timeline\n\n        Returns:\n\n        \"\"\"\n        result = {\n            \"track_id\": task_track[0].track_id,\n        }\n\n        task_names = get_task_names_order(result[\"track_id\"])\n\n        task_results = {}\n        for task in task_track:\n            if task.result_status != \"completed\":\n                continue\n            latency_profile = task.result_json.get(\"latency_profile\", {})\n            task_result = {}\n            for key, value in latency_profile.items():\n                if key.startswith(\"ts\"):\n                    task_result[key] = str_to_datetime(value)\n\n            if timeline is False:\n                # sort out the whole task_result based on time timestamp\n                # and then calculate the relative time to the previous component\n                sorted_task_result = dict(\n                    sorted(task_result.items(), key=lambda item: item[1])\n                )\n                previous_time = None\n                task_relative_time = {}\n                for key, value in sorted_task_result.items():\n                    if previous_time is None:\n                        task_relative_time[key] = 0\n                    else:\n                        task_relative_time[key] = (\n                            value - previous_time\n                        ).total_seconds()\n                    previous_time = value\n                task_results[task.task_name] = task_relative_time\n            else:\n                task_results[task.task_name] = task_result\n\n        # sort the key to be the same as the cluster order, calculate the value to add up the previous component\n        first_start_task = None\n        for task_name in task_names:\n            if task_name not in task_results:\n                break\n            for key, value in task_results[task_name].items():\n                new_key = f\"{task_name}_{key.split('_', 1)[1]}\"\n                if key == \"ts_start_task\":\n                    if first_start_task is None:\n                        first_start_task = value\n                    else:\n                        continue\n                if new_key not in result:\n                    result[new_key] = value\n\n        return result\n\n    @staticmethod\n    def plot_table(df: pd.DataFrame, title: str = \"\") -&gt; str:\n        \"\"\"\n        Plot the table\n        Args:\n            df (pd.DataFrame): The dataframe\n            title (str): The title\n\n        Returns:\n            str: The plot in HTML\n        \"\"\"\n        colors = []\n        for col in df.columns:\n            col_colors = []\n            for val in df[col]:\n                if isinstance(val, float) or isinstance(val, int):\n                    col_colors.append(\"lavender\")\n                else:\n                    if val == \"missing\":\n                        col_colors.append(\"lightcoral\")\n                    elif val == \"started\":\n                        col_colors.append(\"lightyellow\")\n                    elif val == \"failed\":\n                        col_colors.append(\"lightcoral\")\n                    elif val == \"pending\":\n                        col_colors.append(\"lightblue\")\n                    elif val == \"incomplete\":\n                        col_colors.append(\"lightgrey\")\n                    else:\n                        col_colors.append(\"lightgreen\")\n            colors.append(col_colors)\n        # Create a Plotly table\n        fig = go.Figure(\n            data=[\n                go.Table(\n                    header=dict(\n                        values=[\n                            (\n                                [f\"&lt;b&gt;{c.upper()}&lt;/b&gt;\" for c in col]\n                                if isinstance(col, tuple)\n                                else f\"&lt;b&gt;{col.upper()}&lt;/b&gt;\"\n                            )\n                            for col in df.columns\n                        ],\n                        fill_color=\"paleturquoise\",\n                        align=\"left\",\n                    ),\n                    cells=dict(\n                        values=[df[col] for col in df.columns],\n                        fill_color=colors,\n                        align=\"left\",\n                    ),\n                )\n            ]\n        )\n        fig.update_layout(\n            title={\n                \"text\": f\"Latency Summary: {title}\",\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"yanchor\": \"top\",\n            },\n            #     update margin to be 0\n            margin=dict(l=10, r=10, b=0),\n            # get the height to be whatever it requires\n            height=max((len(df) * 35), 300),\n        )\n        # Update layout for better appearance\n        desc_html = fig.to_html(full_html=False)\n        return desc_html\n\n    @staticmethod\n    def plot_distribution(df: pd.DataFrame, title: str = \"\") -&gt; str:\n        \"\"\"\n        Plot the distribution of the latency\n        Args:\n            df (pd.DataFrame): The dataframe\n            title (str): The title\n\n        Returns:\n            str: The plot in HTML\n        \"\"\"\n        # plot the distribution for each column\n        # Calculate mean and max for each latency column\n        mean_latencies = df[df.columns[1:]].mean()\n        max_latencies = df[df.columns[1:]].max()\n        min_latencies = df[df.columns[1:]].min()\n\n        # Create a Plotly figure\n        fig = go.Figure()\n        # Add min latencies to the figure\n        fig.add_trace(\n            go.Bar(x=min_latencies.index, y=min_latencies.values, name=\"Min Latency\")\n        )\n        # Add mean latencies to the figure\n        fig.add_trace(\n            go.Bar(x=mean_latencies.index, y=mean_latencies.values, name=\"Mean Latency\")\n        )\n\n        # Add max latencies to the figure\n        fig.add_trace(\n            go.Bar(x=max_latencies.index, y=max_latencies.values, name=\"Max Latency\")\n        )\n\n        # Customize the layout\n        fig.update_layout(\n            title={\n                \"text\": \"Latency Distribution\" + title,\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"yanchor\": \"top\",\n            },\n            xaxis_title=\"Component and Latency\",\n            yaxis_title=\"Latency (s)\",\n            barmode=\"group\",\n            margin=dict(l=10, r=10, b=0),\n        )\n\n        # Convert Plotly figure to HTML\n        plot_html = fig.to_html(full_html=False)\n        return plot_html\n\n    @staticmethod\n    def plot_stacked_timeline(df: pd.DataFrame, title: str) -&gt; str:\n        \"\"\"\n        Plot the stacked timeline\n        Args:\n            df (pd.DataFrame): The dataframe\n            title (str): The title\n\n        Returns:\n\n        \"\"\"\n        # Create a Plotly figure\n        fig = go.Figure()\n        # get the track id to be the stacked one\n        df[\"track_id\"] = df[\"track_id\"].str.split(\"-\").str[-1]\n        # Add a trace for each component\n        for col in df.columns[1:]:\n            fig.add_trace(\n                go.Bar(\n                    y=df[\"track_id\"],\n                    x=df[col],\n                    name=col,\n                    orientation=\"h\",\n                    hovertemplate=\"%{x}&lt;br&gt;%{fullData.name}&lt;extra&gt;&lt;/extra&gt;\",\n                )\n            )\n\n        # Customize the layout\n        fig.update_layout(\n            title={\n                \"text\": f\"Time Interval in Seconds ({title})\",\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"yanchor\": \"top\",\n            },\n            xaxis_title=\"Relative in Seconds to Start Time\",\n            yaxis_title=\"Track ID\",\n            barmode=\"stack\",\n            height=max((len(df) * 35), 300),\n        )\n\n        # Convert Plotly figure to HTML\n        plot_html = fig.to_html(full_html=False)\n        return plot_html\n\n    @staticmethod\n    def plot_timestamp_timeline_depth(df: pd.DataFrame, title: str) -&gt; str:\n        \"\"\"\n        Plot the timestamp timeline\n        Args:\n            df (pd.DataFrame): The dataframe\n            title (str): The title\n\n        Returns:\n            str: The plot in HTML\n        \"\"\"\n        fig = go.Figure()\n        y_values = list(range(len(df)))\n        shapes = []\n        for y_value in y_values:\n            shapes.append(\n                dict(\n                    type=\"line\",\n                    xref=\"paper\",\n                    x0=0,\n                    x1=1,\n                    yref=\"y\",\n                    y0=y_value,\n                    y1=y_value,\n                    line=dict(color=\"grey\", width=1, dash=\"dot\"),\n                )\n            )\n        y_labels = []\n\n        legend_added = {}\n        # Use Plotly's qualitative color sequence 'Dark24' to generate a spectrum of colors\n        colors = pcolors.qualitative.Dark24\n\n        # Dynamically generate a color map for each column\n        column_colors = {\n            col: colors[i % len(colors)] for i, col in enumerate(df.columns[1:])\n        }\n        for i, row in df.iterrows():\n            y_value = y_values[i]\n            y_labels.append(row[\"track_id\"].split(\"-\")[-1])\n            for col in df.columns[1:]:\n                if not pd.isna(row[col]):\n                    show_legend = False\n                    if col not in legend_added:\n                        show_legend = True\n                        legend_added[col] = True\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[row[col]],\n                            y=[y_value],\n                            mode=\"markers\",\n                            marker=dict(size=10, color=column_colors[col]),\n                            name=f\"{col}\",\n                            hovertemplate=\"%{x}&lt;br&gt;%{fullData.name}&lt;extra&gt;&lt;/extra&gt;\",\n                            showlegend=show_legend,\n                        )\n                    )\n        # Customize the layout\n        fig.update_layout(\n            title={\n                \"text\": f\"Timeline of Events ({title})\",\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"yanchor\": \"top\",\n            },\n            xaxis_title=\"Time\",\n            yaxis=dict(\n                showline=False,\n                showgrid=True,\n                zeroline=False,\n                tickvals=y_values,\n                ticktext=y_labels,\n                title=\"Track ID\",\n            ),\n            showlegend=True,\n            shapes=shapes,\n            height=max((len(df) * 35), 300),\n        )\n        # Convert Plotly figure to HTML\n        plot_html = fig.to_html(full_html=False)\n        return plot_html\n\n    @staticmethod\n    def plot_timestamp_timeline(df: pd.DataFrame) -&gt; str:\n        \"\"\"\n        Plot the timestamp timeline\n        Args:\n            df (pd.DataFrame): The dataframe\n\n        Returns:\n            str: The plot in HTML\n        \"\"\"\n        fig = go.Figure()\n        y_values = range(len(df))\n\n        for i, row in df.iterrows():\n            y_value = y_values[i]\n            for col in df.columns[1:]:\n                if not pd.isna(row[col]):\n                    fig.add_trace(\n                        go.Scatter(\n                            x=[row[col]],\n                            y=[y_value],\n                            mode=\"markers\",\n                            marker=dict(size=10),\n                            name=f\"{col}\",\n                            hovertemplate=\"%{x}&lt;br&gt;%{fullData.name}&lt;extra&gt;&lt;/extra&gt;\",\n                        )\n                    )\n            # break\n        # Customize the layout\n        fig.update_layout(\n            title=\"Timeline of Time Points\",\n            xaxis_title=\"Time\",\n            # show nothing of y, even the label\n            yaxis=dict(\n                showticklabels=False, showline=False, showgrid=False, zeroline=True\n            ),\n            showlegend=True,\n        )\n        # Convert Plotly figure to HTML\n        plot_html = fig.to_html(full_html=False)\n        return plot_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.__init__","title":"<code>__init__(benchmark_cluster=CLUSTER_Q_ETE_CONVERSATION_NAME)</code>","text":"<p>Initialize the benchmark Args:     benchmark_cluster (str): The benchmark cluster</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>def __init__(self, benchmark_cluster: str = CLUSTER_Q_ETE_CONVERSATION_NAME):\n    \"\"\"\n    Initialize the benchmark\n    Args:\n        benchmark_cluster (str): The benchmark cluster\n    \"\"\"\n    # if it is a specific name, gather this metric, otherwise, report all existing cluster\n    self.benchmark_cluster = benchmark_cluster\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.plot_distribution","title":"<code>plot_distribution(df, title='')</code>  <code>staticmethod</code>","text":"<p>Plot the distribution of the latency Args:     df (pd.DataFrame): The dataframe     title (str): The title</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plot in HTML</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef plot_distribution(df: pd.DataFrame, title: str = \"\") -&gt; str:\n    \"\"\"\n    Plot the distribution of the latency\n    Args:\n        df (pd.DataFrame): The dataframe\n        title (str): The title\n\n    Returns:\n        str: The plot in HTML\n    \"\"\"\n    # plot the distribution for each column\n    # Calculate mean and max for each latency column\n    mean_latencies = df[df.columns[1:]].mean()\n    max_latencies = df[df.columns[1:]].max()\n    min_latencies = df[df.columns[1:]].min()\n\n    # Create a Plotly figure\n    fig = go.Figure()\n    # Add min latencies to the figure\n    fig.add_trace(\n        go.Bar(x=min_latencies.index, y=min_latencies.values, name=\"Min Latency\")\n    )\n    # Add mean latencies to the figure\n    fig.add_trace(\n        go.Bar(x=mean_latencies.index, y=mean_latencies.values, name=\"Mean Latency\")\n    )\n\n    # Add max latencies to the figure\n    fig.add_trace(\n        go.Bar(x=max_latencies.index, y=max_latencies.values, name=\"Max Latency\")\n    )\n\n    # Customize the layout\n    fig.update_layout(\n        title={\n            \"text\": \"Latency Distribution\" + title,\n            \"x\": 0.5,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis_title=\"Component and Latency\",\n        yaxis_title=\"Latency (s)\",\n        barmode=\"group\",\n        margin=dict(l=10, r=10, b=0),\n    )\n\n    # Convert Plotly figure to HTML\n    plot_html = fig.to_html(full_html=False)\n    return plot_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.plot_stacked_timeline","title":"<code>plot_stacked_timeline(df, title)</code>  <code>staticmethod</code>","text":"<p>Plot the stacked timeline Args:     df (pd.DataFrame): The dataframe     title (str): The title</p> <p>Returns:</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef plot_stacked_timeline(df: pd.DataFrame, title: str) -&gt; str:\n    \"\"\"\n    Plot the stacked timeline\n    Args:\n        df (pd.DataFrame): The dataframe\n        title (str): The title\n\n    Returns:\n\n    \"\"\"\n    # Create a Plotly figure\n    fig = go.Figure()\n    # get the track id to be the stacked one\n    df[\"track_id\"] = df[\"track_id\"].str.split(\"-\").str[-1]\n    # Add a trace for each component\n    for col in df.columns[1:]:\n        fig.add_trace(\n            go.Bar(\n                y=df[\"track_id\"],\n                x=df[col],\n                name=col,\n                orientation=\"h\",\n                hovertemplate=\"%{x}&lt;br&gt;%{fullData.name}&lt;extra&gt;&lt;/extra&gt;\",\n            )\n        )\n\n    # Customize the layout\n    fig.update_layout(\n        title={\n            \"text\": f\"Time Interval in Seconds ({title})\",\n            \"x\": 0.5,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis_title=\"Relative in Seconds to Start Time\",\n        yaxis_title=\"Track ID\",\n        barmode=\"stack\",\n        height=max((len(df) * 35), 300),\n    )\n\n    # Convert Plotly figure to HTML\n    plot_html = fig.to_html(full_html=False)\n    return plot_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.plot_table","title":"<code>plot_table(df, title='')</code>  <code>staticmethod</code>","text":"<p>Plot the table Args:     df (pd.DataFrame): The dataframe     title (str): The title</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plot in HTML</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef plot_table(df: pd.DataFrame, title: str = \"\") -&gt; str:\n    \"\"\"\n    Plot the table\n    Args:\n        df (pd.DataFrame): The dataframe\n        title (str): The title\n\n    Returns:\n        str: The plot in HTML\n    \"\"\"\n    colors = []\n    for col in df.columns:\n        col_colors = []\n        for val in df[col]:\n            if isinstance(val, float) or isinstance(val, int):\n                col_colors.append(\"lavender\")\n            else:\n                if val == \"missing\":\n                    col_colors.append(\"lightcoral\")\n                elif val == \"started\":\n                    col_colors.append(\"lightyellow\")\n                elif val == \"failed\":\n                    col_colors.append(\"lightcoral\")\n                elif val == \"pending\":\n                    col_colors.append(\"lightblue\")\n                elif val == \"incomplete\":\n                    col_colors.append(\"lightgrey\")\n                else:\n                    col_colors.append(\"lightgreen\")\n        colors.append(col_colors)\n    # Create a Plotly table\n    fig = go.Figure(\n        data=[\n            go.Table(\n                header=dict(\n                    values=[\n                        (\n                            [f\"&lt;b&gt;{c.upper()}&lt;/b&gt;\" for c in col]\n                            if isinstance(col, tuple)\n                            else f\"&lt;b&gt;{col.upper()}&lt;/b&gt;\"\n                        )\n                        for col in df.columns\n                    ],\n                    fill_color=\"paleturquoise\",\n                    align=\"left\",\n                ),\n                cells=dict(\n                    values=[df[col] for col in df.columns],\n                    fill_color=colors,\n                    align=\"left\",\n                ),\n            )\n        ]\n    )\n    fig.update_layout(\n        title={\n            \"text\": f\"Latency Summary: {title}\",\n            \"x\": 0.5,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        #     update margin to be 0\n        margin=dict(l=10, r=10, b=0),\n        # get the height to be whatever it requires\n        height=max((len(df) * 35), 300),\n    )\n    # Update layout for better appearance\n    desc_html = fig.to_html(full_html=False)\n    return desc_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.plot_timestamp_timeline","title":"<code>plot_timestamp_timeline(df)</code>  <code>staticmethod</code>","text":"<p>Plot the timestamp timeline Args:     df (pd.DataFrame): The dataframe</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plot in HTML</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef plot_timestamp_timeline(df: pd.DataFrame) -&gt; str:\n    \"\"\"\n    Plot the timestamp timeline\n    Args:\n        df (pd.DataFrame): The dataframe\n\n    Returns:\n        str: The plot in HTML\n    \"\"\"\n    fig = go.Figure()\n    y_values = range(len(df))\n\n    for i, row in df.iterrows():\n        y_value = y_values[i]\n        for col in df.columns[1:]:\n            if not pd.isna(row[col]):\n                fig.add_trace(\n                    go.Scatter(\n                        x=[row[col]],\n                        y=[y_value],\n                        mode=\"markers\",\n                        marker=dict(size=10),\n                        name=f\"{col}\",\n                        hovertemplate=\"%{x}&lt;br&gt;%{fullData.name}&lt;extra&gt;&lt;/extra&gt;\",\n                    )\n                )\n        # break\n    # Customize the layout\n    fig.update_layout(\n        title=\"Timeline of Time Points\",\n        xaxis_title=\"Time\",\n        # show nothing of y, even the label\n        yaxis=dict(\n            showticklabels=False, showline=False, showgrid=False, zeroline=True\n        ),\n        showlegend=True,\n    )\n    # Convert Plotly figure to HTML\n    plot_html = fig.to_html(full_html=False)\n    return plot_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.plot_timestamp_timeline_depth","title":"<code>plot_timestamp_timeline_depth(df, title)</code>  <code>staticmethod</code>","text":"<p>Plot the timestamp timeline Args:     df (pd.DataFrame): The dataframe     title (str): The title</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The plot in HTML</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef plot_timestamp_timeline_depth(df: pd.DataFrame, title: str) -&gt; str:\n    \"\"\"\n    Plot the timestamp timeline\n    Args:\n        df (pd.DataFrame): The dataframe\n        title (str): The title\n\n    Returns:\n        str: The plot in HTML\n    \"\"\"\n    fig = go.Figure()\n    y_values = list(range(len(df)))\n    shapes = []\n    for y_value in y_values:\n        shapes.append(\n            dict(\n                type=\"line\",\n                xref=\"paper\",\n                x0=0,\n                x1=1,\n                yref=\"y\",\n                y0=y_value,\n                y1=y_value,\n                line=dict(color=\"grey\", width=1, dash=\"dot\"),\n            )\n        )\n    y_labels = []\n\n    legend_added = {}\n    # Use Plotly's qualitative color sequence 'Dark24' to generate a spectrum of colors\n    colors = pcolors.qualitative.Dark24\n\n    # Dynamically generate a color map for each column\n    column_colors = {\n        col: colors[i % len(colors)] for i, col in enumerate(df.columns[1:])\n    }\n    for i, row in df.iterrows():\n        y_value = y_values[i]\n        y_labels.append(row[\"track_id\"].split(\"-\")[-1])\n        for col in df.columns[1:]:\n            if not pd.isna(row[col]):\n                show_legend = False\n                if col not in legend_added:\n                    show_legend = True\n                    legend_added[col] = True\n                fig.add_trace(\n                    go.Scatter(\n                        x=[row[col]],\n                        y=[y_value],\n                        mode=\"markers\",\n                        marker=dict(size=10, color=column_colors[col]),\n                        name=f\"{col}\",\n                        hovertemplate=\"%{x}&lt;br&gt;%{fullData.name}&lt;extra&gt;&lt;/extra&gt;\",\n                        showlegend=show_legend,\n                    )\n                )\n    # Customize the layout\n    fig.update_layout(\n        title={\n            \"text\": f\"Timeline of Events ({title})\",\n            \"x\": 0.5,\n            \"xanchor\": \"center\",\n            \"yanchor\": \"top\",\n        },\n        xaxis_title=\"Time\",\n        yaxis=dict(\n            showline=False,\n            showgrid=True,\n            zeroline=False,\n            tickvals=y_values,\n            ticktext=y_labels,\n            title=\"Track ID\",\n        ),\n        showlegend=True,\n        shapes=shapes,\n        height=max((len(df) * 35), 300),\n    )\n    # Convert Plotly figure to HTML\n    plot_html = fig.to_html(full_html=False)\n    return plot_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.process_cluster","title":"<code>process_cluster(cluster_name)</code>","text":"<p>Process the cluster Args:     cluster_name (str): The cluster name</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>def process_cluster(self, cluster_name: str):\n    \"\"\"\n    Process the cluster\n    Args:\n        cluster_name (str): The cluster name\n    \"\"\"\n    task_groups, required_tasks_count, tasks = extract_task_group(cluster_name)\n    general_desc = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n    general_desc += f\"&lt;p&gt;Required tasks: {required_tasks_count} | Total tasks groups: {len(task_groups)}&lt;/p&gt;\"\n    # loop through the task groups, if the success task is not == required_tasks_count, then we will skip\n    success_pipeline = 0\n    cluster_latency = []\n    for track_id, task_group in task_groups.items():\n        success_tasks = [\n            task for task in task_group if task.result_status == \"completed\"\n        ]\n        if len(success_tasks) != required_tasks_count:\n            # the pipeline is not completed, so we will skip\n            continue\n        success_pipeline += 1\n        cluster_latency.append(self.process_task_group(task_group))\n\n    logger.info(\n        f\"\"\"\n            Cluster: {cluster_name}, Success Ratio: {success_pipeline}/{len(task_groups)}\n            Required Components: {required_tasks_count}, Total tasks: {len(tasks)}\n        \"\"\"\n    )\n\n    general_title = f\"Cluster: &lt;b&gt;{cluster_name}&lt;/b&gt;, Completed Ratio: {success_pipeline}/{len(task_groups)}\"\n    # flatten the cluster_latency\n    result_df = pd.DataFrame(cluster_latency)\n    # get the column split with _ from right, and left element is the component name\n\n    if len(result_df) != 0:\n        logger.debug(result_df.describe())\n        # result_df.to_csv(settings.LOG_DIR / f\"{cluster_name}_benchmark.csv\")\n        # to html and return it\n        logger.debug(result_df.describe())\n        desc = result_df.describe().transpose()\n        desc = desc.round(4)\n\n        # add another column\n        # Extract model accuracy from index and add it as a new column\n        desc[\"latency_type\"] = desc.index.str.rsplit(\"_\", n=2).str[1]\n        # then update the index to two columns, first will be component\n        desc.index = desc.index.str.rsplit(\"_\", n=2, expand=True).get_level_values(\n            0\n        )\n        # reset index, get the index to be the column component\n        desc = desc.reset_index()\n        # rename the index to be component\n        desc = desc.rename(columns={\"index\": \"component\"})\n        desc_html = self.plot_table(desc, title=f\" ({general_title})\")\n        plot_html = self.plot_distribution(result_df, title=f\" ({general_title})\")\n\n        return general_desc + desc_html + plot_html\n    return general_desc\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.process_cluster_detail","title":"<code>process_cluster_detail(cluster_name)</code>","text":"<p>Process the cluster in detail Even if the track is not finished, we will still plot it and stop status Args:     cluster_name (str): html content</p> <p>Returns:</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>def process_cluster_detail(self, cluster_name: str) -&gt; str:\n    \"\"\"\n    Process the cluster in detail\n    Even if the track is not finished, we will still plot it and stop status\n    Args:\n        cluster_name (str): html content\n\n    Returns:\n\n    \"\"\"\n    task_groups, required_tasks_count, tasks = extract_task_group(cluster_name)\n    general_desc = f\"&lt;h2&gt;Cluster: {cluster_name}&lt;/h2&gt;\"\n    general_desc += f\"&lt;p&gt;Required tasks: {required_tasks_count} | Total tasks groups: {len(task_groups)}&lt;/p&gt;\"\n    # loop through the task groups, if the success task is not == required_tasks_count, then we will skip\n    success_pipeline = 0\n    cluster_latency = []\n    cluster_ts_latency = []\n    cluster_tp_latency = []\n    for track_id, task_group in task_groups.items():\n        success_tasks = [\n            task for task in task_group if task.result_status == \"completed\"\n        ]\n        if len(success_tasks) == required_tasks_count:\n            # the pipeline is not completed, so we will skip\n            success_pipeline += 1\n        cluster_latency.append(self.process_task_group_detail(task_group))\n        cluster_ts_latency.append(\n            self.process_task_group_detail_timeline(task_group)\n        )\n        cluster_tp_latency.append(\n            self.process_task_group_detail_timeline(task_group, timeline=True)\n        )\n    general_title = f\"Cluster: &lt;b&gt;{cluster_name}&lt;/b&gt;, Completed Ratio: {success_pipeline}/{len(task_groups)}\"\n    result_df = pd.DataFrame(cluster_latency)\n    if len(result_df) == 0:\n        return general_desc\n\n    # only keep the last element in the track_id\n    result_df[\"track_id\"] = result_df[\"track_id\"].str.split(\"-\").str[-1]\n    # get result into multiple level column, which will split current column into multiple level column name\n    # Split the column names into three parts, but only keep the first two\n    split_columns = result_df.columns.str.rsplit(\"_\", n=2, expand=True)\n\n    # we only need the first two level, so we will get the first two level\n    result_df.columns = [\n        split_columns.get_level_values(0),\n        split_columns.get_level_values(1),\n    ]\n    # sort the column\n    track_tasks_html = self.plot_table(result_df, title=f\" ({general_title})\")\n\n    # cluster ts latency\n    result_ts_df = pd.DataFrame(cluster_ts_latency)\n    # result_ts_df.to_csv(settings.LOG_DIR / f\"{cluster_name}_ts_benchmark.csv\")\n    if len(result_ts_df) == 0:\n        return track_tasks_html\n    # we will plot a bar\n    ts_stacked_html = self.plot_stacked_timeline(result_ts_df, title=general_title)\n\n    # grab the time point latency, and try to draw time point html\n    result_tp_df = pd.DataFrame(cluster_tp_latency)\n    # result_tp_df.to_csv(settings.LOG_DIR / f\"{cluster_name}_tp_benchmark.csv\")\n    ts_timepoint_html = self.plot_timestamp_timeline_depth(\n        result_tp_df, title=general_title\n    )\n    return general_desc + track_tasks_html + ts_stacked_html + ts_timepoint_html\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.process_task_group","title":"<code>process_task_group(task_track)</code>  <code>staticmethod</code>","text":"<p>This will process each component, and then extract the transfer and model latency total</p> <p>Parameters:</p> Name Type Description Default <code>task_track</code> <code>List[Task]</code> <p>The task track</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The benchmark result</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef process_task_group(task_track: List[Task]):\n    \"\"\"\n    This will process each component, and then extract the transfer and model latency total\n\n    Args:\n        task_track (List[Task]): The task track\n\n    Returns:\n        dict: The benchmark result\n    \"\"\"\n    result = {\n        \"track_id\": task_track[0].track_id,\n    }\n    task_names = get_task_names_order(result[\"track_id\"])\n    for task in task_track:\n        latency_profile = task.result_json.get(\"latency_profile\", {})\n        # NOTE: this will require client side do not log overlap durations\n        model_latency = 0\n        transfer_latency = 0\n        logger.debug(latency_profile)\n        task_start_time = None\n        task_end_time = None\n        for key, value in latency_profile.items():\n            if key.startswith(\"model\"):\n                model_latency += float(value)\n            if key.startswith(\"transfer\"):\n                transfer_latency += float(value)\n            if key.startswith(\"ts\"):\n                if key == \"ts_start_task\":\n                    task_start_time = value\n                if key == \"ts_end_task\":\n                    task_end_time = value\n        result[f\"{task.task_name}_model_latency\"] = model_latency\n        result[f\"{task.task_name}_transfer_latency\"] = transfer_latency\n        # look for the ts_start_task and ts_end_task, and the overall_latency should be that value\n        # process time into datetime object\n        # ts_end_trigger_emotion_model 2024-07-01T14:58:36.419352\n        if task_start_time and task_end_time:\n            task_start_time_dt = str_to_datetime(task_start_time)\n            task_end_time_dt = str_to_datetime(task_end_time)\n            result[f\"{task.task_name}_overall_latency\"] = (  # noqa\n                task_end_time_dt - task_start_time_dt\n            ).total_seconds()\n\n        else:\n            logger.error(f\"Task {task.task_name} does not have start and end time\")\n            result[f\"{task.task_name}_overall_latency\"] = (\n                model_latency + transfer_latency\n            )\n    # total_latency should be the sum of all the overall_latency\n    total_latency = 0\n    for key, value in result.items():\n        if key.endswith(\"overall_latency\"):\n            total_latency += value\n    result[\"total_latency\"] = total_latency\n    # loop all value, get it to decimal 4\n    for key, value in result.items():\n        if isinstance(value, float):\n            result[key] = round(value, 4)\n\n    ordered_result = {\n        \"track_id\": result[\"track_id\"],\n    }\n    for task_name in task_names:\n        ordered_result[task_name + \"_model_latency\"] = result[\n            task_name + \"_model_latency\"\n        ]\n        ordered_result[task_name + \"_transfer_latency\"] = result[\n            task_name + \"_transfer_latency\"\n        ]\n        ordered_result[task_name + \"_overall_latency\"] = result[\n            task_name + \"_overall_latency\"\n        ]\n    ordered_result[\"total_latency\"] = result[\"total_latency\"]\n    return ordered_result\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.process_task_group_detail","title":"<code>process_task_group_detail(task_track)</code>  <code>staticmethod</code>","text":"<p>This will process each component, and then extract the transfer and model latency total</p> <p>Parameters:</p> Name Type Description Default <code>task_track</code> <code>List[Task]</code> <p>The task track</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The benchmark result</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef process_task_group_detail(task_track: List[Task]):\n    \"\"\"\n    This will process each component, and then extract the transfer and model latency total\n\n    Args:\n        task_track (List[Task]): The task track\n\n    Returns:\n        dict: The benchmark result\n    \"\"\"\n    result = {\n        \"track_id\": task_track[0].track_id,\n    }\n    task_names = get_task_names_order(result[\"track_id\"])\n    for task in task_track:\n        if task.result_status != \"completed\":\n            result[f\"{task.task_name}_model_latency\"] = task.result_status\n            result[f\"{task.task_name}_transfer_latency\"] = task.result_status\n            result[f\"{task.task_name}_overall_latency\"] = task.result_status\n            continue\n        latency_profile = task.result_json.get(\"latency_profile\", {})\n        # NOTE: this will require client side do not log overlap durations\n        model_latency = 0\n        transfer_latency = 0\n        logger.debug(latency_profile)\n        task_start_time = None\n        task_end_time = None\n        for key, value in latency_profile.items():\n            if key.startswith(\"model\"):\n                model_latency += float(value)\n            if key.startswith(\"transfer\"):\n                transfer_latency += float(value)\n            if key.startswith(\"ts\"):\n                if key == \"ts_start_task\":\n                    task_start_time = value\n                if key == \"ts_end_task\":\n                    task_end_time = value\n        result[f\"{task.task_name}_model_latency\"] = model_latency\n        result[f\"{task.task_name}_transfer_latency\"] = transfer_latency\n        # look for the ts_start_task and ts_end_task, and the overall_latency should be that value\n        # process time into datetime object\n        # ts_end_trigger_emotion_model 2024-07-01T14:58:36.419352\n        if task_start_time and task_end_time:\n            task_start_time_dt = str_to_datetime(task_start_time)\n            task_end_time_dt = str_to_datetime(task_end_time)\n            result[f\"{task.task_name}_overall_latency\"] = (  # noqa\n                task_end_time_dt - task_start_time_dt\n            ).total_seconds()\n\n        else:\n            logger.error(f\"Task {task.task_name} does not have start and end time\")\n            result[f\"{task.task_name}_overall_latency\"] = (\n                model_latency + transfer_latency\n            )\n\n    # sort the key to be the same as the cluster order, also if missed, fill it with missing\n    for task_name in task_names:\n        if f\"{task_name}_overall_latency\" not in result:\n            result[task_name + \"_model_latency\"] = \"missing\"\n            result[task_name + \"_transfer_latency\"] = \"missing\"\n            result[task_name + \"_overall_latency\"] = \"missing\"\n\n    # total_latency should be the sum of all the overall_latency\n    total_latency = 0\n    for key, value in result.items():\n        if key.endswith(\"overall_latency\") and isinstance(value, float):\n            total_latency += value\n        elif key.endswith(\"overall_latency\") and not isinstance(value, float):\n            total_latency = \"incomplete\"\n            break\n    result[\"total_latency\"] = total_latency\n    # loop all value, get it to decimal 4\n    for key, value in result.items():\n        if isinstance(value, float):\n            result[key] = round(value, 4)\n\n    ordered_result = {\n        \"track_id\": result[\"track_id\"],\n    }\n    for task_name in task_names:\n        ordered_result[task_name + \"_model_latency\"] = result[\n            task_name + \"_model_latency\"\n        ]\n        ordered_result[task_name + \"_transfer_latency\"] = result[\n            task_name + \"_transfer_latency\"\n        ]\n        ordered_result[task_name + \"_overall_latency\"] = result[\n            task_name + \"_overall_latency\"\n        ]\n\n    ordered_result[\"total_latency\"] = result[\"total_latency\"]\n    return ordered_result\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.process_task_group_detail_timeline","title":"<code>process_task_group_detail_timeline(task_track, timeline=False)</code>  <code>staticmethod</code>","text":"<p>Based on the result_json =&gt; latency_profile We will gather the time point for each, and then change to the relative second value compared to start point</p> <p>If timeline is True, we will only grab the timestamp information. Otherwise, we will calculate the relative time to the start point</p> <p>In the end, we will grab the Args:     task_track (List[Task]): The task track     timeline (bool): If we want to plot the timeline</p> <p>Returns:</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>@staticmethod\ndef process_task_group_detail_timeline(\n    task_track: List[Task], timeline: bool = False\n):\n    \"\"\"\n    Based on the result_json =&gt; latency_profile\n    We will gather the time point for each, and then change to the relative second value compared to start point\n\n    If timeline is True, we will only grab the timestamp information.\n    Otherwise, we will calculate the relative time to the start point\n\n    In the end, we will grab the\n    Args:\n        task_track (List[Task]): The task track\n        timeline (bool): If we want to plot the timeline\n\n    Returns:\n\n    \"\"\"\n    result = {\n        \"track_id\": task_track[0].track_id,\n    }\n\n    task_names = get_task_names_order(result[\"track_id\"])\n\n    task_results = {}\n    for task in task_track:\n        if task.result_status != \"completed\":\n            continue\n        latency_profile = task.result_json.get(\"latency_profile\", {})\n        task_result = {}\n        for key, value in latency_profile.items():\n            if key.startswith(\"ts\"):\n                task_result[key] = str_to_datetime(value)\n\n        if timeline is False:\n            # sort out the whole task_result based on time timestamp\n            # and then calculate the relative time to the previous component\n            sorted_task_result = dict(\n                sorted(task_result.items(), key=lambda item: item[1])\n            )\n            previous_time = None\n            task_relative_time = {}\n            for key, value in sorted_task_result.items():\n                if previous_time is None:\n                    task_relative_time[key] = 0\n                else:\n                    task_relative_time[key] = (\n                        value - previous_time\n                    ).total_seconds()\n                previous_time = value\n            task_results[task.task_name] = task_relative_time\n        else:\n            task_results[task.task_name] = task_result\n\n    # sort the key to be the same as the cluster order, calculate the value to add up the previous component\n    first_start_task = None\n    for task_name in task_names:\n        if task_name not in task_results:\n            break\n        for key, value in task_results[task_name].items():\n            new_key = f\"{task_name}_{key.split('_', 1)[1]}\"\n            if key == \"ts_start_task\":\n                if first_start_task is None:\n                    first_start_task = value\n                else:\n                    continue\n            if new_key not in result:\n                result[new_key] = value\n\n    return result\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/latency_benchmark/#API.orchestrator.metrics.latency_benchmark.LatencyBenchmark.run","title":"<code>run()</code>","text":"<p>Run the benchmark</p> Source code in <code>API/orchestrator/metrics/latency_benchmark.py</code> <pre><code>def run(self):\n    \"\"\"\n    Run the benchmark\n    \"\"\"\n    html_content = \"\"\n    if self.benchmark_cluster == \"all\":\n        for cluster_name in CLUSTERS.keys():\n            # add a divider\n            html_content += \"&lt;hr&gt;\"\n            html_content += self.process_cluster(cluster_name)\n    else:\n        if self.benchmark_cluster not in CLUSTERS:\n            raise ValueError(f\"Cluster {self.benchmark_cluster} not found\")\n        html_content += \"&lt;hr&gt;\"\n        html_content += self.process_cluster(self.benchmark_cluster)\n    return html_content\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/utils/","title":"Utils","text":""},{"location":"Sources/API/orchestrator/metrics/utils/#API.orchestrator.metrics.utils.extract_task_group","title":"<code>extract_task_group(cluster_name)</code>","text":"<p>Extract the task group Args:     cluster_name (str): The cluster name</p> <p>Returns:</p> Source code in <code>API/orchestrator/metrics/utils.py</code> <pre><code>def extract_task_group(\n    cluster_name: str,\n) -&gt; Tuple[Dict[str, List[Task]], int, List[Task]]:\n    \"\"\"\n    Extract the task group\n    Args:\n        cluster_name (str): The cluster name\n\n    Returns:\n\n    \"\"\"\n    cluster = CLUSTERS.get(cluster_name, None)\n    if cluster is None:\n        raise ValueError(f\"Cluster {cluster_name} not found\")\n\n    required_tasks = [\n        item for item in cluster.values() if item[\"component_type\"] == \"task\"\n    ]\n    required_tasks_count = len(required_tasks)\n    logger.info(f\"Cluster: {cluster_name}, Required tasks: {required_tasks_count}\")\n    # get all related tasks, the track_id is like T_{cluster_name}_XXX\n    tasks = Task.objects.filter(track_id__startswith=f\"T-{cluster_name}-\")\n    logger.info(f\"Cluster: {cluster_name}, Total tasks: {len(tasks)}\")\n    # group the tasks by the track_id\n    task_groups = {}\n    for task in tasks:\n        track_id = task.track_id\n        if track_id not in task_groups:\n            task_groups[track_id] = []\n        task_groups[track_id].append(task)\n\n    # sort the task groups by the first task created time\n    task_groups = dict(\n        sorted(\n            task_groups.items(),\n            key=lambda x: x[1][0].created_at if len(x[1]) &gt; 0 else None,\n        )\n    )\n    return task_groups, required_tasks_count, tasks\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/utils/#API.orchestrator.metrics.utils.get_task_names_order","title":"<code>get_task_names_order(track_id)</code>","text":"<p>Get the task names order Args:     track_id (str): The track ID</p> <p>Returns:</p> Name Type Description <code>str</code> <code>List[str]</code> <p>The task names order</p> Source code in <code>API/orchestrator/metrics/utils.py</code> <pre><code>def get_task_names_order(track_id: str) -&gt; List[str]:\n    \"\"\"\n    Get the task names order\n    Args:\n        track_id (str): The track ID\n\n    Returns:\n        str: The task names order\n\n    \"\"\"\n    cluster_name = track_id.split(\"-\")[1]\n    cluster = CLUSTERS.get(cluster_name)\n    task_name_order = [\n        item for item in cluster.values() if item[\"component_type\"] == \"task\"\n    ]\n    task_name_order = sorted(task_name_order, key=lambda x: x[\"order\"])\n    task_names = [item[\"task_name\"] for item in task_name_order]\n    return task_names\n</code></pre>"},{"location":"Sources/API/orchestrator/metrics/utils/#API.orchestrator.metrics.utils.str_to_datetime","title":"<code>str_to_datetime(datetime_str)</code>","text":"<p>Convert the datetime string to datetime object Args:     datetime_str (str): the string datetime, like this: 2024-07-01T14:58:36.419352</p> <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>The datetime object</p> Source code in <code>API/orchestrator/metrics/utils.py</code> <pre><code>def str_to_datetime(datetime_str: str) -&gt; datetime:\n    \"\"\"\n    Convert the datetime string to datetime object\n    Args:\n        datetime_str (str): the string datetime, like this: 2024-07-01T14:58:36.419352\n\n    Returns:\n        datetime: The datetime object\n    \"\"\"\n    return datetime.strptime(datetime_str, \"%Y-%m-%dT%H:%M:%S.%f\")\n</code></pre>"},{"location":"Sources/API/orchestrator/migrations/0001_init/","title":"0001 init","text":""},{"location":"Sources/Agent/main/","title":"main","text":""},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator","title":"<code>AIOrchestrator</code>","text":"<p>This is the AI Orchestrator</p> <p>We will pull the task from the API end And then based on which type of the task it is, we will send it to the respective handler</p> Source code in <code>Agent/main.py</code> <pre><code>class AIOrchestrator:\n    \"\"\"\n    This is the AI Orchestrator\n\n    We will pull the task from the API end\n    And then based on which type of the task it is, we will send it to the respective handler\n    \"\"\"\n\n    def __init__(\n        self,\n        api_domain: str,\n        token: str,\n        task_name: Optional[str] = \"all\",\n        time_sleep: Optional[float] = 1.5,\n    ):\n        \"\"\"\n        Initialize the AI Orchestrator\n        Args:\n            api_domain (str): The API Domain\n            token (str): The API Token\n            task_name (str): The task name. Default is \"all\"\n            time_sleep (float): The time to sleep. Default is 1.5 during each loop\n        \"\"\"\n        self.uuid = str(uuid.uuid4())\n        self.api_domain = api_domain\n        self.token = token\n        self.task_name = task_name\n        self.api = API(\n            domain=api_domain, token=token, task_name=task_name, uuid=self.uuid\n        )\n        self.api.register_or_update_worker()\n        self.storage_solution = self.api.get_storage_solution()\n        # controller\n        self.counter = 0\n        self.time_sleep = time_sleep\n\n        # first check the authentication of the token valid or not\n        if not self.authenticate_token():\n            raise Exception(\"Token is not valid\")\n\n        if not self.pre_env_check():\n            raise Exception(\"Pre Environment Check Failed\")\n\n        self.speech2text = None\n        self.text2speech = None\n        self.emotion_detection = None\n        self.quantization_llm = None\n        self.hf_llm = None\n        self.general_ml = None\n        self.openai_handler = None\n        self.rag_handler = None\n\n        self.task_name_router = {\n            TaskName.speech2text.value: self.handle_speech2text_task,\n            TaskName.text2speech.value: self.handle_text2speech_task,\n            TaskName.emotion_detection.value: self.handle_emotion_detection_task,\n            TaskName.quantization_llm.value: self.handle_quantization_llm_task,\n            TaskName.hf_llm.value: self.handle_hf_llm_task,\n            TaskName.general_ml.value: self.handle_general_ml_task,\n            TaskName.openai_gpt4o.value: self.handle_openai_task,\n            TaskName.openai_speech2text.value: self.handle_openai_task,\n            TaskName.openai_text2speech.value: self.handle_openai_task,\n            TaskName.openai_gpt4o_text_only.value: self.handle_openai_task,\n            TaskName.openai_gpt_4o_text_and_image.value: self.handle_openai_task,\n            TaskName.rag.value: self.handle_rag_task,\n        }\n\n    def authenticate_token(self):\n        \"\"\"\n        Authenticate the token\n        Returns:\n            bool: True if the token is valid\n        \"\"\"\n        return self.api.verify_token()\n\n    def pre_env_check(self):\n        # if task is text 2 speech, check openai key\n        load_dotenv()\n        if self.task_name in [\"all\", \"text2speech\"]:\n            # check openai key\n            openai_key = os.getenv(\"OPENAI_API_KEY\")\n            if openai_key is None:\n                # READ from .env, and set it\n                # if it not exists, then return False\n                logger.error(\"OpenAI API Key is not set\")\n                return False\n        if self.task_name in [\"all\", \"hf_llm\"]:\n            # check openai key\n            openai_key = os.getenv(\"HF_TOKEN\")\n            if openai_key is None:\n                logger.error(\"OpenAI HF TOKEN is not set\")\n                return False\n        return True\n\n    def run(self):\n        logger.info(f\"AI Worker Running UUID: {self.uuid}\")\n        while True:\n            self.counter += 1\n            if self.counter % 50 == 0:\n                # report to the cloud that we are still alive\n                logger.info(f\"Still alive. Counter: {self.counter}\")\n                self.api.register_or_update_worker()\n            try:\n                with timer(logger=logger, message=\"get_task\"):\n                    task = self.api.get_task()\n                # after get the task, then feed it to the model to evaluate the model params\n                if task is None:\n                    logger.info(\"No task found\")\n                    time.sleep(self.time_sleep)\n                    continue\n                self.handle_task(task)\n            # allow it accepts keyboard interrupt\n            except KeyboardInterrupt:\n                logger.info(\"Keyboard Interrupt\")\n                break\n            except Exception as e:\n                logger.exception(e)\n            time.sleep(self.time_sleep)\n\n    def handle_task(self, task: dict):\n        \"\"\"\n        Handle the task\n        Args:\n            task (dict): The task\n        \"\"\"\n        task_obj = Task(**task)\n        TimeLogger.log_task(task_obj, \"start_task\")\n        if task_obj.task_name in self.task_name_router:\n            task_obj = self.task_name_router[task_obj.task_name](task_obj)\n        elif \"openai\" in task_obj.task_name:\n            task_obj = self.handle_openai_task(task_obj)\n        else:\n            logger.error(f\"Unknown task type: {task_obj.task_name}\")\n            task_obj.result_status = ResultStatus.failed.value\n            task_obj.description = f\"Unknown task type: {task_obj.task_name}\"\n        TimeLogger.log_task(task_obj, \"end_task\")\n        # then update the task status\n        self.api.post_task_result(task_obj)\n\n    def handle_speech2text_task(self, task: Task):\n        \"\"\"\n        Handle the speech2text task\n        Args:\n            task (Task): The task\n        \"\"\"\n        if self.speech2text is None:\n            self.speech2text = Speech2Text()\n        task = self.speech2text.handle_task(task)\n        return task\n\n    def handle_text2speech_task(self, task: Task):\n        \"\"\"\n        Handle the text2speech task\n        Args:\n            task (Task): The task\n        \"\"\"\n        if self.text2speech is None:\n            self.text2speech = Text2Speech()\n        task = self.text2speech.handle_task(task)\n        return task\n\n    def handle_emotion_detection_task(self, task: Task):\n        \"\"\"\n        Handle the emotion detection task\n        Args:\n            task (Task): The task\n        \"\"\"\n        if self.emotion_detection is None:\n            self.emotion_detection = EmotionDetectionHandler()\n        task = self.emotion_detection.handle_task(task)\n        return task\n\n    def handle_quantization_llm_task(self, task: Task):\n        \"\"\"\n        Handle the quantization llm task\n        Args:\n            task (Task): The task\n        \"\"\"\n        if self.quantization_llm is None:\n            self.quantization_llm = QuantizationLLM(api=self.api)\n        task = self.quantization_llm.handle_task(task)\n        return task\n\n    def handle_hf_llm_task(self, task: Task):\n        \"\"\"\n        Handle the hf llm task which will require more time compare to other tasks\n        Args:\n            task (Task): The task\n\n        Returns:\n\n        \"\"\"\n        if self.hf_llm is None:\n            self.hf_llm = HFLLM()\n        task = self.hf_llm.handle_task(task)\n        return task\n\n    def handle_general_ml_task(self, task: Task):\n        \"\"\"\n        Handle the general ml task\n        Args:\n            task (Task): The task\n\n        Returns:\n\n        \"\"\"\n        if self.general_ml is None:\n            self.general_ml = GeneralMLModel()\n        task = self.general_ml.handle_task(task)\n        return task\n\n    def handle_openai_task(self, task: Task):\n        \"\"\"\n        Handle the openai task\n        Args:\n            task (Task): The task\n\n        Returns:\n\n        \"\"\"\n        if self.openai_handler is None:\n            self.openai_handler = OpenAIHandler()\n        task = self.openai_handler.handle_task(task)\n        return task\n\n    def handle_rag_task(self, task: Task):\n        \"\"\"\n        Handle the rag task\n        Args:\n            task (Task): The task\n\n        Returns:\n\n        \"\"\"\n        if self.rag_handler is None:\n            self.rag_handler = RAGHandler()\n        task = self.rag_handler.handle_task(task)\n        return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.__init__","title":"<code>__init__(api_domain, token, task_name='all', time_sleep=1.5)</code>","text":"<p>Initialize the AI Orchestrator Args:     api_domain (str): The API Domain     token (str): The API Token     task_name (str): The task name. Default is \"all\"     time_sleep (float): The time to sleep. Default is 1.5 during each loop</p> Source code in <code>Agent/main.py</code> <pre><code>def __init__(\n    self,\n    api_domain: str,\n    token: str,\n    task_name: Optional[str] = \"all\",\n    time_sleep: Optional[float] = 1.5,\n):\n    \"\"\"\n    Initialize the AI Orchestrator\n    Args:\n        api_domain (str): The API Domain\n        token (str): The API Token\n        task_name (str): The task name. Default is \"all\"\n        time_sleep (float): The time to sleep. Default is 1.5 during each loop\n    \"\"\"\n    self.uuid = str(uuid.uuid4())\n    self.api_domain = api_domain\n    self.token = token\n    self.task_name = task_name\n    self.api = API(\n        domain=api_domain, token=token, task_name=task_name, uuid=self.uuid\n    )\n    self.api.register_or_update_worker()\n    self.storage_solution = self.api.get_storage_solution()\n    # controller\n    self.counter = 0\n    self.time_sleep = time_sleep\n\n    # first check the authentication of the token valid or not\n    if not self.authenticate_token():\n        raise Exception(\"Token is not valid\")\n\n    if not self.pre_env_check():\n        raise Exception(\"Pre Environment Check Failed\")\n\n    self.speech2text = None\n    self.text2speech = None\n    self.emotion_detection = None\n    self.quantization_llm = None\n    self.hf_llm = None\n    self.general_ml = None\n    self.openai_handler = None\n    self.rag_handler = None\n\n    self.task_name_router = {\n        TaskName.speech2text.value: self.handle_speech2text_task,\n        TaskName.text2speech.value: self.handle_text2speech_task,\n        TaskName.emotion_detection.value: self.handle_emotion_detection_task,\n        TaskName.quantization_llm.value: self.handle_quantization_llm_task,\n        TaskName.hf_llm.value: self.handle_hf_llm_task,\n        TaskName.general_ml.value: self.handle_general_ml_task,\n        TaskName.openai_gpt4o.value: self.handle_openai_task,\n        TaskName.openai_speech2text.value: self.handle_openai_task,\n        TaskName.openai_text2speech.value: self.handle_openai_task,\n        TaskName.openai_gpt4o_text_only.value: self.handle_openai_task,\n        TaskName.openai_gpt_4o_text_and_image.value: self.handle_openai_task,\n        TaskName.rag.value: self.handle_rag_task,\n    }\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.authenticate_token","title":"<code>authenticate_token()</code>","text":"<p>Authenticate the token Returns:     bool: True if the token is valid</p> Source code in <code>Agent/main.py</code> <pre><code>def authenticate_token(self):\n    \"\"\"\n    Authenticate the token\n    Returns:\n        bool: True if the token is valid\n    \"\"\"\n    return self.api.verify_token()\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_emotion_detection_task","title":"<code>handle_emotion_detection_task(task)</code>","text":"<p>Handle the emotion detection task Args:     task (Task): The task</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_emotion_detection_task(self, task: Task):\n    \"\"\"\n    Handle the emotion detection task\n    Args:\n        task (Task): The task\n    \"\"\"\n    if self.emotion_detection is None:\n        self.emotion_detection = EmotionDetectionHandler()\n    task = self.emotion_detection.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_general_ml_task","title":"<code>handle_general_ml_task(task)</code>","text":"<p>Handle the general ml task Args:     task (Task): The task</p> <p>Returns:</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_general_ml_task(self, task: Task):\n    \"\"\"\n    Handle the general ml task\n    Args:\n        task (Task): The task\n\n    Returns:\n\n    \"\"\"\n    if self.general_ml is None:\n        self.general_ml = GeneralMLModel()\n    task = self.general_ml.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_hf_llm_task","title":"<code>handle_hf_llm_task(task)</code>","text":"<p>Handle the hf llm task which will require more time compare to other tasks Args:     task (Task): The task</p> <p>Returns:</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_hf_llm_task(self, task: Task):\n    \"\"\"\n    Handle the hf llm task which will require more time compare to other tasks\n    Args:\n        task (Task): The task\n\n    Returns:\n\n    \"\"\"\n    if self.hf_llm is None:\n        self.hf_llm = HFLLM()\n    task = self.hf_llm.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_openai_task","title":"<code>handle_openai_task(task)</code>","text":"<p>Handle the openai task Args:     task (Task): The task</p> <p>Returns:</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_openai_task(self, task: Task):\n    \"\"\"\n    Handle the openai task\n    Args:\n        task (Task): The task\n\n    Returns:\n\n    \"\"\"\n    if self.openai_handler is None:\n        self.openai_handler = OpenAIHandler()\n    task = self.openai_handler.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_quantization_llm_task","title":"<code>handle_quantization_llm_task(task)</code>","text":"<p>Handle the quantization llm task Args:     task (Task): The task</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_quantization_llm_task(self, task: Task):\n    \"\"\"\n    Handle the quantization llm task\n    Args:\n        task (Task): The task\n    \"\"\"\n    if self.quantization_llm is None:\n        self.quantization_llm = QuantizationLLM(api=self.api)\n    task = self.quantization_llm.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_rag_task","title":"<code>handle_rag_task(task)</code>","text":"<p>Handle the rag task Args:     task (Task): The task</p> <p>Returns:</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_rag_task(self, task: Task):\n    \"\"\"\n    Handle the rag task\n    Args:\n        task (Task): The task\n\n    Returns:\n\n    \"\"\"\n    if self.rag_handler is None:\n        self.rag_handler = RAGHandler()\n    task = self.rag_handler.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_speech2text_task","title":"<code>handle_speech2text_task(task)</code>","text":"<p>Handle the speech2text task Args:     task (Task): The task</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_speech2text_task(self, task: Task):\n    \"\"\"\n    Handle the speech2text task\n    Args:\n        task (Task): The task\n    \"\"\"\n    if self.speech2text is None:\n        self.speech2text = Speech2Text()\n    task = self.speech2text.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task (dict): The task</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_task(self, task: dict):\n    \"\"\"\n    Handle the task\n    Args:\n        task (dict): The task\n    \"\"\"\n    task_obj = Task(**task)\n    TimeLogger.log_task(task_obj, \"start_task\")\n    if task_obj.task_name in self.task_name_router:\n        task_obj = self.task_name_router[task_obj.task_name](task_obj)\n    elif \"openai\" in task_obj.task_name:\n        task_obj = self.handle_openai_task(task_obj)\n    else:\n        logger.error(f\"Unknown task type: {task_obj.task_name}\")\n        task_obj.result_status = ResultStatus.failed.value\n        task_obj.description = f\"Unknown task type: {task_obj.task_name}\"\n    TimeLogger.log_task(task_obj, \"end_task\")\n    # then update the task status\n    self.api.post_task_result(task_obj)\n</code></pre>"},{"location":"Sources/Agent/main/#Agent.main.AIOrchestrator.handle_text2speech_task","title":"<code>handle_text2speech_task(task)</code>","text":"<p>Handle the text2speech task Args:     task (Task): The task</p> Source code in <code>Agent/main.py</code> <pre><code>def handle_text2speech_task(self, task: Task):\n    \"\"\"\n    Handle the text2speech task\n    Args:\n        task (Task): The task\n    \"\"\"\n    if self.text2speech is None:\n        self.text2speech = Text2Speech()\n    task = self.text2speech.handle_task(task)\n    return task\n</code></pre>"},{"location":"Sources/Agent/setup/","title":"setup","text":""},{"location":"Sources/Agent/storage/","title":"storage","text":"<p>This is the storage module.</p> <p>It will include two process</p> <ul> <li>One is to pull data down</li> <li>Another is upload data</li> </ul>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution","title":"<code>StorageSolution</code>","text":"Source code in <code>Agent/storage.py</code> <pre><code>class StorageSolution:\n    def __init__(\n            self,\n            api_domain: str,\n            token: str,\n            input_source_dir: str = None,\n            output_dest_dir: str = None,\n            dest_password: str = None,\n    ):\n        self.api_domain = api_domain\n        self.token = token\n        self.api = API(domain=api_domain, token=token)\n        self.storage_solution = self.api.get_storage_solution()\n        self.input_source_dir = input_source_dir\n        self.output_dest_dir = output_dest_dir\n        self.dest_password = dest_password\n\n    def sync_push_data(self):\n        \"\"\"\n        Sync the data to the storage\n        \"\"\"\n        if self.storage_solution == \"volume\":\n            return\n        if self.storage_solution == \"s3\":\n            self.sync_push_s3()\n        if self.storage_solution == \"local\":\n            self.sync_push_local()\n        if self.storage_solution == \"api\":\n            self.sync_push_api()\n\n    def sync_push_local(self):\n        \"\"\"\n        Sync the data to the local network\n        \"\"\"\n        observer = Observer()\n        local_handler = LocalSyncHandler(\n            src_path=str(DATA_DIR / \"tts\"),\n            dest_path=self.output_dest_dir,\n            sshpass=self.dest_password,\n        )\n        observer.schedule(local_handler, str(DATA_DIR / \"tts\"), recursive=True)\n        observer.start()\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()\n\n    @staticmethod\n    def sync_push_s3():\n        \"\"\"\n        Sync the data to the s3\n        \"\"\"\n        observer = Observer()\n        s3_handler = S3SyncHandler(s3_client=boto3.client(\"s3\"))\n        observer.schedule(s3_handler, str(DATA_DIR / \"tts\"), recursive=True)\n        observer.start()\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()\n\n    def sync_push_api(self):\n        \"\"\"\n        Sync the data to the api\n        \"\"\"\n        observer = Observer()\n        api_handler = APISyncHandler(self.api)\n        logger.info(str(DATA_DIR / \"tts\"))\n        observer.schedule(api_handler, str(DATA_DIR / \"tts\"), recursive=True)\n        observer.start()\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()\n\n    def sync_pull_data(self):\n        \"\"\"\n        If storage solution is volume or local, this means the data is accessible locally, do not need to worry about it\n        This will first call cloud to list all audio and video files\n        And then compare them with local ones\n        If there is any new files, download them\n\n        Returns:\n\n        \"\"\"\n        if self.storage_solution == \"volume\":\n            return\n        if self.storage_solution == \"local\":\n            self.sync_pull_local()\n        if self.storage_solution == \"s3\":\n            self.sync_pull_s3()\n        if self.storage_solution == \"api\":\n            self.sync_pull_api()\n\n    def sync_pull_local(self):\n        \"\"\"\n        Sync the data from the local network\n        directly run the rsync command\n        \"\"\"\n        while True:\n            os.system(\n                \"sshpass -p {} rsync -avz {} {}\".format(self.dest_password, self.input_source_dir,\n                                                        str(CLIENT_DATA_FOLDER))\n            )\n            time.sleep(1)\n\n    def sync_pull_s3(self):\n        \"\"\"\n        Sync the data from s3\n        \"\"\"\n        pass\n\n    def sync_pull_api(self):\n        \"\"\"\n        Sync the data from api\n        \"\"\"\n        from_time = None\n        while True:\n            try:\n                logger.info(f\"Syncing data from {from_time}\")\n                files = self.api.list_files(from_time=from_time)\n                # set from time to now for the next sync in timestamp format\n                from_time = time.time()\n                self.download_data(files)\n            except Exception as e:\n                logger.error(f\"Error syncing data: {e}\")\n                logger.exception(e)\n            time.sleep(1)\n\n    def download_data(self, files):\n        \"\"\"\n        Download the data from the cloud\n        Args:\n            files:\n\n        Returns:\n\n        \"\"\"\n        audio_files = files.get(\"audio_files\", [])\n        video_files = files.get(\"video_files\", [])\n        logger.info(\n            f\"Checking {len(audio_files)} audio files and {len(video_files)} video files\"\n        )\n        for audio_file in audio_files:\n            dest_path = (\n                    CLIENT_DATA_FOLDER\n                    / \"audio\"\n                    / audio_file[\"uid\"]\n                    / audio_file[\"audio_file\"]\n            )\n            if not dest_path.exists():\n                # TODO: do the download here\n                logger.info(f\"Downloading {audio_file['audio_file']} to {dest_path}\")\n                dest_path.parent.mkdir(parents=True, exist_ok=True)\n                self.download_audio(audio_file[\"id\"], dest_path)\n        for video_file in video_files:\n            dest_path = (\n                    CLIENT_DATA_FOLDER\n                    / \"videos\"\n                    / video_file[\"uid\"]\n                    / video_file[\"video_file\"]\n            )\n            if not dest_path.exists():\n                # TODO: do the download here\n                logger.info(f\"Downloading {video_file['video_file']} to {dest_path}\")\n                dest_path.parent.mkdir(parents=True, exist_ok=True)\n                self.download_video(video_file[\"id\"], dest_path)\n\n    def download_audio(self, audio_file_id, dest_path: Path):\n        \"\"\"\n        Download the audio file\n        Args:\n            audio_file_id (str): the audio file id\n            dest_path (str): the destination\n\n        Returns:\n\n        \"\"\"\n        link_json = self.api.download_file_link(audio_file_id, \"audio\")\n        audio_url = link_json.get(\"audio_url\", None)\n        if audio_url is None:\n            return\n\n        try:\n            r = requests.get(audio_url, stream=True)\n\n            if r.status_code != 404:\n                with open(dest_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=1024):\n                        if chunk:\n                            f.write(chunk)\n            else:\n                logger.error(f\"Error downloading audio file: {audio_url}, NOT FOUND\")\n        except Exception as e:\n            logger.error(f\"Error downloading audio file: {e}\")\n\n    def download_video(self, video_file_id, dest_path: Path):\n        \"\"\"\n        Download the video file\n        Args:\n            video_file_id (str): the video file id\n            dest_path (str): the destination\n\n        Returns:\n\n        \"\"\"\n        link_json = self.api.download_file_link(video_file_id, \"video\")\n        video_url = link_json.get(\"video_url\", None)\n        frames = link_json.get(\"frames\", None)\n        logger.info(f\"video_url: {video_url}, frames: {frames}\")\n        if video_url is not None:\n            try:\n                r = requests.get(video_url, stream=True)\n                if r.status_code != 404:\n                    with open(dest_path, \"wb\") as f:\n                        for chunk in r.iter_content(chunk_size=1024):\n                            if chunk:\n                                f.write(chunk)\n                else:\n                    logger.error(\n                        f\"Error downloading video file: {video_url}, NOT FOUND\"\n                    )\n            except Exception as e:\n                logger.error(f\"Error downloading video file: {e}\")\n\n        for frame_url in frames:\n            # rsplit from the third /, get the last part\n\n            frame_path = dest_path.parent / \"frames\" / frame_url.rsplit(\"/\", 3)[-1]\n            logger.info(f\"Downloading frame {frame_url} to {frame_path}\")\n            if frame_path.exists():\n                continue\n            try:\n                r = requests.get(frame_url, stream=True)\n                with open(frame_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=1024):\n                        if chunk:\n                            f.write(chunk)\n            except Exception as e:\n                logger.error(f\"Error downloading frame file: {e}\")\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.download_audio","title":"<code>download_audio(audio_file_id, dest_path)</code>","text":"<p>Download the audio file Args:     audio_file_id (str): the audio file id     dest_path (str): the destination</p> <p>Returns:</p> Source code in <code>Agent/storage.py</code> <pre><code>def download_audio(self, audio_file_id, dest_path: Path):\n    \"\"\"\n    Download the audio file\n    Args:\n        audio_file_id (str): the audio file id\n        dest_path (str): the destination\n\n    Returns:\n\n    \"\"\"\n    link_json = self.api.download_file_link(audio_file_id, \"audio\")\n    audio_url = link_json.get(\"audio_url\", None)\n    if audio_url is None:\n        return\n\n    try:\n        r = requests.get(audio_url, stream=True)\n\n        if r.status_code != 404:\n            with open(dest_path, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk:\n                        f.write(chunk)\n        else:\n            logger.error(f\"Error downloading audio file: {audio_url}, NOT FOUND\")\n    except Exception as e:\n        logger.error(f\"Error downloading audio file: {e}\")\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.download_data","title":"<code>download_data(files)</code>","text":"<p>Download the data from the cloud Args:     files:</p> <p>Returns:</p> Source code in <code>Agent/storage.py</code> <pre><code>def download_data(self, files):\n    \"\"\"\n    Download the data from the cloud\n    Args:\n        files:\n\n    Returns:\n\n    \"\"\"\n    audio_files = files.get(\"audio_files\", [])\n    video_files = files.get(\"video_files\", [])\n    logger.info(\n        f\"Checking {len(audio_files)} audio files and {len(video_files)} video files\"\n    )\n    for audio_file in audio_files:\n        dest_path = (\n                CLIENT_DATA_FOLDER\n                / \"audio\"\n                / audio_file[\"uid\"]\n                / audio_file[\"audio_file\"]\n        )\n        if not dest_path.exists():\n            # TODO: do the download here\n            logger.info(f\"Downloading {audio_file['audio_file']} to {dest_path}\")\n            dest_path.parent.mkdir(parents=True, exist_ok=True)\n            self.download_audio(audio_file[\"id\"], dest_path)\n    for video_file in video_files:\n        dest_path = (\n                CLIENT_DATA_FOLDER\n                / \"videos\"\n                / video_file[\"uid\"]\n                / video_file[\"video_file\"]\n        )\n        if not dest_path.exists():\n            # TODO: do the download here\n            logger.info(f\"Downloading {video_file['video_file']} to {dest_path}\")\n            dest_path.parent.mkdir(parents=True, exist_ok=True)\n            self.download_video(video_file[\"id\"], dest_path)\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.download_video","title":"<code>download_video(video_file_id, dest_path)</code>","text":"<p>Download the video file Args:     video_file_id (str): the video file id     dest_path (str): the destination</p> <p>Returns:</p> Source code in <code>Agent/storage.py</code> <pre><code>def download_video(self, video_file_id, dest_path: Path):\n    \"\"\"\n    Download the video file\n    Args:\n        video_file_id (str): the video file id\n        dest_path (str): the destination\n\n    Returns:\n\n    \"\"\"\n    link_json = self.api.download_file_link(video_file_id, \"video\")\n    video_url = link_json.get(\"video_url\", None)\n    frames = link_json.get(\"frames\", None)\n    logger.info(f\"video_url: {video_url}, frames: {frames}\")\n    if video_url is not None:\n        try:\n            r = requests.get(video_url, stream=True)\n            if r.status_code != 404:\n                with open(dest_path, \"wb\") as f:\n                    for chunk in r.iter_content(chunk_size=1024):\n                        if chunk:\n                            f.write(chunk)\n            else:\n                logger.error(\n                    f\"Error downloading video file: {video_url}, NOT FOUND\"\n                )\n        except Exception as e:\n            logger.error(f\"Error downloading video file: {e}\")\n\n    for frame_url in frames:\n        # rsplit from the third /, get the last part\n\n        frame_path = dest_path.parent / \"frames\" / frame_url.rsplit(\"/\", 3)[-1]\n        logger.info(f\"Downloading frame {frame_url} to {frame_path}\")\n        if frame_path.exists():\n            continue\n        try:\n            r = requests.get(frame_url, stream=True)\n            with open(frame_path, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk:\n                        f.write(chunk)\n        except Exception as e:\n            logger.error(f\"Error downloading frame file: {e}\")\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_pull_api","title":"<code>sync_pull_api()</code>","text":"<p>Sync the data from api</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_pull_api(self):\n    \"\"\"\n    Sync the data from api\n    \"\"\"\n    from_time = None\n    while True:\n        try:\n            logger.info(f\"Syncing data from {from_time}\")\n            files = self.api.list_files(from_time=from_time)\n            # set from time to now for the next sync in timestamp format\n            from_time = time.time()\n            self.download_data(files)\n        except Exception as e:\n            logger.error(f\"Error syncing data: {e}\")\n            logger.exception(e)\n        time.sleep(1)\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_pull_data","title":"<code>sync_pull_data()</code>","text":"<p>If storage solution is volume or local, this means the data is accessible locally, do not need to worry about it This will first call cloud to list all audio and video files And then compare them with local ones If there is any new files, download them</p> <p>Returns:</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_pull_data(self):\n    \"\"\"\n    If storage solution is volume or local, this means the data is accessible locally, do not need to worry about it\n    This will first call cloud to list all audio and video files\n    And then compare them with local ones\n    If there is any new files, download them\n\n    Returns:\n\n    \"\"\"\n    if self.storage_solution == \"volume\":\n        return\n    if self.storage_solution == \"local\":\n        self.sync_pull_local()\n    if self.storage_solution == \"s3\":\n        self.sync_pull_s3()\n    if self.storage_solution == \"api\":\n        self.sync_pull_api()\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_pull_local","title":"<code>sync_pull_local()</code>","text":"<p>Sync the data from the local network directly run the rsync command</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_pull_local(self):\n    \"\"\"\n    Sync the data from the local network\n    directly run the rsync command\n    \"\"\"\n    while True:\n        os.system(\n            \"sshpass -p {} rsync -avz {} {}\".format(self.dest_password, self.input_source_dir,\n                                                    str(CLIENT_DATA_FOLDER))\n        )\n        time.sleep(1)\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_pull_s3","title":"<code>sync_pull_s3()</code>","text":"<p>Sync the data from s3</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_pull_s3(self):\n    \"\"\"\n    Sync the data from s3\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_push_api","title":"<code>sync_push_api()</code>","text":"<p>Sync the data to the api</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_push_api(self):\n    \"\"\"\n    Sync the data to the api\n    \"\"\"\n    observer = Observer()\n    api_handler = APISyncHandler(self.api)\n    logger.info(str(DATA_DIR / \"tts\"))\n    observer.schedule(api_handler, str(DATA_DIR / \"tts\"), recursive=True)\n    observer.start()\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_push_data","title":"<code>sync_push_data()</code>","text":"<p>Sync the data to the storage</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_push_data(self):\n    \"\"\"\n    Sync the data to the storage\n    \"\"\"\n    if self.storage_solution == \"volume\":\n        return\n    if self.storage_solution == \"s3\":\n        self.sync_push_s3()\n    if self.storage_solution == \"local\":\n        self.sync_push_local()\n    if self.storage_solution == \"api\":\n        self.sync_push_api()\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_push_local","title":"<code>sync_push_local()</code>","text":"<p>Sync the data to the local network</p> Source code in <code>Agent/storage.py</code> <pre><code>def sync_push_local(self):\n    \"\"\"\n    Sync the data to the local network\n    \"\"\"\n    observer = Observer()\n    local_handler = LocalSyncHandler(\n        src_path=str(DATA_DIR / \"tts\"),\n        dest_path=self.output_dest_dir,\n        sshpass=self.dest_password,\n    )\n    observer.schedule(local_handler, str(DATA_DIR / \"tts\"), recursive=True)\n    observer.start()\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()\n</code></pre>"},{"location":"Sources/Agent/storage/#Agent.storage.StorageSolution.sync_push_s3","title":"<code>sync_push_s3()</code>  <code>staticmethod</code>","text":"<p>Sync the data to the s3</p> Source code in <code>Agent/storage.py</code> <pre><code>@staticmethod\ndef sync_push_s3():\n    \"\"\"\n    Sync the data to the s3\n    \"\"\"\n    observer = Observer()\n    s3_handler = S3SyncHandler(s3_client=boto3.client(\"s3\"))\n    observer.schedule(s3_handler, str(DATA_DIR / \"tts\"), recursive=True)\n    observer.start()\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()\n</code></pre>"},{"location":"Sources/Agent/models/parameters/","title":"Parameters","text":""},{"location":"Sources/Agent/models/results/","title":"Results","text":""},{"location":"Sources/Agent/models/task/","title":"Task","text":""},{"location":"Sources/Agent/models/task/#Agent.models.task.Task","title":"<code>Task</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The Task Model This is the one we will pull and ask for the task from the API</p> Source code in <code>Agent/models/task.py</code> <pre><code>class Task(BaseModel):\n    \"\"\"\n    The Task Model\n    This is the one we will pull and ask for the task from the API\n    \"\"\"\n\n    id: int = Field(description=\"The ID of the task\")\n    name: str = Field(description=\"A unique name to track the cluster of tasks\")\n    user_id: Optional[int] = Field(\n        None, description=\"The ID of the user who created the task\"\n    )\n    task_name: TaskName = Field(description=\"The name of the task\")\n    parameters: dict = Field(\n        default_factory=dict, description=\"The parameters for the task\"\n    )\n    result_status: ResultStatus = Field(\n        ResultStatus.pending, description=\"The status of the task\"\n    )\n    result_json: TaskResultJSON = Field(\n        default_factory=lambda: TaskResultJSON(result_profile={}, latency_profile={}),\n        description=\"The result of the task\",\n    )\n    description: Optional[str] = Field(\n        None, description=\"The description of the task result\"\n    )\n</code></pre>"},{"location":"Sources/Agent/models/track_type/","title":"TrackType","text":""},{"location":"Sources/Agent/modules/emotion_detection/features_extraction/","title":"FeaturesExtraction","text":""},{"location":"Sources/Agent/modules/emotion_detection/features_extraction/#Agent.modules.emotion_detection.features_extraction.FeaturesExtractor","title":"<code>FeaturesExtractor</code>","text":"Source code in <code>Agent/modules/emotion_detection/features_extraction.py</code> <pre><code>class FeaturesExtractor:\n    def __init__(self) -&gt; None:\n        self.padding_mode = \"zeros\"\n        self.padding_location = \"back\"\n\n    @staticmethod\n    def get_audio_embedding(audios: List[str]) -&gt; torch.Tensor:\n        \"\"\"Extracts and returns average audio features from a list of audio files.\"\"\"\n        features = []\n        for audio_path in audios:\n            y, sr = librosa.load(audio_path)\n            hop_length = 512\n            f0 = librosa.feature.zero_crossing_rate(y, hop_length=hop_length).T\n            mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, htk=True).T\n            cqt = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length).T\n            temp_feature = np.concatenate([f0, mfcc, cqt], axis=-1)\n            features.append(temp_feature)\n        feature = np.mean(np.concatenate(features), axis=0).reshape(1, -1)\n        # get them into tensor\n        feature = torch.tensor(feature).float()\n        return feature\n\n    def get_images_tensor(self, images: List[np.ndarray]) -&gt; torch.Tensor:\n        \"\"\"Extracts features from a list of images using a specified model.\"\"\"\n        model_name = \"OpenFace\"\n        image_features = [\n            self.represent(image, model_name=model_name)[0][\"embedding\"]\n            for image in images\n        ]\n        return torch.tensor(image_features)\n\n    def represent(\n        self,\n        img,\n        model_name: str = \"VGG-Face\",\n        enforce_detection: bool = False,\n        detector_backend: str = \"opencv\",\n        align: bool = True,\n        expand_percentage: int = 0,\n        normalization: str = \"base\",\n    ) -&gt; List[Dict[str, Any]]:\n        resp_objs = []\n\n        model: FacialRecognition = modeling.build_model(model_name)\n\n        # ---------------------------------\n        # we have run pre-process in verification. so, this can be skipped if it is coming from verifying.\n        target_size = model.input_shape\n        if detector_backend != \"skip\":\n            img_objs = self.extract_faces(\n                img,\n                target_size=(target_size[1], target_size[0]),\n                detector_backend=detector_backend,\n                grayscale=False,\n                enforce_detection=enforce_detection,\n                align=align,\n                expand_percentage=expand_percentage,\n            )\n        else:  # skip\n            # --------------------------------\n            if len(img.shape) == 4:\n                img = img[0]  # e.g. (1, 224, 224, 3) to (224, 224, 3)\n            if len(img.shape) == 3:\n                img = cv2.resize(img, target_size)\n                img = np.expand_dims(img, axis=0)\n                # When called from verifying, this is already normalized. But needed when user given.\n                if img.max() &gt; 1:\n                    img = (img.astype(np.float32) / 255.0).astype(np.float32)\n            # --------------------------------\n            # make dummy region and confidence to keep compatibility with `extract_faces`\n            img_objs = [\n                {\n                    \"face\": img,\n                    \"facial_area\": {\n                        \"x\": 0,\n                        \"y\": 0,\n                        \"w\": img.shape[1],\n                        \"h\": img.shape[2],\n                    },\n                    \"confidence\": 0,\n                }\n            ]\n        # ---------------------------------\n\n        for img_obj in img_objs:\n            img = img_obj[\"face\"]\n            region = img_obj[\"facial_area\"]\n            confidence = img_obj[\"confidence\"]\n            # custom normalization\n            img = preprocessing.normalize_input(img=img, normalization=normalization)\n\n            embedding = model.find_embeddings(img)\n\n            resp_obj = {\n                \"embedding\": embedding,\n                \"facial_area\": region,\n                \"face_confidence\": confidence,\n            }\n            resp_objs.append(resp_obj)\n\n        return resp_objs\n\n    logger = Logger(module=\"deepface/modules/detection.py\")\n\n    @staticmethod\n    def extract_faces(\n        img,\n        target_size: Optional[Tuple[int, int]] = (224, 224),\n        detector_backend: str = \"opencv\",\n        enforce_detection: bool = True,\n        align: bool = False,\n        expand_percentage: int = 0.2,\n        grayscale: bool = False,\n        human_readable=False,\n    ) -&gt; List[Dict[str, Any]]:\n        resp_objs = []\n        base_region = FacialAreaRegion(\n            x=0, y=0, w=img.shape[1], h=img.shape[0], confidence=0\n        )\n\n        if detector_backend == \"skip\":\n            face_objs = [DetectedFace(img=img, facial_area=base_region, confidence=0)]\n        else:\n            face_objs = DetectorWrapper.detect_faces(\n                detector_backend=detector_backend,\n                img=img,\n                align=align,\n                expand_percentage=expand_percentage,\n            )\n        # logger.info(f\"Detected {len(face_objs)} faces.\")\n        # in case of no face found\n        if len(face_objs) == 0 and enforce_detection is True:\n            raise ValueError(\n                \"Face could not be detected. Please confirm that the picture is a face photo \"\n                \"or consider to set enforce_detection param to False.\"\n            )\n\n        if len(face_objs) == 0 and enforce_detection is False:\n            face_objs = [DetectedFace(img=img, facial_area=base_region, confidence=0)]\n\n        for face_obj in face_objs:\n            current_img = face_obj.img\n            current_region = face_obj.facial_area\n\n            if current_img.shape[0] == 0 or current_img.shape[1] == 0:\n                continue\n\n            if grayscale is True:\n                current_img = cv2.cvtColor(current_img, cv2.COLOR_BGR2GRAY)\n\n            # resize and padding\n            if target_size is not None:\n                factor_0 = target_size[0] / current_img.shape[0]\n                factor_1 = target_size[1] / current_img.shape[1]\n                factor = min(factor_0, factor_1)\n\n                dsize = (\n                    int(current_img.shape[1] * factor),\n                    int(current_img.shape[0] * factor),\n                )\n                current_img = cv2.resize(current_img, dsize)\n\n                diff_0 = target_size[0] - current_img.shape[0]\n                diff_1 = target_size[1] - current_img.shape[1]\n                if grayscale is False:\n                    # Put the base image in the middle of the padded image\n                    current_img = np.pad(\n                        current_img,\n                        (\n                            (diff_0 // 2, diff_0 - diff_0 // 2),\n                            (diff_1 // 2, diff_1 - diff_1 // 2),\n                            (0, 0),\n                        ),\n                        \"constant\",\n                    )\n                else:\n                    current_img = np.pad(\n                        current_img,\n                        (\n                            (diff_0 // 2, diff_0 - diff_0 // 2),\n                            (diff_1 // 2, diff_1 - diff_1 // 2),\n                        ),\n                        \"constant\",\n                    )\n\n                # double check: if target image is not still the same size with target.\n                if current_img.shape[0:2] != target_size:\n                    current_img = cv2.resize(current_img, target_size)\n\n            # normalizing the image pixels\n            # what this line doing? must?\n            img_pixels = image.img_to_array(current_img)\n            img_pixels = np.expand_dims(img_pixels, axis=0)\n            img_pixels /= 255  # normalize input in [0, 1]\n            # discard expanded dimension\n            if human_readable is True and len(img_pixels.shape) == 4:\n                img_pixels = img_pixels[0]\n\n            resp_objs.append(\n                {\n                    \"face\": (\n                        img_pixels[:, :, ::-1] if human_readable is True else img_pixels\n                    ),\n                    \"facial_area\": {\n                        \"x\": int(current_region.x),\n                        \"y\": int(current_region.y),\n                        \"w\": int(current_region.w),\n                        \"h\": int(current_region.h),\n                        \"left_eye\": current_region.left_eye,\n                        \"right_eye\": current_region.right_eye,\n                    },\n                    \"confidence\": round(current_region.confidence, 2),\n                }\n            )\n\n        if len(resp_objs) == 0 and enforce_detection is True:\n            raise ValueError(\n                \"Exception while extracting faces from ....\"\n                \"Consider to set enforce_detection arg to False.\"\n            )\n\n        return resp_objs\n\n    @staticmethod\n    def align_face(\n        img: np.ndarray,\n        left_eye: Union[list, tuple],\n        right_eye: Union[list, tuple],\n    ) -&gt; Tuple[np.ndarray, float]:\n        # if eye could not be detected for the given image, return the image itself\n        if left_eye is None or right_eye is None:\n            return img, 0\n\n        # sometimes unexpectedly detected images come with nil dimensions\n        if img.shape[0] == 0 or img.shape[1] == 0:\n            return img, 0\n\n        angle = float(\n            np.degrees(\n                np.arctan2(right_eye[1] - left_eye[1], right_eye[0] - left_eye[0])\n            )\n        )\n        img = np.array(Image.fromarray(img).rotate(angle))\n        return img, angle\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/features_extraction/#Agent.modules.emotion_detection.features_extraction.FeaturesExtractor.get_audio_embedding","title":"<code>get_audio_embedding(audios)</code>  <code>staticmethod</code>","text":"<p>Extracts and returns average audio features from a list of audio files.</p> Source code in <code>Agent/modules/emotion_detection/features_extraction.py</code> <pre><code>@staticmethod\ndef get_audio_embedding(audios: List[str]) -&gt; torch.Tensor:\n    \"\"\"Extracts and returns average audio features from a list of audio files.\"\"\"\n    features = []\n    for audio_path in audios:\n        y, sr = librosa.load(audio_path)\n        hop_length = 512\n        f0 = librosa.feature.zero_crossing_rate(y, hop_length=hop_length).T\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, htk=True).T\n        cqt = librosa.feature.chroma_cqt(y=y, sr=sr, hop_length=hop_length).T\n        temp_feature = np.concatenate([f0, mfcc, cqt], axis=-1)\n        features.append(temp_feature)\n    feature = np.mean(np.concatenate(features), axis=0).reshape(1, -1)\n    # get them into tensor\n    feature = torch.tensor(feature).float()\n    return feature\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/features_extraction/#Agent.modules.emotion_detection.features_extraction.FeaturesExtractor.get_images_tensor","title":"<code>get_images_tensor(images)</code>","text":"<p>Extracts features from a list of images using a specified model.</p> Source code in <code>Agent/modules/emotion_detection/features_extraction.py</code> <pre><code>def get_images_tensor(self, images: List[np.ndarray]) -&gt; torch.Tensor:\n    \"\"\"Extracts features from a list of images using a specified model.\"\"\"\n    model_name = \"OpenFace\"\n    image_features = [\n        self.represent(image, model_name=model_name)[0][\"embedding\"]\n        for image in images\n    ]\n    return torch.tensor(image_features)\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/handler/","title":"Handler","text":""},{"location":"Sources/Agent/modules/emotion_detection/handler/#Agent.modules.emotion_detection.handler.EmotionDetectionHandler","title":"<code>EmotionDetectionHandler</code>","text":"Source code in <code>Agent/modules/emotion_detection/handler.py</code> <pre><code>class EmotionDetectionHandler:\n\n    def handle_task(self, task: Task) -&gt; Optional[Task]:\n        \"\"\"\n        Handle the task\n        Args:\n            task (Task): The task to handle\n        Returns:\n            The task with the result\n        \"\"\"\n        emotion_detection_parameters = EmotionDetectionParameters(**task.parameters)\n        text = emotion_detection_parameters.text\n        audio_file = emotion_detection_parameters.audio_file\n        images_path_list = emotion_detection_parameters.images_path_list\n\n        logger.info(f\"Text: {text}\")\n        logger.info(f\"Audio: {audio_file}\")\n        logger.info(f\"Images: {len(images_path_list)}\")\n        TimeLogger.log_task(task, \"start_trigger_emotion_model\")\n        result_profile, latency_profile = self.trigger_model(\n            text, [audio_file], images_path_list\n        )\n        TimeLogger.log_task(task, \"end_trigger_emotion_model\")\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        return task\n\n    @staticmethod\n    def trigger_model(\n        text: str, audio_paths: List[str], images_paths: List[str]\n    ) -&gt; Tuple[dict, dict]:\n        \"\"\"\n\n        Args:\n            text (str): The text to analyze for emotion\n            audio_paths (List[str]): The audio data to analyze for emotion\n            images_paths (List[str]): The images data to analyze for emotion\n\n        Returns:\n\n        \"\"\"\n        result_profile = {}\n        latency_profile = {}\n\n        if not text or not audio_paths or not images_paths:\n            logger.error(\"No text or audio or images provided\")\n            logger.error(\n                f\"text: {text is None}, audio: {audio_paths is None}, images: {images_paths is None}\"\n            )\n            return {}, {}\n        # audio is the file path\n        # same as the images, we need to read the images first\n        audio = []\n        for audio_path in audio_paths:\n            audio.append((CLIENT_DATA_FOLDER / audio_path).as_posix())\n\n        start_time = datetime.now()\n        # read the images\n        images = []\n        for images_path in images_paths:\n            folder = CLIENT_DATA_FOLDER / images_path\n            if not folder.exists():\n                continue\n            # Time Killer\n            for image_file in folder.iterdir():\n                image = cv2.imread(image_file.as_posix())\n                images.append(image)\n        latency_profile[\"io_images_read\"] = (\n            datetime.now() - start_time\n        ).total_seconds()\n\n        # 1. get the features with bert cn model\n        with time_tracker(\n            \"feature_extraction\", latency_profile, track_type=TrackType.MODEL.value\n        ):\n            features_extractor = FeaturesExtractor()\n            feature_video = (\n                features_extractor.get_images_tensor(images)\n                if images is not None\n                else None\n            )  # (n/5,709)\n            feature_audio = (\n                features_extractor.get_audio_embedding(audio)\n                if audio is not None\n                else None\n            )  # (94,33)\n\n        (\n            logger.info(f\"feature_video: {feature_video.shape}\")\n            if feature_video is not None\n            else logger.info(\"feature_video: there are no information about video\")\n        )\n        (\n            logger.info(f\"feature_audio: {feature_audio.shape}\")\n            if feature_audio is not None\n            else logger.info(\"feature_audio: there are no information about audio\")\n        )\n\n        # data is ready\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        model = SentimentAnalysis().to(device)\n        # load the model\n        with time_tracker(\n            \"load_model\", latency_profile, track_type=TrackType.MODEL.value\n        ):\n            model.load_state_dict(\n                {\n                    k.replace(\"Model.\", \"\"): v\n                    for k, v in torch.load(models_dir / \"sa_sims.pth\").items()\n                },\n                strict=True,\n            )\n\n            model.eval()\n\n        # run model\n        with time_tracker(\"infer\", latency_profile, track_type=TrackType.MODEL.value):\n            output = model(text, feature_audio, feature_video)\n\n        logger.critical(f\"output: {output}\")\n        # loop the output dict, get all of them into float\n        for k, v in output.items():\n            output[k] = float(v)\n            # and get it to decimal 2\n            output[k] = round(output[k], 2)\n        multi_modal_output = output.get(\"M\", 0)\n        result_profile[\"multi_modal_output\"] = output\n        logger.critical(f\"multi_modal_output: {multi_modal_output}\")\n        return result_profile, latency_profile\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/handler/#Agent.modules.emotion_detection.handler.EmotionDetectionHandler.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task (Task): The task to handle Returns:     The task with the result</p> Source code in <code>Agent/modules/emotion_detection/handler.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Optional[Task]:\n    \"\"\"\n    Handle the task\n    Args:\n        task (Task): The task to handle\n    Returns:\n        The task with the result\n    \"\"\"\n    emotion_detection_parameters = EmotionDetectionParameters(**task.parameters)\n    text = emotion_detection_parameters.text\n    audio_file = emotion_detection_parameters.audio_file\n    images_path_list = emotion_detection_parameters.images_path_list\n\n    logger.info(f\"Text: {text}\")\n    logger.info(f\"Audio: {audio_file}\")\n    logger.info(f\"Images: {len(images_path_list)}\")\n    TimeLogger.log_task(task, \"start_trigger_emotion_model\")\n    result_profile, latency_profile = self.trigger_model(\n        text, [audio_file], images_path_list\n    )\n    TimeLogger.log_task(task, \"end_trigger_emotion_model\")\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/handler/#Agent.modules.emotion_detection.handler.EmotionDetectionHandler.trigger_model","title":"<code>trigger_model(text, audio_paths, images_paths)</code>  <code>staticmethod</code>","text":"<p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to analyze for emotion</p> required <code>audio_paths</code> <code>List[str]</code> <p>The audio data to analyze for emotion</p> required <code>images_paths</code> <code>List[str]</code> <p>The images data to analyze for emotion</p> required <p>Returns:</p> Source code in <code>Agent/modules/emotion_detection/handler.py</code> <pre><code>@staticmethod\ndef trigger_model(\n    text: str, audio_paths: List[str], images_paths: List[str]\n) -&gt; Tuple[dict, dict]:\n    \"\"\"\n\n    Args:\n        text (str): The text to analyze for emotion\n        audio_paths (List[str]): The audio data to analyze for emotion\n        images_paths (List[str]): The images data to analyze for emotion\n\n    Returns:\n\n    \"\"\"\n    result_profile = {}\n    latency_profile = {}\n\n    if not text or not audio_paths or not images_paths:\n        logger.error(\"No text or audio or images provided\")\n        logger.error(\n            f\"text: {text is None}, audio: {audio_paths is None}, images: {images_paths is None}\"\n        )\n        return {}, {}\n    # audio is the file path\n    # same as the images, we need to read the images first\n    audio = []\n    for audio_path in audio_paths:\n        audio.append((CLIENT_DATA_FOLDER / audio_path).as_posix())\n\n    start_time = datetime.now()\n    # read the images\n    images = []\n    for images_path in images_paths:\n        folder = CLIENT_DATA_FOLDER / images_path\n        if not folder.exists():\n            continue\n        # Time Killer\n        for image_file in folder.iterdir():\n            image = cv2.imread(image_file.as_posix())\n            images.append(image)\n    latency_profile[\"io_images_read\"] = (\n        datetime.now() - start_time\n    ).total_seconds()\n\n    # 1. get the features with bert cn model\n    with time_tracker(\n        \"feature_extraction\", latency_profile, track_type=TrackType.MODEL.value\n    ):\n        features_extractor = FeaturesExtractor()\n        feature_video = (\n            features_extractor.get_images_tensor(images)\n            if images is not None\n            else None\n        )  # (n/5,709)\n        feature_audio = (\n            features_extractor.get_audio_embedding(audio)\n            if audio is not None\n            else None\n        )  # (94,33)\n\n    (\n        logger.info(f\"feature_video: {feature_video.shape}\")\n        if feature_video is not None\n        else logger.info(\"feature_video: there are no information about video\")\n    )\n    (\n        logger.info(f\"feature_audio: {feature_audio.shape}\")\n        if feature_audio is not None\n        else logger.info(\"feature_audio: there are no information about audio\")\n    )\n\n    # data is ready\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = SentimentAnalysis().to(device)\n    # load the model\n    with time_tracker(\n        \"load_model\", latency_profile, track_type=TrackType.MODEL.value\n    ):\n        model.load_state_dict(\n            {\n                k.replace(\"Model.\", \"\"): v\n                for k, v in torch.load(models_dir / \"sa_sims.pth\").items()\n            },\n            strict=True,\n        )\n\n        model.eval()\n\n    # run model\n    with time_tracker(\"infer\", latency_profile, track_type=TrackType.MODEL.value):\n        output = model(text, feature_audio, feature_video)\n\n    logger.critical(f\"output: {output}\")\n    # loop the output dict, get all of them into float\n    for k, v in output.items():\n        output[k] = float(v)\n        # and get it to decimal 2\n        output[k] = round(output[k], 2)\n    multi_modal_output = output.get(\"M\", 0)\n    result_profile[\"multi_modal_output\"] = output\n    logger.critical(f\"multi_modal_output: {multi_modal_output}\")\n    return result_profile, latency_profile\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/sentiment/","title":"Sentiment","text":""},{"location":"Sources/Agent/modules/emotion_detection/sentiment/#Agent.modules.emotion_detection.sentiment.BertTextEncoder","title":"<code>BertTextEncoder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>Agent/modules/emotion_detection/sentiment.py</code> <pre><code>class BertTextEncoder(nn.Module):\n    def __init__(self, language=\"en\", use_finetune=False):\n        \"\"\"\n        language: en / cn\n        \"\"\"\n        super(BertTextEncoder, self).__init__()\n\n        assert language in [\"en\", \"cn\"]\n\n        tokenizer_class = BertTokenizer\n        model_class = BertModel\n        # directory is fine\n        # pretrained_weights = '/home/sharing/disk3/pretrained_embedding/Chinese/bert/pytorch'\n        if language == \"en\":\n            self.tokenizer = tokenizer_class.from_pretrained(\n                f\"{models_dir}/bert_en\", do_lower_case=True\n            )\n            self.model = model_class.from_pretrained(f\"{models_dir}/bert_en\")\n        elif language == \"cn\":\n            self.tokenizer = tokenizer_class.from_pretrained(f\"{models_dir}/bert_cn\")\n            self.model = model_class.from_pretrained(f\"{models_dir}/bert_cn\")\n\n        self.use_finetune = use_finetune\n\n    def get_tokenizer(self):\n        return self.tokenizer\n\n    def from_text(self, text):\n        \"\"\"\n        text: raw data\n        \"\"\"\n        input_ids = self.get_id(text)\n        with torch.no_grad():\n            last_hidden_states = self.model(input_ids)[\n                0\n            ]  # Models outputs are now tuples\n        return last_hidden_states.squeeze()\n\n    def forward(self, text):\n        \"\"\"\n        text: (batch_size, 3, seq_len)\n        3: input_ids, input_mask, segment_ids\n        input_ids: input_ids,\n        input_mask: attention_mask,\n        segment_ids: token_type_ids\n        \"\"\"\n        text = self.tokenizer(text)\n        input_ids, input_mask, segment_ids = (\n            torch.tensor(text[\"input_ids\"]).long().unsqueeze(0),\n            torch.tensor(text[\"token_type_ids\"]).unsqueeze(0).float(),\n            torch.tensor(text[\"attention_mask\"]).unsqueeze(0).long(),\n        )\n        if self.use_finetune:\n            last_hidden_states = self.model(\n                input_ids=input_ids,\n                attention_mask=input_mask,\n                token_type_ids=segment_ids,\n            )[\n                0\n            ]  # Models outputs are now tuples\n        else:\n            with torch.no_grad():\n                last_hidden_states = self.model(\n                    input_ids=input_ids,\n                    attention_mask=input_mask,\n                    token_type_ids=segment_ids,\n                )[\n                    0\n                ]  # Models outputs are now tuples\n        return last_hidden_states\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/sentiment/#Agent.modules.emotion_detection.sentiment.BertTextEncoder.__init__","title":"<code>__init__(language='en', use_finetune=False)</code>","text":"<p>language: en / cn</p> Source code in <code>Agent/modules/emotion_detection/sentiment.py</code> <pre><code>def __init__(self, language=\"en\", use_finetune=False):\n    \"\"\"\n    language: en / cn\n    \"\"\"\n    super(BertTextEncoder, self).__init__()\n\n    assert language in [\"en\", \"cn\"]\n\n    tokenizer_class = BertTokenizer\n    model_class = BertModel\n    # directory is fine\n    # pretrained_weights = '/home/sharing/disk3/pretrained_embedding/Chinese/bert/pytorch'\n    if language == \"en\":\n        self.tokenizer = tokenizer_class.from_pretrained(\n            f\"{models_dir}/bert_en\", do_lower_case=True\n        )\n        self.model = model_class.from_pretrained(f\"{models_dir}/bert_en\")\n    elif language == \"cn\":\n        self.tokenizer = tokenizer_class.from_pretrained(f\"{models_dir}/bert_cn\")\n        self.model = model_class.from_pretrained(f\"{models_dir}/bert_cn\")\n\n    self.use_finetune = use_finetune\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/sentiment/#Agent.modules.emotion_detection.sentiment.BertTextEncoder.forward","title":"<code>forward(text)</code>","text":"<p>text: (batch_size, 3, seq_len) 3: input_ids, input_mask, segment_ids input_ids: input_ids, input_mask: attention_mask, segment_ids: token_type_ids</p> Source code in <code>Agent/modules/emotion_detection/sentiment.py</code> <pre><code>def forward(self, text):\n    \"\"\"\n    text: (batch_size, 3, seq_len)\n    3: input_ids, input_mask, segment_ids\n    input_ids: input_ids,\n    input_mask: attention_mask,\n    segment_ids: token_type_ids\n    \"\"\"\n    text = self.tokenizer(text)\n    input_ids, input_mask, segment_ids = (\n        torch.tensor(text[\"input_ids\"]).long().unsqueeze(0),\n        torch.tensor(text[\"token_type_ids\"]).unsqueeze(0).float(),\n        torch.tensor(text[\"attention_mask\"]).unsqueeze(0).long(),\n    )\n    if self.use_finetune:\n        last_hidden_states = self.model(\n            input_ids=input_ids,\n            attention_mask=input_mask,\n            token_type_ids=segment_ids,\n        )[\n            0\n        ]  # Models outputs are now tuples\n    else:\n        with torch.no_grad():\n            last_hidden_states = self.model(\n                input_ids=input_ids,\n                attention_mask=input_mask,\n                token_type_ids=segment_ids,\n            )[\n                0\n            ]  # Models outputs are now tuples\n    return last_hidden_states\n</code></pre>"},{"location":"Sources/Agent/modules/emotion_detection/sentiment/#Agent.modules.emotion_detection.sentiment.BertTextEncoder.from_text","title":"<code>from_text(text)</code>","text":"<p>text: raw data</p> Source code in <code>Agent/modules/emotion_detection/sentiment.py</code> <pre><code>def from_text(self, text):\n    \"\"\"\n    text: raw data\n    \"\"\"\n    input_ids = self.get_id(text)\n    with torch.no_grad():\n        last_hidden_states = self.model(input_ids)[\n            0\n        ]  # Models outputs are now tuples\n    return last_hidden_states.squeeze()\n</code></pre>"},{"location":"Sources/Agent/modules/general_ml/handler/","title":"Handler","text":""},{"location":"Sources/Agent/modules/general_ml/handler/#Agent.modules.general_ml.handler.GeneralMLModel","title":"<code>GeneralMLModel</code>","text":"Source code in <code>Agent/modules/general_ml/handler.py</code> <pre><code>class GeneralMLModel:\n    def __init__(self):\n        self.avail_models = {}\n\n    def handle_task(self, task: Task) -&gt; Task:\n        \"\"\"\n        Handle the task\n        Args:\n            task (Task): The task to handle\n\n        Returns:\n            Updated task\n        \"\"\"\n        TimeLogger.log_task(task, \"start_general_ml\")\n        result_profile = {}\n        latency_profile = {}\n        general_ml_parameters = GeneralMLParameters(**task.parameters)\n        text = general_ml_parameters.text\n        general_model_name = general_ml_parameters.general_model_name\n        params = general_ml_parameters.params\n        if general_model_name not in self.avail_models:\n            logger.error(f\"Model {general_model_name} not loaded yet\")\n            with time_tracker(\n                \"init\", latency_profile, track_type=TrackType.MODEL.value\n            ):\n                ml_model = self.load_model(general_model_name)\n                self.avail_models[general_model_name] = ml_model\n\n        else:\n            ml_model = self.avail_models[general_model_name]\n\n        with timer(logger, f\"Model infer {general_model_name}\"):\n            with time_tracker(\n                \"infer\", latency_profile, track_type=TrackType.MODEL.value\n            ):\n                res = self.infer(ml_model, general_model_name, text, params)\n        result_profile[\"result\"] = res\n\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        TimeLogger.log_task(task, \"end_general_ml\")\n        return task\n\n    @staticmethod\n    def load_model(general_model_name: str):\n        \"\"\"\n        Load model\n        Args:\n            general_model_name (str): Model name\n\n        Returns:\n\n        \"\"\"\n        if general_model_name == \"sentence_transformer\":\n            return SentenceTransformer(\"all-MiniLM-L6-v2\")\n        raise ValueError(f\"Model {general_model_name} is not implemented\")\n\n    @staticmethod\n    def infer(ml_model, general_model_name: str, text: str, params: dict):\n        \"\"\"\n        Infer the model\n        Args:\n            ml_model: General model\n            general_model_name (str): Model name\n            text (str): Text\n            params (dict): Model params\n\n        Returns:\n\n        \"\"\"\n        if general_model_name == \"sentence_transformer\":\n            result = ml_model.encode(text)\n            return result.tolist()\n        logger.info(params)\n        raise ValueError(f\"Model {general_model_name} is not implemented\")\n</code></pre>"},{"location":"Sources/Agent/modules/general_ml/handler/#Agent.modules.general_ml.handler.GeneralMLModel.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task (Task): The task to handle</p> <p>Returns:</p> Type Description <code>Task</code> <p>Updated task</p> Source code in <code>Agent/modules/general_ml/handler.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Task:\n    \"\"\"\n    Handle the task\n    Args:\n        task (Task): The task to handle\n\n    Returns:\n        Updated task\n    \"\"\"\n    TimeLogger.log_task(task, \"start_general_ml\")\n    result_profile = {}\n    latency_profile = {}\n    general_ml_parameters = GeneralMLParameters(**task.parameters)\n    text = general_ml_parameters.text\n    general_model_name = general_ml_parameters.general_model_name\n    params = general_ml_parameters.params\n    if general_model_name not in self.avail_models:\n        logger.error(f\"Model {general_model_name} not loaded yet\")\n        with time_tracker(\n            \"init\", latency_profile, track_type=TrackType.MODEL.value\n        ):\n            ml_model = self.load_model(general_model_name)\n            self.avail_models[general_model_name] = ml_model\n\n    else:\n        ml_model = self.avail_models[general_model_name]\n\n    with timer(logger, f\"Model infer {general_model_name}\"):\n        with time_tracker(\n            \"infer\", latency_profile, track_type=TrackType.MODEL.value\n        ):\n            res = self.infer(ml_model, general_model_name, text, params)\n    result_profile[\"result\"] = res\n\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    TimeLogger.log_task(task, \"end_general_ml\")\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/general_ml/handler/#Agent.modules.general_ml.handler.GeneralMLModel.infer","title":"<code>infer(ml_model, general_model_name, text, params)</code>  <code>staticmethod</code>","text":"<p>Infer the model Args:     ml_model: General model     general_model_name (str): Model name     text (str): Text     params (dict): Model params</p> <p>Returns:</p> Source code in <code>Agent/modules/general_ml/handler.py</code> <pre><code>@staticmethod\ndef infer(ml_model, general_model_name: str, text: str, params: dict):\n    \"\"\"\n    Infer the model\n    Args:\n        ml_model: General model\n        general_model_name (str): Model name\n        text (str): Text\n        params (dict): Model params\n\n    Returns:\n\n    \"\"\"\n    if general_model_name == \"sentence_transformer\":\n        result = ml_model.encode(text)\n        return result.tolist()\n    logger.info(params)\n    raise ValueError(f\"Model {general_model_name} is not implemented\")\n</code></pre>"},{"location":"Sources/Agent/modules/general_ml/handler/#Agent.modules.general_ml.handler.GeneralMLModel.load_model","title":"<code>load_model(general_model_name)</code>  <code>staticmethod</code>","text":"<p>Load model Args:     general_model_name (str): Model name</p> <p>Returns:</p> Source code in <code>Agent/modules/general_ml/handler.py</code> <pre><code>@staticmethod\ndef load_model(general_model_name: str):\n    \"\"\"\n    Load model\n    Args:\n        general_model_name (str): Model name\n\n    Returns:\n\n    \"\"\"\n    if general_model_name == \"sentence_transformer\":\n        return SentenceTransformer(\"all-MiniLM-L6-v2\")\n    raise ValueError(f\"Model {general_model_name} is not implemented\")\n</code></pre>"},{"location":"Sources/Agent/modules/general_ml/ml_models/","title":"MLModels","text":""},{"location":"Sources/Agent/modules/hf_llm/handler/","title":"Handler","text":""},{"location":"Sources/Agent/modules/hf_llm/handler/#Agent.modules.hf_llm.handler.HFLLM","title":"<code>HFLLM</code>","text":"Source code in <code>Agent/modules/hf_llm/handler.py</code> <pre><code>class HFLLM:\n    def __init__(self):\n        self.avail_models = {}\n        self.avail_tokenizers = {}\n\n    def handle_task(self, task: Task) -&gt; Task:\n        \"\"\"\n        Handle the task\n        Args:\n            task (Task): The task to handle\n\n        Returns:\n            Updated task\n        \"\"\"\n        TimeLogger.log_task(task, \"start_hf_llm\")\n        result_profile = {}\n        latency_profile = {}\n        hf_parameters = HFParameters(**task.parameters)\n        hf_model_name = hf_parameters.hf_model_name\n        text = hf_parameters.text\n        hf_model = self.avail_models.get(hf_model_name, None)\n        if hf_model is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            logger.error(f\"Model {hf_model_name} not loaded yet\")\n            with time_tracker(\n                \"init_model\", latency_profile, track_type=TrackType.TRANSFER.value\n            ):\n                hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n                hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name)\n                hf_model.to(device)\n                self.avail_models[hf_model_name] = hf_model\n                self.avail_tokenizers[hf_model_name] = hf_tokenizer\n\n        with timer(logger, f\"Model infer {hf_model_name}\"):\n            with time_tracker(\n                \"infer\", latency_profile, track_type=TrackType.MODEL.value\n            ):\n                inputs = self.avail_tokenizers[hf_model_name](\n                    text,\n                    return_tensors=\"pt\",\n                    max_length=1024,\n                    truncation=True,\n                )\n                # to device\n                inputs = {k: v.to(hf_model.device) for k, v in inputs.items()}\n                num_of_tokens = len(inputs[\"input_ids\"][0])\n                res = hf_model.generate(**inputs, max_new_tokens=num_of_tokens + 100)\n                generated_text = self.avail_tokenizers[hf_model_name].decode(\n                    res[0].cpu().tolist(), skip_special_tokens=True\n                )\n        result_profile[\"text\"] = generated_text\n        result_profile[\"logs\"] = res[0].tolist()\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        TimeLogger.log_task(task, \"end_hf_llm\")\n        return task\n</code></pre>"},{"location":"Sources/Agent/modules/hf_llm/handler/#Agent.modules.hf_llm.handler.HFLLM.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task (Task): The task to handle</p> <p>Returns:</p> Type Description <code>Task</code> <p>Updated task</p> Source code in <code>Agent/modules/hf_llm/handler.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Task:\n    \"\"\"\n    Handle the task\n    Args:\n        task (Task): The task to handle\n\n    Returns:\n        Updated task\n    \"\"\"\n    TimeLogger.log_task(task, \"start_hf_llm\")\n    result_profile = {}\n    latency_profile = {}\n    hf_parameters = HFParameters(**task.parameters)\n    hf_model_name = hf_parameters.hf_model_name\n    text = hf_parameters.text\n    hf_model = self.avail_models.get(hf_model_name, None)\n    if hf_model is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        logger.error(f\"Model {hf_model_name} not loaded yet\")\n        with time_tracker(\n            \"init_model\", latency_profile, track_type=TrackType.TRANSFER.value\n        ):\n            hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name)\n            hf_model = AutoModelForCausalLM.from_pretrained(hf_model_name)\n            hf_model.to(device)\n            self.avail_models[hf_model_name] = hf_model\n            self.avail_tokenizers[hf_model_name] = hf_tokenizer\n\n    with timer(logger, f\"Model infer {hf_model_name}\"):\n        with time_tracker(\n            \"infer\", latency_profile, track_type=TrackType.MODEL.value\n        ):\n            inputs = self.avail_tokenizers[hf_model_name](\n                text,\n                return_tensors=\"pt\",\n                max_length=1024,\n                truncation=True,\n            )\n            # to device\n            inputs = {k: v.to(hf_model.device) for k, v in inputs.items()}\n            num_of_tokens = len(inputs[\"input_ids\"][0])\n            res = hf_model.generate(**inputs, max_new_tokens=num_of_tokens + 100)\n            generated_text = self.avail_tokenizers[hf_model_name].decode(\n                res[0].cpu().tolist(), skip_special_tokens=True\n            )\n    result_profile[\"text\"] = generated_text\n    result_profile[\"logs\"] = res[0].tolist()\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    TimeLogger.log_task(task, \"end_hf_llm\")\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/","title":"Handler","text":""},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler","title":"<code>OpenAIHandler</code>","text":"Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>class OpenAIHandler:\n    def __init__(self):\n        self.client = OpenAI()\n\n    def handle_task(self, task: Task) -&gt; Task:\n        \"\"\"\n        Handle the task\n        Args:\n            task (Task): The task\n\n        Returns:\n            The task with the result\n        \"\"\"\n        result_profile = {}\n        latency_profile = {}\n        TimeLogger.log_task(task, \"start_openai\")\n        if \"speech2text\" in task.task_name:\n            TimeLogger.log(latency_profile, \"start_openai_speech2text\")\n            text = self.speech2text(task)\n            TimeLogger.log(latency_profile, \"end_openai_speech2text\")\n            result_profile[\"text\"] = text\n        if \"openai_gpt_4o_text_and_image\" in task.task_name:\n            TimeLogger.log(latency_profile, \"start_openai_gpt_4o\")\n            text = self.gpt_4o_text_and_images(task)\n            TimeLogger.log(latency_profile, \"end_openai_gpt_4o\")\n            result_profile[\"text\"] = text\n        if \"openai_gpt_4o_text_only\" in task.task_name:\n            TimeLogger.log(latency_profile, \"start_openai_gpt_4o\")\n            text = self.gpt_4o_text_only(task)\n            TimeLogger.log(latency_profile, \"end_openai_gpt_4o\")\n            result_profile[\"text\"] = text\n        if \"openai_gpt_35\" in task.task_name:\n            TimeLogger.log(latency_profile, \"start_openai_gpt_35\")\n            text = self.gpt_35(task)\n            TimeLogger.log(latency_profile, \"end_openai_gpt_35\")\n            result_profile[\"text\"] = text\n        if \"text2speech\" in task.task_name:\n            TimeLogger.log(latency_profile, \"start_openai_text2speech\")\n            audio_file_path = self.text2speech(task)\n            TimeLogger.log(latency_profile, \"end_openai_text2speech\")\n            result_profile[\"audio_file_path\"] = audio_file_path.split(\"/\")[-1]\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        TimeLogger.log_task(task, \"end_openai\")\n        return task\n\n    def speech2text(self, task: Task) -&gt; Optional[str]:\n        \"\"\"\n        Call OpenAI endpoints to convert speech to text\n        Args:\n            task (Task): The path to the audio file\n\n        Returns:\n            str: The transcribed text\n        \"\"\"\n\n        try:\n            logger.info(task.parameters)\n            params = Speech2TextParameters(**task.parameters)\n            with time_tracker(\n                    \"locate_audio_file\",\n                    task.result_json.latency_profile,\n                    track_type=TrackType.TRANSFER.value,\n            ):\n                audio_file_path = Speech2Text.locate_audio_file(\n                    params.uid, params.audio_index, params.end_time\n                )\n\n            logger.info(f\"Transcribing audio file: {audio_file_path}\")\n\n            audio_file_path = Path(audio_file_path)\n            if not audio_file_path.exists():\n                logger.error(f\"Audio file {audio_file_path} not found\")\n                return None\n            with time_tracker(\n                    \"openai_stt\",\n                    task.result_json.latency_profile,\n                    track_type=TrackType.MODEL.value,\n            ):\n                with open(audio_file_path, \"rb\") as audio_file:\n                    res = self.client.audio.transcriptions.create(\n                        model=\"whisper-1\", file=audio_file\n                    )\n\n            text = res.text\n            logger.info(f\"Transcription result: {text}\")\n            return text\n        except Exception as e:\n            logger.error(f\"Error transcribing audio file: {e}\")\n        return None\n\n    def gpt_4o_text_only(self, task: Task) -&gt; str:\n        \"\"\"\n        Get the text only\n        Args:\n            task:\n\n        Returns:\n\n        \"\"\"\n        params = OpenAIGPT4OTextOnlyParameters(**task.parameters)\n        text = params.text\n        prompt_template = params.prompt_template\n        logger.info(f\"Text: {text}\")\n        prompt = prompt_template.format(text=text)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n        ]\n\n        with time_tracker(\n                \"gpt-4o-call\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n        ):\n            res = self.client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages,\n            )\n        return res.choices[0].message.content\n\n    def gpt_35(self, task: Task) -&gt; Optional[str]:\n        \"\"\"\n        Call OpenAI endpoints to convert speech to text\n        Args:\n            task (Task): The path to the audio file\n\n        Returns:\n            str: The transcribed text\n        \"\"\"\n\n        try:\n            logger.info(task.parameters)\n            params = OpenAIGPT4OTextOnlyParameters(**task.parameters)\n            text = params.text\n            prompt_template = params.prompt_template\n            logger.info(f\"Text: {text}\")\n            prompt = prompt_template.format(text=text)\n            messages = [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": prompt},\n                    ],\n                }\n            ]\n            with time_tracker(\n                    \"openai_gpt_35\",\n                    task.result_json.latency_profile,\n                    track_type=TrackType.MODEL.value,\n            ):\n                res = self.client.chat.completions.create(\n                    model=\"gpt-3.5-turbo\",\n                    messages=messages,\n                )\n\n            return res.choices[0].message.content\n        except Exception as e:\n            logger.error(f\"Error locating audio file: {e}\")\n            return None\n\n    def gpt_4o_text_and_images(self, task: Task) -&gt; Optional[str]:\n        \"\"\"\n        Get the text and images\n        And then call the GPT-4o endpoints\n\n        # we need to sample the images as it will be a lot of them\n\n        Args:\n            task (Task): The task\n\n        Returns:\n\n        \"\"\"\n        params = OpenAIGPT4OParameters(**task.parameters)\n        text = params.text\n        images_path_list = params.images_path_list\n        sample_ratio = params.sample_ratio\n        prompt_template = params.prompt_template\n        logger.info(f\"Text: {text}\")\n\n        # sample the images\n        # so, we will only get the images for every sample_ratio images\n        logger.info(f\"Current length of images: {len(images_path_list)}\")\n        logger.debug(images_path_list)\n        images_path_list = images_path_list[::sample_ratio]\n        logger.info(f\"Sampled length of images: {len(images_path_list)}\")\n\n        # read image data to the one gpt-4o can take, something like data:image/jpeg;base64\n        with time_tracker(\n                label=\"encode_images\",\n                profile=task.result_json.latency_profile,\n                track_type=TrackType.TRANSFER.value,\n        ):\n            images = []\n            for images_path in images_path_list:\n                folder = CLIENT_DATA_FOLDER / images_path\n                if not folder.exists():\n                    continue\n                for image_file in folder.iterdir():\n                    images.append(self.encode_image(image_file))\n        \"\"\"\n        messages = [\n            {\n              \"role\": \"user\",\n              \"content\": [\n                {\n                  \"type\": \"text\",\n                  \"text\": \"What\u2019s in this image?\"\n                },\n                {\n                  \"type\": \"image_url\",\n                  \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                  }\n                }\n              ]\n            }\n          ]\n        \"\"\"\n\n        prompt = prompt_template.format(text=text)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n        ]\n        for image in images:\n            if not image:\n                continue\n            messages[0][\"content\"].append(\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n                }\n            )\n\n        logger.debug(messages)\n        # call gpt-4o\n        with time_tracker(\n                \"gpt-4o-call\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n        ):\n            res = self.client.chat.completions.create(\n                model=\"gpt-4o\",\n                messages=messages,\n            )\n        return res.choices[0].message.content\n\n    @staticmethod\n    def encode_image(image_path):\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\n    def text2speech(self, task: Task) -&gt; Optional[str]:\n        \"\"\"\n        Call OpenAI endpoints to convert text to speech\n        Args:\n            task (Task): The text to convert\n\n        Returns:\n\n        \"\"\"\n        params = Text2SpeechParameters(**task.parameters)\n        text = params.text\n        logger.info(f\"Text: {text}\")\n        output_audio_file_path = DATA_DIR / \"tts\" / f\"{task.id}.mp3\"\n        # if folder does not exist, create it\n        output_audio_file_path.parent.mkdir(parents=True, exist_ok=True)\n        output_audio_file_path = output_audio_file_path.as_posix()\n\n        with time_tracker(\n                \"openai_tts\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n        ):\n            res = self.client.audio.speech.create(\n                model=\"tts-1\",\n                voice=\"alloy\",\n                input=text,\n            )\n        with time_tracker(\n                \"save_audio\",\n                task.result_json.latency_profile,\n                track_type=TrackType.TRANSFER.value,\n        ):\n            res.stream_to_file(output_audio_file_path)\n        return output_audio_file_path\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.gpt_35","title":"<code>gpt_35(task)</code>","text":"<p>Call OpenAI endpoints to convert speech to text Args:     task (Task): The path to the audio file</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The transcribed text</p> Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>def gpt_35(self, task: Task) -&gt; Optional[str]:\n    \"\"\"\n    Call OpenAI endpoints to convert speech to text\n    Args:\n        task (Task): The path to the audio file\n\n    Returns:\n        str: The transcribed text\n    \"\"\"\n\n    try:\n        logger.info(task.parameters)\n        params = OpenAIGPT4OTextOnlyParameters(**task.parameters)\n        text = params.text\n        prompt_template = params.prompt_template\n        logger.info(f\"Text: {text}\")\n        prompt = prompt_template.format(text=text)\n        messages = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                ],\n            }\n        ]\n        with time_tracker(\n                \"openai_gpt_35\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n        ):\n            res = self.client.chat.completions.create(\n                model=\"gpt-3.5-turbo\",\n                messages=messages,\n            )\n\n        return res.choices[0].message.content\n    except Exception as e:\n        logger.error(f\"Error locating audio file: {e}\")\n        return None\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.gpt_4o_text_and_images","title":"<code>gpt_4o_text_and_images(task)</code>","text":"<p>Get the text and images And then call the GPT-4o endpoints</p>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.gpt_4o_text_and_images--we-need-to-sample-the-images-as-it-will-be-a-lot-of-them","title":"we need to sample the images as it will be a lot of them","text":"<p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task</p> required <p>Returns:</p> Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>def gpt_4o_text_and_images(self, task: Task) -&gt; Optional[str]:\n    \"\"\"\n    Get the text and images\n    And then call the GPT-4o endpoints\n\n    # we need to sample the images as it will be a lot of them\n\n    Args:\n        task (Task): The task\n\n    Returns:\n\n    \"\"\"\n    params = OpenAIGPT4OParameters(**task.parameters)\n    text = params.text\n    images_path_list = params.images_path_list\n    sample_ratio = params.sample_ratio\n    prompt_template = params.prompt_template\n    logger.info(f\"Text: {text}\")\n\n    # sample the images\n    # so, we will only get the images for every sample_ratio images\n    logger.info(f\"Current length of images: {len(images_path_list)}\")\n    logger.debug(images_path_list)\n    images_path_list = images_path_list[::sample_ratio]\n    logger.info(f\"Sampled length of images: {len(images_path_list)}\")\n\n    # read image data to the one gpt-4o can take, something like data:image/jpeg;base64\n    with time_tracker(\n            label=\"encode_images\",\n            profile=task.result_json.latency_profile,\n            track_type=TrackType.TRANSFER.value,\n    ):\n        images = []\n        for images_path in images_path_list:\n            folder = CLIENT_DATA_FOLDER / images_path\n            if not folder.exists():\n                continue\n            for image_file in folder.iterdir():\n                images.append(self.encode_image(image_file))\n    \"\"\"\n    messages = [\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\n              \"type\": \"text\",\n              \"text\": \"What\u2019s in this image?\"\n            },\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n              }\n            }\n          ]\n        }\n      ]\n    \"\"\"\n\n    prompt = prompt_template.format(text=text)\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        }\n    ]\n    for image in images:\n        if not image:\n            continue\n        messages[0][\"content\"].append(\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n        )\n\n    logger.debug(messages)\n    # call gpt-4o\n    with time_tracker(\n            \"gpt-4o-call\",\n            task.result_json.latency_profile,\n            track_type=TrackType.MODEL.value,\n    ):\n        res = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n        )\n    return res.choices[0].message.content\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.gpt_4o_text_only","title":"<code>gpt_4o_text_only(task)</code>","text":"<p>Get the text only Args:     task:</p> <p>Returns:</p> Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>def gpt_4o_text_only(self, task: Task) -&gt; str:\n    \"\"\"\n    Get the text only\n    Args:\n        task:\n\n    Returns:\n\n    \"\"\"\n    params = OpenAIGPT4OTextOnlyParameters(**task.parameters)\n    text = params.text\n    prompt_template = params.prompt_template\n    logger.info(f\"Text: {text}\")\n    prompt = prompt_template.format(text=text)\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        }\n    ]\n\n    with time_tracker(\n            \"gpt-4o-call\",\n            task.result_json.latency_profile,\n            track_type=TrackType.MODEL.value,\n    ):\n        res = self.client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=messages,\n        )\n    return res.choices[0].message.content\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task (Task): The task</p> <p>Returns:</p> Type Description <code>Task</code> <p>The task with the result</p> Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Task:\n    \"\"\"\n    Handle the task\n    Args:\n        task (Task): The task\n\n    Returns:\n        The task with the result\n    \"\"\"\n    result_profile = {}\n    latency_profile = {}\n    TimeLogger.log_task(task, \"start_openai\")\n    if \"speech2text\" in task.task_name:\n        TimeLogger.log(latency_profile, \"start_openai_speech2text\")\n        text = self.speech2text(task)\n        TimeLogger.log(latency_profile, \"end_openai_speech2text\")\n        result_profile[\"text\"] = text\n    if \"openai_gpt_4o_text_and_image\" in task.task_name:\n        TimeLogger.log(latency_profile, \"start_openai_gpt_4o\")\n        text = self.gpt_4o_text_and_images(task)\n        TimeLogger.log(latency_profile, \"end_openai_gpt_4o\")\n        result_profile[\"text\"] = text\n    if \"openai_gpt_4o_text_only\" in task.task_name:\n        TimeLogger.log(latency_profile, \"start_openai_gpt_4o\")\n        text = self.gpt_4o_text_only(task)\n        TimeLogger.log(latency_profile, \"end_openai_gpt_4o\")\n        result_profile[\"text\"] = text\n    if \"openai_gpt_35\" in task.task_name:\n        TimeLogger.log(latency_profile, \"start_openai_gpt_35\")\n        text = self.gpt_35(task)\n        TimeLogger.log(latency_profile, \"end_openai_gpt_35\")\n        result_profile[\"text\"] = text\n    if \"text2speech\" in task.task_name:\n        TimeLogger.log(latency_profile, \"start_openai_text2speech\")\n        audio_file_path = self.text2speech(task)\n        TimeLogger.log(latency_profile, \"end_openai_text2speech\")\n        result_profile[\"audio_file_path\"] = audio_file_path.split(\"/\")[-1]\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    TimeLogger.log_task(task, \"end_openai\")\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.speech2text","title":"<code>speech2text(task)</code>","text":"<p>Call OpenAI endpoints to convert speech to text Args:     task (Task): The path to the audio file</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The transcribed text</p> Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>def speech2text(self, task: Task) -&gt; Optional[str]:\n    \"\"\"\n    Call OpenAI endpoints to convert speech to text\n    Args:\n        task (Task): The path to the audio file\n\n    Returns:\n        str: The transcribed text\n    \"\"\"\n\n    try:\n        logger.info(task.parameters)\n        params = Speech2TextParameters(**task.parameters)\n        with time_tracker(\n                \"locate_audio_file\",\n                task.result_json.latency_profile,\n                track_type=TrackType.TRANSFER.value,\n        ):\n            audio_file_path = Speech2Text.locate_audio_file(\n                params.uid, params.audio_index, params.end_time\n            )\n\n        logger.info(f\"Transcribing audio file: {audio_file_path}\")\n\n        audio_file_path = Path(audio_file_path)\n        if not audio_file_path.exists():\n            logger.error(f\"Audio file {audio_file_path} not found\")\n            return None\n        with time_tracker(\n                \"openai_stt\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n        ):\n            with open(audio_file_path, \"rb\") as audio_file:\n                res = self.client.audio.transcriptions.create(\n                    model=\"whisper-1\", file=audio_file\n                )\n\n        text = res.text\n        logger.info(f\"Transcription result: {text}\")\n        return text\n    except Exception as e:\n        logger.error(f\"Error transcribing audio file: {e}\")\n    return None\n</code></pre>"},{"location":"Sources/Agent/modules/openai/handler/#Agent.modules.openai.handler.OpenAIHandler.text2speech","title":"<code>text2speech(task)</code>","text":"<p>Call OpenAI endpoints to convert text to speech Args:     task (Task): The text to convert</p> <p>Returns:</p> Source code in <code>Agent/modules/openai/handler.py</code> <pre><code>def text2speech(self, task: Task) -&gt; Optional[str]:\n    \"\"\"\n    Call OpenAI endpoints to convert text to speech\n    Args:\n        task (Task): The text to convert\n\n    Returns:\n\n    \"\"\"\n    params = Text2SpeechParameters(**task.parameters)\n    text = params.text\n    logger.info(f\"Text: {text}\")\n    output_audio_file_path = DATA_DIR / \"tts\" / f\"{task.id}.mp3\"\n    # if folder does not exist, create it\n    output_audio_file_path.parent.mkdir(parents=True, exist_ok=True)\n    output_audio_file_path = output_audio_file_path.as_posix()\n\n    with time_tracker(\n            \"openai_tts\",\n            task.result_json.latency_profile,\n            track_type=TrackType.MODEL.value,\n    ):\n        res = self.client.audio.speech.create(\n            model=\"tts-1\",\n            voice=\"alloy\",\n            input=text,\n        )\n    with time_tracker(\n            \"save_audio\",\n            task.result_json.latency_profile,\n            track_type=TrackType.TRANSFER.value,\n    ):\n        res.stream_to_file(output_audio_file_path)\n    return output_audio_file_path\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/adaptor_worker/","title":"AdaptorWorker","text":""},{"location":"Sources/Agent/modules/quantization_llm/adaptor_worker/#Agent.modules.quantization_llm.adaptor_worker.QuantizationLLMAdaptor","title":"<code>QuantizationLLMAdaptor</code>","text":"<p>This is the adaptor for the Quantization LLM model</p> Source code in <code>Agent/modules/quantization_llm/adaptor_worker.py</code> <pre><code>class QuantizationLLMAdaptor:\n    \"\"\"\n    This is the adaptor for the Quantization LLM model\n    \"\"\"\n\n    def __init__(self, model_config: QuantizationLLMModelConfig):\n        self.model_config = model_config\n        self.model_path = model_config.model_path()\n        self.llm = self.model_config.llm\n\n    def create_completion(self, prompt: str) -&gt; str:\n        \"\"\"\n        Create completion for the given prompt\n        Args:\n            prompt (str): The prompt to generate completion for the model\n\n        Returns:\n            str: The completion generated by the model\n\n        \"\"\"\n\n        output = self.llm(\n            f\"Q: {prompt} A: \",\n            max_tokens=500,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n            stop=[\n                \"Q:\",\n                \"\\n\",\n            ],  # Stop generating just before the model would generate a new question\n            echo=True,  # Echo the prompt back in the output\n        )\n        logger.info(f\"Response: {output}\")\n        return output\n\n    def create_chat_completion(\n        self,\n        prompt: str = None,\n        messages: List[Dict[str, str]] = None,\n        tools: List[ChatCompletionTool] = None,\n        tool_choice: ChatCompletionToolChoiceOption = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Create chat completion for the given prompt and messages\n        Args:\n            prompt (str): The prompt to generate completion for the model\n            messages (List[Dict[str, str]]): The messages to generate completion for the model\n            tools (List[ChatCompletionTool]): The tools to use for chat completion\n            tool_choice (ChatCompletionToolChoiceOption): The tool choice to use for chat completion\n            *args:\n            **kwargs:\n\n        Returns:\n\n        \"\"\"\n        if messages is not None:\n            \"\"\"\n            This is trying to replicate passing all params chat completion provided via llama_cpp\n            \"\"\"\n\n            logger.info(f\"Creating chat completion for messages: {messages}\")\n            return self.llm.create_chat_completion(\n                messages=messages, tools=tools, tool_choice=tool_choice\n            )\n\n        if prompt:\n            \"\"\"\n            Simple version of it, without message \"role\" definition\n            \"\"\"\n\n            res = self.llm.create_chat_completion(\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt},\n                ]\n            )\n            return res\n\n        raise ValueError(\"Prompt or messages are required\")\n\n    def create_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"\n        Create embedding for the given text\n        Args:\n            text (str): The text to generate embedding for\n\n        Returns:\n            List[float]: The embedding generated by the model\n\n        \"\"\"\n        if text is None:\n            raise ValueError(\"Text is required\")\n\n        logger.info(f\"Creating embedding for text: {text}\")\n        return self.llm.create_embedding(text)\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/adaptor_worker/#Agent.modules.quantization_llm.adaptor_worker.QuantizationLLMAdaptor.create_chat_completion","title":"<code>create_chat_completion(prompt=None, messages=None, tools=None, tool_choice=None, *args, **kwargs)</code>","text":"<p>Create chat completion for the given prompt and messages Args:     prompt (str): The prompt to generate completion for the model     messages (List[Dict[str, str]]): The messages to generate completion for the model     tools (List[ChatCompletionTool]): The tools to use for chat completion     tool_choice (ChatCompletionToolChoiceOption): The tool choice to use for chat completion     args:     *kwargs:</p> <p>Returns:</p> Source code in <code>Agent/modules/quantization_llm/adaptor_worker.py</code> <pre><code>def create_chat_completion(\n    self,\n    prompt: str = None,\n    messages: List[Dict[str, str]] = None,\n    tools: List[ChatCompletionTool] = None,\n    tool_choice: ChatCompletionToolChoiceOption = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Create chat completion for the given prompt and messages\n    Args:\n        prompt (str): The prompt to generate completion for the model\n        messages (List[Dict[str, str]]): The messages to generate completion for the model\n        tools (List[ChatCompletionTool]): The tools to use for chat completion\n        tool_choice (ChatCompletionToolChoiceOption): The tool choice to use for chat completion\n        *args:\n        **kwargs:\n\n    Returns:\n\n    \"\"\"\n    if messages is not None:\n        \"\"\"\n        This is trying to replicate passing all params chat completion provided via llama_cpp\n        \"\"\"\n\n        logger.info(f\"Creating chat completion for messages: {messages}\")\n        return self.llm.create_chat_completion(\n            messages=messages, tools=tools, tool_choice=tool_choice\n        )\n\n    if prompt:\n        \"\"\"\n        Simple version of it, without message \"role\" definition\n        \"\"\"\n\n        res = self.llm.create_chat_completion(\n            messages=[\n                {\"role\": \"user\", \"content\": prompt},\n            ]\n        )\n        return res\n\n    raise ValueError(\"Prompt or messages are required\")\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/adaptor_worker/#Agent.modules.quantization_llm.adaptor_worker.QuantizationLLMAdaptor.create_completion","title":"<code>create_completion(prompt)</code>","text":"<p>Create completion for the given prompt Args:     prompt (str): The prompt to generate completion for the model</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The completion generated by the model</p> Source code in <code>Agent/modules/quantization_llm/adaptor_worker.py</code> <pre><code>def create_completion(self, prompt: str) -&gt; str:\n    \"\"\"\n    Create completion for the given prompt\n    Args:\n        prompt (str): The prompt to generate completion for the model\n\n    Returns:\n        str: The completion generated by the model\n\n    \"\"\"\n\n    output = self.llm(\n        f\"Q: {prompt} A: \",\n        max_tokens=500,  # Generate up to 32 tokens, set to None to generate up to the end of the context window\n        stop=[\n            \"Q:\",\n            \"\\n\",\n        ],  # Stop generating just before the model would generate a new question\n        echo=True,  # Echo the prompt back in the output\n    )\n    logger.info(f\"Response: {output}\")\n    return output\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/adaptor_worker/#Agent.modules.quantization_llm.adaptor_worker.QuantizationLLMAdaptor.create_embedding","title":"<code>create_embedding(text)</code>","text":"<p>Create embedding for the given text Args:     text (str): The text to generate embedding for</p> <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: The embedding generated by the model</p> Source code in <code>Agent/modules/quantization_llm/adaptor_worker.py</code> <pre><code>def create_embedding(self, text: str) -&gt; List[float]:\n    \"\"\"\n    Create embedding for the given text\n    Args:\n        text (str): The text to generate embedding for\n\n    Returns:\n        List[float]: The embedding generated by the model\n\n    \"\"\"\n    if text is None:\n        raise ValueError(\"Text is required\")\n\n    logger.info(f\"Creating embedding for text: {text}\")\n    return self.llm.create_embedding(text)\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/handler/","title":"Handler","text":""},{"location":"Sources/Agent/modules/quantization_llm/handler/#Agent.modules.quantization_llm.handler.QuantizationLLM","title":"<code>QuantizationLLM</code>","text":"Source code in <code>Agent/modules/quantization_llm/handler.py</code> <pre><code>class QuantizationLLM:\n    def __init__(self, api: API):\n        \"\"\"\n        Here is used to load and manage the quantization LLM model\n\n        Args:\n            api (API): The API object to query the API\n        \"\"\"\n        # query the available models\n        # init for llm models\n        self.api_llm_available_models = api.get_available_models()\n        logger.info(f\"Available LLM Models: {len(self.api_llm_available_models)}\")\n        self.local_llm_available_models = {}\n        for model in self.api_llm_available_models:\n            self.local_llm_available_models[model[\"model_name\"]] = (\n                QuantizationLLMModelConfig(**model)\n            )\n\n    def handle_task(self, task: Task):\n        \"\"\"\n        Handle the task\n        Args:\n            task (Task): The task to handle\n\n        Returns:\n\n        \"\"\"\n        TimeLogger.log_task(task, \"start_quantization_llm\")\n        result_profile = {}\n        latency_profile = {}\n        quantization_llm_parameters = QuantizationLLMParameters(**task.parameters)\n        text = quantization_llm_parameters.text\n        llm_model_name = quantization_llm_parameters.llm_model_name\n        # get llm_model\n        llm_model = self.local_llm_available_models.get(llm_model_name, None)\n        if llm_model is None:\n            logger.error(f\"Model {llm_model_name} not found\")\n            task.result_status = ResultStatus.failed.value\n            task.description = f\"Model {llm_model_name} not found\"\n            return task\n\n        if llm_model.llm is None:\n            logger.error(f\"Model {llm_model_name} not loaded\")\n            try:\n                with time_tracker(\n                    \"init_llm\", latency_profile, track_type=TrackType.MODEL.value\n                ):\n                    llm_model.init_llm()\n            except Exception as llm_err:\n                logger.exception(llm_err)\n                task.result_status = ResultStatus.failed.value\n                task.description = str(llm_err)\n                return task\n        with time_tracker(\"infer\", latency_profile, track_type=TrackType.MODEL.value):\n            logger.info(f\"Text: {text}\")\n            res_text, logs = self.infer(\n                text=text,\n                llm_model_config=llm_model,\n            )\n        result_profile[\"logs\"] = logs\n        result_profile[\"text\"] = res_text\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        TimeLogger.log_task(task, \"end_quantization_llm\")\n        return task\n\n    @staticmethod\n    def infer(text: str, llm_model_config: QuantizationLLMModelConfig):\n        \"\"\"\n        Infer the task\n        Args:\n            text (str): The text to infer\n            llm_model_config (QuantizationLLMModelConfig): The llm model config\n\n        Returns:\n\n        \"\"\"\n        llm_adaptor = QuantizationLLMAdaptor(llm_model_config)\n        res = llm_adaptor.create_chat_completion(\n            prompt=text,\n        )\n        logger.info(res)\n        text = res[\"choices\"][0][\"message\"][\"content\"]\n\n        return text, res\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/handler/#Agent.modules.quantization_llm.handler.QuantizationLLM.__init__","title":"<code>__init__(api)</code>","text":"<p>Here is used to load and manage the quantization LLM model</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>API</code> <p>The API object to query the API</p> required Source code in <code>Agent/modules/quantization_llm/handler.py</code> <pre><code>def __init__(self, api: API):\n    \"\"\"\n    Here is used to load and manage the quantization LLM model\n\n    Args:\n        api (API): The API object to query the API\n    \"\"\"\n    # query the available models\n    # init for llm models\n    self.api_llm_available_models = api.get_available_models()\n    logger.info(f\"Available LLM Models: {len(self.api_llm_available_models)}\")\n    self.local_llm_available_models = {}\n    for model in self.api_llm_available_models:\n        self.local_llm_available_models[model[\"model_name\"]] = (\n            QuantizationLLMModelConfig(**model)\n        )\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/handler/#Agent.modules.quantization_llm.handler.QuantizationLLM.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task (Task): The task to handle</p> <p>Returns:</p> Source code in <code>Agent/modules/quantization_llm/handler.py</code> <pre><code>def handle_task(self, task: Task):\n    \"\"\"\n    Handle the task\n    Args:\n        task (Task): The task to handle\n\n    Returns:\n\n    \"\"\"\n    TimeLogger.log_task(task, \"start_quantization_llm\")\n    result_profile = {}\n    latency_profile = {}\n    quantization_llm_parameters = QuantizationLLMParameters(**task.parameters)\n    text = quantization_llm_parameters.text\n    llm_model_name = quantization_llm_parameters.llm_model_name\n    # get llm_model\n    llm_model = self.local_llm_available_models.get(llm_model_name, None)\n    if llm_model is None:\n        logger.error(f\"Model {llm_model_name} not found\")\n        task.result_status = ResultStatus.failed.value\n        task.description = f\"Model {llm_model_name} not found\"\n        return task\n\n    if llm_model.llm is None:\n        logger.error(f\"Model {llm_model_name} not loaded\")\n        try:\n            with time_tracker(\n                \"init_llm\", latency_profile, track_type=TrackType.MODEL.value\n            ):\n                llm_model.init_llm()\n        except Exception as llm_err:\n            logger.exception(llm_err)\n            task.result_status = ResultStatus.failed.value\n            task.description = str(llm_err)\n            return task\n    with time_tracker(\"infer\", latency_profile, track_type=TrackType.MODEL.value):\n        logger.info(f\"Text: {text}\")\n        res_text, logs = self.infer(\n            text=text,\n            llm_model_config=llm_model,\n        )\n    result_profile[\"logs\"] = logs\n    result_profile[\"text\"] = res_text\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    TimeLogger.log_task(task, \"end_quantization_llm\")\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/handler/#Agent.modules.quantization_llm.handler.QuantizationLLM.infer","title":"<code>infer(text, llm_model_config)</code>  <code>staticmethod</code>","text":"<p>Infer the task Args:     text (str): The text to infer     llm_model_config (QuantizationLLMModelConfig): The llm model config</p> <p>Returns:</p> Source code in <code>Agent/modules/quantization_llm/handler.py</code> <pre><code>@staticmethod\ndef infer(text: str, llm_model_config: QuantizationLLMModelConfig):\n    \"\"\"\n    Infer the task\n    Args:\n        text (str): The text to infer\n        llm_model_config (QuantizationLLMModelConfig): The llm model config\n\n    Returns:\n\n    \"\"\"\n    llm_adaptor = QuantizationLLMAdaptor(llm_model_config)\n    res = llm_adaptor.create_chat_completion(\n        prompt=text,\n    )\n    logger.info(res)\n    text = res[\"choices\"][0][\"message\"][\"content\"]\n\n    return text, res\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/models/","title":"Models","text":""},{"location":"Sources/Agent/modules/quantization_llm/models/#Agent.modules.quantization_llm.models.QuantizationLLMModelConfig","title":"<code>QuantizationLLMModelConfig</code>","text":"Source code in <code>Agent/modules/quantization_llm/models.py</code> <pre><code>class QuantizationLLMModelConfig:\n    def __init__(\n        self,\n        model_name: str,\n        model_family: str,\n        repo: str,\n        filename: str,\n        file_size: float,\n        available: bool,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize the LLM Model Config\n        Args:\n            model_name (str): The name of the model\n            model_size (str): The size of the model\n            model_family (str): The family of the model\n            model_type (str): The type of the model\n            repo (str): The repo of the model\n            filename (str): The filename of the model\n            file_size (float): The size of the model file\n            available (bool): If the model is\n            *args:\n            **kwargs:\n        \"\"\"\n        self.model_name = model_name\n        self.model_family = model_family\n        self.repo = repo\n        self.filename = filename\n        self.file_size = file_size\n        self.available = available\n        self.llm = None\n        logger.debug(args)\n        logger.debug(kwargs)\n\n    def model_path(self):\n        \"\"\"\n        Check or load the model from the local directory\n        Returns:\n\n        \"\"\"\n        model_file = LLM_MODEL_DIR / self.model_family / self.filename\n        if model_file.exists():\n            return model_file\n        if self.download_model():\n            return model_file\n        return None\n\n    def download_model(self):\n        \"\"\"\n        If the model is not available, download it from the HuggingFace Hub\n        Returns:\n        \"\"\"\n\n        download_url = hf_hub_url(repo_id=self.repo, filename=self.filename)\n        logger.critical(f\"Downloading model from {download_url}\")\n        model_general_folder = LLM_MODEL_DIR / self.model_family\n        logger.critical(f\"Model folder {model_general_folder}\")\n        model_general_folder.mkdir(parents=True, exist_ok=True)\n        filename = model_general_folder / self.filename\n        response = requests.get(download_url, stream=True)\n        # Total size in bytes.\n        total_size = int(response.headers.get(\"content-length\", 0))\n        block_size = 1024  # 1 Kilobyte\n        logger.critical(f\"Downloading {self.filename} to {model_general_folder}\")\n        logger.critical(f\"Total size: {total_size}\")\n        progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n        with open(filename, \"wb\") as file:\n            for data in response.iter_content(block_size):\n                progress_bar.update(len(data))\n                file.write(data)\n        progress_bar.close()\n        if total_size != 0 and progress_bar.n != total_size:\n            logger.error(\"ERROR, something went wrong\")\n            return False\n        return True\n\n    def init_llm(self):\n        self.llm = Llama(\n            model_path=self.model_path().as_posix(),\n            n_gpu_layers=-1,\n            embedding=True,\n            n_ctx=4096,\n        )\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/models/#Agent.modules.quantization_llm.models.QuantizationLLMModelConfig.__init__","title":"<code>__init__(model_name, model_family, repo, filename, file_size, available, *args, **kwargs)</code>","text":"<p>Initialize the LLM Model Config Args:     model_name (str): The name of the model     model_size (str): The size of the model     model_family (str): The family of the model     model_type (str): The type of the model     repo (str): The repo of the model     filename (str): The filename of the model     file_size (float): The size of the model file     available (bool): If the model is     args:     *kwargs:</p> Source code in <code>Agent/modules/quantization_llm/models.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    model_family: str,\n    repo: str,\n    filename: str,\n    file_size: float,\n    available: bool,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Initialize the LLM Model Config\n    Args:\n        model_name (str): The name of the model\n        model_size (str): The size of the model\n        model_family (str): The family of the model\n        model_type (str): The type of the model\n        repo (str): The repo of the model\n        filename (str): The filename of the model\n        file_size (float): The size of the model file\n        available (bool): If the model is\n        *args:\n        **kwargs:\n    \"\"\"\n    self.model_name = model_name\n    self.model_family = model_family\n    self.repo = repo\n    self.filename = filename\n    self.file_size = file_size\n    self.available = available\n    self.llm = None\n    logger.debug(args)\n    logger.debug(kwargs)\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/models/#Agent.modules.quantization_llm.models.QuantizationLLMModelConfig.download_model","title":"<code>download_model()</code>","text":"<p>If the model is not available, download it from the HuggingFace Hub Returns:</p> Source code in <code>Agent/modules/quantization_llm/models.py</code> <pre><code>def download_model(self):\n    \"\"\"\n    If the model is not available, download it from the HuggingFace Hub\n    Returns:\n    \"\"\"\n\n    download_url = hf_hub_url(repo_id=self.repo, filename=self.filename)\n    logger.critical(f\"Downloading model from {download_url}\")\n    model_general_folder = LLM_MODEL_DIR / self.model_family\n    logger.critical(f\"Model folder {model_general_folder}\")\n    model_general_folder.mkdir(parents=True, exist_ok=True)\n    filename = model_general_folder / self.filename\n    response = requests.get(download_url, stream=True)\n    # Total size in bytes.\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kilobyte\n    logger.critical(f\"Downloading {self.filename} to {model_general_folder}\")\n    logger.critical(f\"Total size: {total_size}\")\n    progress_bar = tqdm(total=total_size, unit=\"iB\", unit_scale=True)\n    with open(filename, \"wb\") as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size != 0 and progress_bar.n != total_size:\n        logger.error(\"ERROR, something went wrong\")\n        return False\n    return True\n</code></pre>"},{"location":"Sources/Agent/modules/quantization_llm/models/#Agent.modules.quantization_llm.models.QuantizationLLMModelConfig.model_path","title":"<code>model_path()</code>","text":"<p>Check or load the model from the local directory Returns:</p> Source code in <code>Agent/modules/quantization_llm/models.py</code> <pre><code>def model_path(self):\n    \"\"\"\n    Check or load the model from the local directory\n    Returns:\n\n    \"\"\"\n    model_file = LLM_MODEL_DIR / self.model_family / self.filename\n    if model_file.exists():\n        return model_file\n    if self.download_model():\n        return model_file\n    return None\n</code></pre>"},{"location":"Sources/Agent/modules/rag/handler/","title":"Handler","text":""},{"location":"Sources/Agent/modules/rag/handler/#Agent.modules.rag.handler.RAGHandler","title":"<code>RAGHandler</code>","text":"Source code in <code>Agent/modules/rag/handler.py</code> <pre><code>class RAGHandler:\n    def __init__(self):\n        pass\n\n    def handle_task(self, task: Task) -&gt; Task:\n        \"\"\"\n        Handle the task\n        Args:\n            task:\n\n        Returns:\n\n        \"\"\"\n        result_profile = {}\n        latency_profile = {}\n        TimeLogger.log_task(task, \"start_rag\")\n        # NOTE: this is a placeholder for the actual implementation\n        result_profile[\"text\"] = \"This is a placeholder for the actual implementation\"\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        TimeLogger.log_task(task, \"end_rag\")\n        return task\n</code></pre>"},{"location":"Sources/Agent/modules/rag/handler/#Agent.modules.rag.handler.RAGHandler.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Handle the task Args:     task:</p> <p>Returns:</p> Source code in <code>Agent/modules/rag/handler.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Task:\n    \"\"\"\n    Handle the task\n    Args:\n        task:\n\n    Returns:\n\n    \"\"\"\n    result_profile = {}\n    latency_profile = {}\n    TimeLogger.log_task(task, \"start_rag\")\n    # NOTE: this is a placeholder for the actual implementation\n    result_profile[\"text\"] = \"This is a placeholder for the actual implementation\"\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    TimeLogger.log_task(task, \"end_rag\")\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/rag/neo4j_connector/","title":"Neo4jConnector","text":""},{"location":"Sources/Agent/modules/rag/postgresql_connector/","title":"PostgreSQLConnector","text":""},{"location":"Sources/Agent/modules/speech_to_text/speech2text/","title":"Speech2Text","text":""},{"location":"Sources/Agent/modules/speech_to_text/speech2text/#Agent.modules.speech_to_text.speech2text.Speech2Text","title":"<code>Speech2Text</code>","text":"Source code in <code>Agent/modules/speech_to_text/speech2text.py</code> <pre><code>class Speech2Text:\n    SUPPORTED_MODELS = [\"whisper\"]\n\n    def __init__(\n        self,\n        model_name: str = \"whisper\",\n        model_size: str = \"small\",\n        multi_language: bool = True,\n    ):\n        \"\"\"\n        Initialize the translator\n        Args:\n            model_name (str): The name of the model to use\n            model_size (str): The size of the model to use\n            multi_language (bool): If the model is multi-language\n        \"\"\"\n        self.model_name = model_name\n        if self.model_name == \"whisper\":\n            if not multi_language and \"large\" not in model_size:\n                model_size = f\"{model_size}.en\"\n            self.audio_model = whisper.load_model(model_size)\n        else:\n            raise ValueError(f\"Model {model_name} not supported\")\n\n    @staticmethod\n    def locate_audio_file(uid: str, sequence_index: str, end_time: str):\n        \"\"\"\n        Locate the audio file\n        Args:\n            uid (str): The uid\n            sequence_index (str): The sequence index\n            end_time (str): The end time\n\n        Returns:\n            The audio file (str): The audio file\n        \"\"\"\n        audio_folder = CLIENT_DATA_FOLDER / \"audio\" / uid\n        # audio file will be within this folder, and name like sequence_index-endtimetimestap.wav\n        end_time_obj = datetime.strptime(end_time, \"%Y-%m-%dT%H:%M:%S.%f\")\n        audio_file = (\n            audio_folder\n            / f\"{sequence_index}-{end_time_obj.strftime('%Y%m%d%H%M%S')}.wav\"\n        )\n        if not audio_file.exists():\n            logger.error(f\"Audio file {audio_file} not found\")\n            raise FileNotFoundError(f\"Audio file {audio_file} not found\")\n        return audio_file\n\n    def translate(self, message: Speech2TextParameters, task: Task) -&gt; Task:\n        \"\"\"\n        This is the key function to translate the audio to text\n        Args:\n            message (dict): The message to translate\n            task (Task): The task\n\n        Returns:\n            task (Task): The task\n\n        \"\"\"\n\n        logger.info(f\"Translating message {message}\")\n        # read the data from the audio file in .wav file, then do the translation\n        audio_file = self.locate_audio_file(\n            message.uid, message.audio_index, message.end_time\n        )\n        logger.info(f\"Audio file {audio_file}\")\n        if audio_file is None:\n            return task\n\n        with timer(logger, \"Loading audio\"):\n            with time_tracker(\n                \"load_audio\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n            ):\n                audio_np = whisper.load_audio(audio_file.as_posix())\n\n        with timer(logger, \"Transcribing\"):\n            with time_tracker(\n                \"transcribe\",\n                task.result_json.latency_profile,\n                track_type=TrackType.MODEL.value,\n            ):\n                result = self.audio_model.transcribe(\n                    audio_np, fp16=torch.cuda.is_available()\n                )\n        logger.critical(result)\n        task.result_json.result_profile.update(result)\n        return task\n\n    def handle_task(self, task: Task) -&gt; Task:\n        \"\"\"\n        Args:\n            task: The task to process\n\n        Returns:\n            The processed task\n        \"\"\"\n        try:\n            task_parameters = Speech2TextParameters(**task.parameters)\n            TimeLogger.log_task(task, \"start_translate\")\n            task = self.translate(task_parameters, task)\n            TimeLogger.log_task(task, \"end_translate\")\n            task.result_status = ResultStatus.completed.value\n        except FileNotFoundError:\n            # then we need to try later as the sync is not done yet\n            logger.error(\"Audio file not found, will try later\")\n            task.result_status = ResultStatus.pending.value\n        except Exception as e:\n            logger.error(e)\n            task.result_status = ResultStatus.failed.value\n            task.description = str(e)\n        return task\n</code></pre>"},{"location":"Sources/Agent/modules/speech_to_text/speech2text/#Agent.modules.speech_to_text.speech2text.Speech2Text.__init__","title":"<code>__init__(model_name='whisper', model_size='small', multi_language=True)</code>","text":"<p>Initialize the translator Args:     model_name (str): The name of the model to use     model_size (str): The size of the model to use     multi_language (bool): If the model is multi-language</p> Source code in <code>Agent/modules/speech_to_text/speech2text.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"whisper\",\n    model_size: str = \"small\",\n    multi_language: bool = True,\n):\n    \"\"\"\n    Initialize the translator\n    Args:\n        model_name (str): The name of the model to use\n        model_size (str): The size of the model to use\n        multi_language (bool): If the model is multi-language\n    \"\"\"\n    self.model_name = model_name\n    if self.model_name == \"whisper\":\n        if not multi_language and \"large\" not in model_size:\n            model_size = f\"{model_size}.en\"\n        self.audio_model = whisper.load_model(model_size)\n    else:\n        raise ValueError(f\"Model {model_name} not supported\")\n</code></pre>"},{"location":"Sources/Agent/modules/speech_to_text/speech2text/#Agent.modules.speech_to_text.speech2text.Speech2Text.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to process</p> required <p>Returns:</p> Type Description <code>Task</code> <p>The processed task</p> Source code in <code>Agent/modules/speech_to_text/speech2text.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Task:\n    \"\"\"\n    Args:\n        task: The task to process\n\n    Returns:\n        The processed task\n    \"\"\"\n    try:\n        task_parameters = Speech2TextParameters(**task.parameters)\n        TimeLogger.log_task(task, \"start_translate\")\n        task = self.translate(task_parameters, task)\n        TimeLogger.log_task(task, \"end_translate\")\n        task.result_status = ResultStatus.completed.value\n    except FileNotFoundError:\n        # then we need to try later as the sync is not done yet\n        logger.error(\"Audio file not found, will try later\")\n        task.result_status = ResultStatus.pending.value\n    except Exception as e:\n        logger.error(e)\n        task.result_status = ResultStatus.failed.value\n        task.description = str(e)\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/speech_to_text/speech2text/#Agent.modules.speech_to_text.speech2text.Speech2Text.locate_audio_file","title":"<code>locate_audio_file(uid, sequence_index, end_time)</code>  <code>staticmethod</code>","text":"<p>Locate the audio file Args:     uid (str): The uid     sequence_index (str): The sequence index     end_time (str): The end time</p> <p>Returns:</p> Type Description <p>The audio file (str): The audio file</p> Source code in <code>Agent/modules/speech_to_text/speech2text.py</code> <pre><code>@staticmethod\ndef locate_audio_file(uid: str, sequence_index: str, end_time: str):\n    \"\"\"\n    Locate the audio file\n    Args:\n        uid (str): The uid\n        sequence_index (str): The sequence index\n        end_time (str): The end time\n\n    Returns:\n        The audio file (str): The audio file\n    \"\"\"\n    audio_folder = CLIENT_DATA_FOLDER / \"audio\" / uid\n    # audio file will be within this folder, and name like sequence_index-endtimetimestap.wav\n    end_time_obj = datetime.strptime(end_time, \"%Y-%m-%dT%H:%M:%S.%f\")\n    audio_file = (\n        audio_folder\n        / f\"{sequence_index}-{end_time_obj.strftime('%Y%m%d%H%M%S')}.wav\"\n    )\n    if not audio_file.exists():\n        logger.error(f\"Audio file {audio_file} not found\")\n        raise FileNotFoundError(f\"Audio file {audio_file} not found\")\n    return audio_file\n</code></pre>"},{"location":"Sources/Agent/modules/speech_to_text/speech2text/#Agent.modules.speech_to_text.speech2text.Speech2Text.translate","title":"<code>translate(message, task)</code>","text":"<p>This is the key function to translate the audio to text Args:     message (dict): The message to translate     task (Task): The task</p> <p>Returns:</p> Name Type Description <code>task</code> <code>Task</code> <p>The task</p> Source code in <code>Agent/modules/speech_to_text/speech2text.py</code> <pre><code>def translate(self, message: Speech2TextParameters, task: Task) -&gt; Task:\n    \"\"\"\n    This is the key function to translate the audio to text\n    Args:\n        message (dict): The message to translate\n        task (Task): The task\n\n    Returns:\n        task (Task): The task\n\n    \"\"\"\n\n    logger.info(f\"Translating message {message}\")\n    # read the data from the audio file in .wav file, then do the translation\n    audio_file = self.locate_audio_file(\n        message.uid, message.audio_index, message.end_time\n    )\n    logger.info(f\"Audio file {audio_file}\")\n    if audio_file is None:\n        return task\n\n    with timer(logger, \"Loading audio\"):\n        with time_tracker(\n            \"load_audio\",\n            task.result_json.latency_profile,\n            track_type=TrackType.MODEL.value,\n        ):\n            audio_np = whisper.load_audio(audio_file.as_posix())\n\n    with timer(logger, \"Transcribing\"):\n        with time_tracker(\n            \"transcribe\",\n            task.result_json.latency_profile,\n            track_type=TrackType.MODEL.value,\n        ):\n            result = self.audio_model.transcribe(\n                audio_np, fp16=torch.cuda.is_available()\n            )\n    logger.critical(result)\n    task.result_json.result_profile.update(result)\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/text_to_speech/text2speech/","title":"Text2Speech","text":""},{"location":"Sources/Agent/modules/text_to_speech/text2speech/#Agent.modules.text_to_speech.text2speech.Text2Speech","title":"<code>Text2Speech</code>","text":"Source code in <code>Agent/modules/text_to_speech/text2speech.py</code> <pre><code>class Text2Speech:\n\n    def __init__(self, model_name: str = \"openai\", to_s3: bool = False):\n        \"\"\"\n        Initialize the STT object\n\n        Args:\n            model_name (str): The name of the model to use\n            to_s3 (bool): If the audio file should be uploaded to S3\n        \"\"\"\n\n        self.tts = None\n        self.model_name = model_name\n        self.to_s3 = to_s3\n\n    def handle_task(self, task: Task) -&gt; Task:\n        \"\"\"\n        Args:\n            task (Task): The task to handle\n\n        Returns:\n            The task with the result\n        \"\"\"\n        TimeLogger.log_task(task, \"start_text2speech\")\n        text2speech_parameters = Text2SpeechParameters(**task.parameters)\n        logger.info(f\"Text to speech: {text2speech_parameters.text}\")\n\n        if self.model_name == \"openai\":\n            return self.text_to_speech_openai(\n                task=task, task_param=text2speech_parameters\n            )\n        TimeLogger.log_task(task, \"end_text2speech\")\n        return task\n\n    def text_to_speech_openai(\n        self, task: Task, task_param: Text2SpeechParameters\n    ) -&gt; Task:\n        \"\"\"\n        Convert the text to speech using OpenAI API\n        Args:\n            task (Task): The task to handle\n            task_param (Text2SpeechParameters): The parameters for the task\n\n        Returns:\n\n        \"\"\"\n        result_profile = {}\n        latency_profile = {}\n        audio_file_path = DATA_DIR / \"tts\" / f\"{task.id}.mp3\"\n        # if folder does not exist, create it\n        audio_file_path.parent.mkdir(parents=True, exist_ok=True)\n        audio_file_path = audio_file_path.as_posix()\n\n        client = OpenAI()\n        with time_tracker(\"openai_tts\", latency_profile, TrackType.MODEL.value):\n            response = client.audio.speech.create(\n                model=\"tts-1\",\n                voice=\"alloy\",\n                input=task_param.text,\n            )\n        with time_tracker(\"save_audio\", latency_profile, TrackType.TRANSFER.value):\n            response.stream_to_file(audio_file_path)\n\n        result_profile[\"audio_file_path\"] = audio_file_path.split(\"/\")[-1]\n\n        if self.to_s3:\n            with time_tracker(\"to_s3\", latency_profile, TrackType.TRANSFER.value):\n                self.upload_to_s3(audio_file_path, f\"tts/{task.id}.mp3\")\n\n        task.result_status = ResultStatus.completed.value\n        task.result_json.result_profile.update(result_profile)\n        task.result_json.latency_profile.update(latency_profile)\n        return task\n\n    @staticmethod\n    def upload_to_s3(file_path: str, s3_key: str):\n        \"\"\"\n        Upload the file to S3\n        Args:\n            file_path (str): The path to the file\n            s3_key (str): The key to use in S3\n\n        \"\"\"\n        s3_client = BOTO3_SESSION.client(\"s3\")\n        s3_client.upload_file(\n            file_path,\n            S3_BUCKET,\n            s3_key,\n        )\n</code></pre>"},{"location":"Sources/Agent/modules/text_to_speech/text2speech/#Agent.modules.text_to_speech.text2speech.Text2Speech.__init__","title":"<code>__init__(model_name='openai', to_s3=False)</code>","text":"<p>Initialize the STT object</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>The name of the model to use</p> <code>'openai'</code> <code>to_s3</code> <code>bool</code> <p>If the audio file should be uploaded to S3</p> <code>False</code> Source code in <code>Agent/modules/text_to_speech/text2speech.py</code> <pre><code>def __init__(self, model_name: str = \"openai\", to_s3: bool = False):\n    \"\"\"\n    Initialize the STT object\n\n    Args:\n        model_name (str): The name of the model to use\n        to_s3 (bool): If the audio file should be uploaded to S3\n    \"\"\"\n\n    self.tts = None\n    self.model_name = model_name\n    self.to_s3 = to_s3\n</code></pre>"},{"location":"Sources/Agent/modules/text_to_speech/text2speech/#Agent.modules.text_to_speech.text2speech.Text2Speech.handle_task","title":"<code>handle_task(task)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>The task to handle</p> required <p>Returns:</p> Type Description <code>Task</code> <p>The task with the result</p> Source code in <code>Agent/modules/text_to_speech/text2speech.py</code> <pre><code>def handle_task(self, task: Task) -&gt; Task:\n    \"\"\"\n    Args:\n        task (Task): The task to handle\n\n    Returns:\n        The task with the result\n    \"\"\"\n    TimeLogger.log_task(task, \"start_text2speech\")\n    text2speech_parameters = Text2SpeechParameters(**task.parameters)\n    logger.info(f\"Text to speech: {text2speech_parameters.text}\")\n\n    if self.model_name == \"openai\":\n        return self.text_to_speech_openai(\n            task=task, task_param=text2speech_parameters\n        )\n    TimeLogger.log_task(task, \"end_text2speech\")\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/text_to_speech/text2speech/#Agent.modules.text_to_speech.text2speech.Text2Speech.text_to_speech_openai","title":"<code>text_to_speech_openai(task, task_param)</code>","text":"<p>Convert the text to speech using OpenAI API Args:     task (Task): The task to handle     task_param (Text2SpeechParameters): The parameters for the task</p> <p>Returns:</p> Source code in <code>Agent/modules/text_to_speech/text2speech.py</code> <pre><code>def text_to_speech_openai(\n    self, task: Task, task_param: Text2SpeechParameters\n) -&gt; Task:\n    \"\"\"\n    Convert the text to speech using OpenAI API\n    Args:\n        task (Task): The task to handle\n        task_param (Text2SpeechParameters): The parameters for the task\n\n    Returns:\n\n    \"\"\"\n    result_profile = {}\n    latency_profile = {}\n    audio_file_path = DATA_DIR / \"tts\" / f\"{task.id}.mp3\"\n    # if folder does not exist, create it\n    audio_file_path.parent.mkdir(parents=True, exist_ok=True)\n    audio_file_path = audio_file_path.as_posix()\n\n    client = OpenAI()\n    with time_tracker(\"openai_tts\", latency_profile, TrackType.MODEL.value):\n        response = client.audio.speech.create(\n            model=\"tts-1\",\n            voice=\"alloy\",\n            input=task_param.text,\n        )\n    with time_tracker(\"save_audio\", latency_profile, TrackType.TRANSFER.value):\n        response.stream_to_file(audio_file_path)\n\n    result_profile[\"audio_file_path\"] = audio_file_path.split(\"/\")[-1]\n\n    if self.to_s3:\n        with time_tracker(\"to_s3\", latency_profile, TrackType.TRANSFER.value):\n            self.upload_to_s3(audio_file_path, f\"tts/{task.id}.mp3\")\n\n    task.result_status = ResultStatus.completed.value\n    task.result_json.result_profile.update(result_profile)\n    task.result_json.latency_profile.update(latency_profile)\n    return task\n</code></pre>"},{"location":"Sources/Agent/modules/text_to_speech/text2speech/#Agent.modules.text_to_speech.text2speech.Text2Speech.upload_to_s3","title":"<code>upload_to_s3(file_path, s3_key)</code>  <code>staticmethod</code>","text":"<p>Upload the file to S3 Args:     file_path (str): The path to the file     s3_key (str): The key to use in S3</p> Source code in <code>Agent/modules/text_to_speech/text2speech.py</code> <pre><code>@staticmethod\ndef upload_to_s3(file_path: str, s3_key: str):\n    \"\"\"\n    Upload the file to S3\n    Args:\n        file_path (str): The path to the file\n        s3_key (str): The key to use in S3\n\n    \"\"\"\n    s3_client = BOTO3_SESSION.client(\"s3\")\n    s3_client.upload_file(\n        file_path,\n        S3_BUCKET,\n        s3_key,\n    )\n</code></pre>"},{"location":"Sources/Agent/utils/api/","title":"API","text":""},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API","title":"<code>API</code>","text":"<p>This is the class to communicate with the API component</p> Source code in <code>Agent/utils/api.py</code> <pre><code>class API:\n    \"\"\"\n    This is the class to communicate with the API component\n    \"\"\"\n\n    def __init__(\n        self,\n        domain: str = API_DOMAIN,\n        token: str = \"\",\n        uuid: str = \"\",\n        task_name: str = \"llm\",\n    ):\n        \"\"\"\n        Init API class to communicate with the API\n        Args:\n            domain (str): The domain of the API\n            token (str): The token to authenticate\n            uuid (str): The UUID of the worker\n            task_name (str): The task type of the worker\n        \"\"\"\n        self.domain = domain\n        self.token = token\n        self.task_name = task_name\n        self.uuid = uuid\n        self.mac_address = getmac.get_mac_address()\n        self.ip_address = self.get_local_ip()\n\n    def verify_token(self) -&gt; bool:\n        try:\n            url = f\"{self.domain}/authenticate/api/token/verify/\"\n            r = requests.post(\n                url,\n                headers={\"Authorization\": f\"Token {self.token}\"},\n                data={\"token\": self.token},\n            )\n            logger.info(f\"POST {url} {r.status_code}\")\n            logger.info(r.json())\n            if r.status_code != 200:\n                return False\n            return True\n        except Exception as e:\n            logger.error(f\"Error verifying token: {e}\")\n            return False\n\n    def get_available_models(self):\n        \"\"\"\n        Get the available LLM models from the API\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/llm/config\"\n        r = requests.get(url, headers={\"Authorization\": f\"Token {self.token}\"})\n        logger.info(f\"GET {url} {r.status_code}\")\n        return r.json()\n\n    def get_task(self):\n        \"\"\"\n        Get the task from the API\n        Returns:\n\n        \"\"\"\n        logger.debug(self.task_name)\n        url = f\"{self.domain}/queue_task/task/{self.task_name}/\"\n        r = requests.get(url, headers={\"Authorization\": f\"Token {self.token}\"})\n        logger.info(f\"GET {url} {r.status_code}\")\n        logger.info(r.text)\n        if r.status_code != 200:\n            return None\n        return r.json()\n\n    def post_task_result(\n        self,\n        task: Task,\n    ):\n        \"\"\"\n        Post the task result to the API\n        Args:\n            task[Task]: The task to post the result\n\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/queue_task/{task.id}/update_result/\"\n        r = requests.post(\n            url,\n            data=task.json(),\n            headers={\n                \"Authorization\": f\"Token {self.token}\",\n                \"Content-Type\": \"application/json\",\n            },\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        logger.info(r.text)\n        if r.status_code != 200:\n            return None\n        return r.json()\n\n    def register_or_update_worker(self):\n        \"\"\"\n        Register or update the  worker\n        So we can know whether the worker is alive or not\n        \"\"\"\n        try:\n            url = f\"{self.domain}/queue_task/worker/\"\n            r = requests.post(\n                url,\n                data={\n                    \"uuid\": self.uuid,\n                    \"mac_address\": self.mac_address,\n                    \"ip_address\": self.ip_address,\n                    \"task_name\": self.task_name,\n                },\n                headers={\"Authorization\": f\"Token {self.token}\"},\n            )\n            logger.info(f\"POST {url} {r.status_code}\")\n            # logger.info(r.text)\n            return r.json()\n        except Exception as e:\n            logger.error(f\"Error registering worker: {e}\")\n\n    @staticmethod\n    def get_local_ip() -&gt; str:\n        \"\"\"\n        Get the local IP address\n        Returns:\n            str: The local IP address\n\n        \"\"\"\n        # Create a socket object\n        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        try:\n            # doesn't matter if the address is reachable\n            s.connect((\"10.255.255.255\", 1))\n            ip = s.getsockname()[0]\n        except Exception as e:\n            logger.error(f\"Error getting local IP: {e}\")\n            ip = \"127.0.0.1\"\n        finally:\n            s.close()\n        return ip\n\n    def get_storage_solution(self):\n        \"\"\"\n        Get the storage solution from the API\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/storage_solution/\"\n        r = requests.get(\n            url, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n        )\n        logger.info(f\"GET {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        data = r.json()\n        logger.info(data)\n        return data.get(\"storage_solution\", \"volume\")\n\n    def upload_file(\n        self,\n        source_file: str,\n        dest_path: str,\n    ):\n        \"\"\"\n        Upload the file to the API\n        \"\"\"\n        url = f\"{self.domain}/hardware/upload_file/\"\n        files = {\"file\": open(source_file, \"rb\")}\n        data = {\n            \"dest_path\": dest_path,\n        }\n        r = requests.post(\n            url,\n            files=files,\n            data=data,\n            headers={\"Authorization\": f\"Token {self.token}\"},\n            timeout=30,\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        return True\n\n    def list_files(self, from_time=None):\n        \"\"\"\n        List the files from the API\n        \"\"\"\n        url = f\"{self.domain}/hardware/list_files/\"\n        data = {\n            \"from_time\": from_time,\n        }\n        r = requests.get(\n            url,\n            data=data,\n            headers={\"Authorization\": f\"Token {self.token}\"},\n        )\n        logger.info(f\"GET {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        return r.json()\n\n    def download_file_link(self, file_id, file_type):\n        \"\"\"\n        Get the download file link\n        \"\"\"\n        url = f\"{self.domain}/hardware/download_file_link/\"\n        data = {\n            \"file_id\": file_id,\n            \"file_type\": file_type,\n        }\n\n        r = requests.get(\n            url,\n            data=data,\n            headers={\"Authorization\": f\"Token {self.token}\"},\n        )\n        logger.info(f\"GET {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        return r.json()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.__init__","title":"<code>__init__(domain=API_DOMAIN, token='', uuid='', task_name='llm')</code>","text":"<p>Init API class to communicate with the API Args:     domain (str): The domain of the API     token (str): The token to authenticate     uuid (str): The UUID of the worker     task_name (str): The task type of the worker</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def __init__(\n    self,\n    domain: str = API_DOMAIN,\n    token: str = \"\",\n    uuid: str = \"\",\n    task_name: str = \"llm\",\n):\n    \"\"\"\n    Init API class to communicate with the API\n    Args:\n        domain (str): The domain of the API\n        token (str): The token to authenticate\n        uuid (str): The UUID of the worker\n        task_name (str): The task type of the worker\n    \"\"\"\n    self.domain = domain\n    self.token = token\n    self.task_name = task_name\n    self.uuid = uuid\n    self.mac_address = getmac.get_mac_address()\n    self.ip_address = self.get_local_ip()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.download_file_link","title":"<code>download_file_link(file_id, file_type)</code>","text":"<p>Get the download file link</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def download_file_link(self, file_id, file_type):\n    \"\"\"\n    Get the download file link\n    \"\"\"\n    url = f\"{self.domain}/hardware/download_file_link/\"\n    data = {\n        \"file_id\": file_id,\n        \"file_type\": file_type,\n    }\n\n    r = requests.get(\n        url,\n        data=data,\n        headers={\"Authorization\": f\"Token {self.token}\"},\n    )\n    logger.info(f\"GET {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    return r.json()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.get_available_models","title":"<code>get_available_models()</code>","text":"<p>Get the available LLM models from the API Returns:</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def get_available_models(self):\n    \"\"\"\n    Get the available LLM models from the API\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/llm/config\"\n    r = requests.get(url, headers={\"Authorization\": f\"Token {self.token}\"})\n    logger.info(f\"GET {url} {r.status_code}\")\n    return r.json()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.get_local_ip","title":"<code>get_local_ip()</code>  <code>staticmethod</code>","text":"<p>Get the local IP address Returns:     str: The local IP address</p> Source code in <code>Agent/utils/api.py</code> <pre><code>@staticmethod\ndef get_local_ip() -&gt; str:\n    \"\"\"\n    Get the local IP address\n    Returns:\n        str: The local IP address\n\n    \"\"\"\n    # Create a socket object\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        # doesn't matter if the address is reachable\n        s.connect((\"10.255.255.255\", 1))\n        ip = s.getsockname()[0]\n    except Exception as e:\n        logger.error(f\"Error getting local IP: {e}\")\n        ip = \"127.0.0.1\"\n    finally:\n        s.close()\n    return ip\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.get_storage_solution","title":"<code>get_storage_solution()</code>","text":"<p>Get the storage solution from the API Returns:</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def get_storage_solution(self):\n    \"\"\"\n    Get the storage solution from the API\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/storage_solution/\"\n    r = requests.get(\n        url, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n    )\n    logger.info(f\"GET {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    data = r.json()\n    logger.info(data)\n    return data.get(\"storage_solution\", \"volume\")\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.get_task","title":"<code>get_task()</code>","text":"<p>Get the task from the API Returns:</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def get_task(self):\n    \"\"\"\n    Get the task from the API\n    Returns:\n\n    \"\"\"\n    logger.debug(self.task_name)\n    url = f\"{self.domain}/queue_task/task/{self.task_name}/\"\n    r = requests.get(url, headers={\"Authorization\": f\"Token {self.token}\"})\n    logger.info(f\"GET {url} {r.status_code}\")\n    logger.info(r.text)\n    if r.status_code != 200:\n        return None\n    return r.json()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.list_files","title":"<code>list_files(from_time=None)</code>","text":"<p>List the files from the API</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def list_files(self, from_time=None):\n    \"\"\"\n    List the files from the API\n    \"\"\"\n    url = f\"{self.domain}/hardware/list_files/\"\n    data = {\n        \"from_time\": from_time,\n    }\n    r = requests.get(\n        url,\n        data=data,\n        headers={\"Authorization\": f\"Token {self.token}\"},\n    )\n    logger.info(f\"GET {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    return r.json()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.post_task_result","title":"<code>post_task_result(task)</code>","text":"<p>Post the task result to the API Args:     task[Task]: The task to post the result</p> <p>Returns:</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def post_task_result(\n    self,\n    task: Task,\n):\n    \"\"\"\n    Post the task result to the API\n    Args:\n        task[Task]: The task to post the result\n\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/queue_task/{task.id}/update_result/\"\n    r = requests.post(\n        url,\n        data=task.json(),\n        headers={\n            \"Authorization\": f\"Token {self.token}\",\n            \"Content-Type\": \"application/json\",\n        },\n    )\n    logger.info(f\"POST {url} {r.status_code}\")\n    logger.info(r.text)\n    if r.status_code != 200:\n        return None\n    return r.json()\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.register_or_update_worker","title":"<code>register_or_update_worker()</code>","text":"<p>Register or update the  worker So we can know whether the worker is alive or not</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def register_or_update_worker(self):\n    \"\"\"\n    Register or update the  worker\n    So we can know whether the worker is alive or not\n    \"\"\"\n    try:\n        url = f\"{self.domain}/queue_task/worker/\"\n        r = requests.post(\n            url,\n            data={\n                \"uuid\": self.uuid,\n                \"mac_address\": self.mac_address,\n                \"ip_address\": self.ip_address,\n                \"task_name\": self.task_name,\n            },\n            headers={\"Authorization\": f\"Token {self.token}\"},\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        # logger.info(r.text)\n        return r.json()\n    except Exception as e:\n        logger.error(f\"Error registering worker: {e}\")\n</code></pre>"},{"location":"Sources/Agent/utils/api/#Agent.utils.api.API.upload_file","title":"<code>upload_file(source_file, dest_path)</code>","text":"<p>Upload the file to the API</p> Source code in <code>Agent/utils/api.py</code> <pre><code>def upload_file(\n    self,\n    source_file: str,\n    dest_path: str,\n):\n    \"\"\"\n    Upload the file to the API\n    \"\"\"\n    url = f\"{self.domain}/hardware/upload_file/\"\n    files = {\"file\": open(source_file, \"rb\")}\n    data = {\n        \"dest_path\": dest_path,\n    }\n    r = requests.post(\n        url,\n        files=files,\n        data=data,\n        headers={\"Authorization\": f\"Token {self.token}\"},\n        timeout=30,\n    )\n    logger.info(f\"POST {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    return True\n</code></pre>"},{"location":"Sources/Agent/utils/aws/","title":"AWS","text":""},{"location":"Sources/Agent/utils/constants/","title":"Constants","text":""},{"location":"Sources/Agent/utils/constants/#Agent.utils.constants.NORMAL_MODELS","title":"<code>NORMAL_MODELS = [BERT]</code>  <code>module-attribute</code>","text":"<pre><code>LLM_MODEL_DIR = BASE_DIR / \"llm\" / \"models\"\n\nAPI_DOMAIN = \"http://localhost:8000\"  # default domain\n\n# model types\nHF_LLAMA = \"HuggingFace\"\nMT_LLAMA = \"llama.cpp\"\nMT_API = \"api\"\nMT_CHATGLM = \"chatglm.cpp\"\nMODEL_TYPES = [HF_LLAMA, MT_LLAMA, MT_API, MT_CHATGLM]\n\n# model names\nMN_LLAMA2 = \"llama2\"\nMN_GEMMA = \"gemma\"\n\nBERT = \"bert\"\nNORMAL_MODELS = [BERT]\n</code></pre>"},{"location":"Sources/Agent/utils/get_logger/","title":"GetLogger","text":""},{"location":"Sources/Agent/utils/get_logger/#Agent.utils.get_logger.get_logger","title":"<code>get_logger(logger_name=None, stream=True)</code>","text":"<p>init the logger, give it proper format, log them both in terminal stream and file</p> Source code in <code>Agent/utils/get_logger.py</code> <pre><code>def get_logger(logger_name: Optional[str] = None, stream: bool = True):\n    \"\"\"\n    init the logger, give it proper format, log them both in terminal stream and file\n    \"\"\"\n    logging.basicConfig(\n        format=\"%(name)s: %(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n        datefmt=\"%Y-%m-%d:%H:%M:%S\",\n        level=logging.INFO,\n    )\n\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n    formatter = logging.Formatter(\n        \"CLIENT: %(name)s | %(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n    )\n    if not logger.hasHandlers() and stream:\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setFormatter(formatter)\n        stdout_handler.setLevel(logging.INFO)\n        logger.addHandler(stdout_handler)\n\n    return logger\n</code></pre>"},{"location":"Sources/Agent/utils/time_logger/","title":"TimeLogger","text":""},{"location":"Sources/Agent/utils/time_logger/#Agent.utils.time_logger.logger","title":"<code>logger = get_logger(__name__)</code>  <code>module-attribute</code>","text":"<p>For the latency</p> <p>If it is model, the name will start with model_xx, and it is a duration If it is transfer time, the name will start with transfer_xx, and it is a duration If it is just to log the timestamp, the name will start with ts_xx, and it is a timestamp</p>"},{"location":"Sources/Agent/utils/time_logger/#Agent.utils.time_logger.TimeLogger","title":"<code>TimeLogger</code>","text":"Source code in <code>Agent/utils/time_logger.py</code> <pre><code>class TimeLogger:\n\n    @staticmethod\n    def log_task(task: Task, name: str):\n        \"\"\"\n        Log the time taken to execute a block of code\n        Args:\n            task (Task): The task to store the time\n            name (str): The name of the block\n\n        Returns:\n\n        \"\"\"\n        # check whether the task has the latency profile\n\n        TimeLogger.log(task.result_json.latency_profile, name)\n\n    @staticmethod\n    def log(profile: dict, name: str):\n        \"\"\"\n        Log the time taken to execute a block of code\n        Args:\n            profile (dict): The profile to store the time\n            name (str): The name of the block\n\n        Returns:\n\n        \"\"\"\n        logger.info(profile)\n        logger.info(name)\n        profile[f\"ts_{name}\"] = datetime.now()\n</code></pre>"},{"location":"Sources/Agent/utils/time_logger/#Agent.utils.time_logger.TimeLogger.log","title":"<code>log(profile, name)</code>  <code>staticmethod</code>","text":"<p>Log the time taken to execute a block of code Args:     profile (dict): The profile to store the time     name (str): The name of the block</p> <p>Returns:</p> Source code in <code>Agent/utils/time_logger.py</code> <pre><code>@staticmethod\ndef log(profile: dict, name: str):\n    \"\"\"\n    Log the time taken to execute a block of code\n    Args:\n        profile (dict): The profile to store the time\n        name (str): The name of the block\n\n    Returns:\n\n    \"\"\"\n    logger.info(profile)\n    logger.info(name)\n    profile[f\"ts_{name}\"] = datetime.now()\n</code></pre>"},{"location":"Sources/Agent/utils/time_logger/#Agent.utils.time_logger.TimeLogger.log_task","title":"<code>log_task(task, name)</code>  <code>staticmethod</code>","text":"<p>Log the time taken to execute a block of code Args:     task (Task): The task to store the time     name (str): The name of the block</p> <p>Returns:</p> Source code in <code>Agent/utils/time_logger.py</code> <pre><code>@staticmethod\ndef log_task(task: Task, name: str):\n    \"\"\"\n    Log the time taken to execute a block of code\n    Args:\n        task (Task): The task to store the time\n        name (str): The name of the block\n\n    Returns:\n\n    \"\"\"\n    # check whether the task has the latency profile\n\n    TimeLogger.log(task.result_json.latency_profile, name)\n</code></pre>"},{"location":"Sources/Agent/utils/time_tracker/","title":"TimeTracker","text":""},{"location":"Sources/Agent/utils/time_tracker/#Agent.utils.time_tracker.time_tracker","title":"<code>time_tracker(label, profile, track_type=TrackType.MODEL.value)</code>","text":"<p>Track the time taken to execute a block of code Args:     label (str): The name of the block     profile (dict): The profile to store the time     track_type (str): The type of tracking</p> Source code in <code>Agent/utils/time_tracker.py</code> <pre><code>@contextmanager\ndef time_tracker(\n    label: str, profile: dict, track_type: TrackType = TrackType.MODEL.value\n):\n    \"\"\"\n    Track the time taken to execute a block of code\n    Args:\n        label (str): The name of the block\n        profile (dict): The profile to store the time\n        track_type (str): The type of tracking\n    \"\"\"\n    # It will be either model or transfer\n    start_time = time.time()\n    yield\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    profile[f\"{track_type}_{label}\"] = elapsed_time\n    logger.info(f\"{label} took {elapsed_time} seconds\")\n</code></pre>"},{"location":"Sources/Agent/utils/timer/","title":"Timer","text":""},{"location":"Sources/Agent/utils/timer/#Agent.utils.timer.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>Agent/utils/timer.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Parameters\n        ----------\n        logger: Logger\n            logger to write the logs\n        message: str\n            message to log, like start xxx\n        \"\"\"\n        self.message = message\n        self.logger = logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enter to start write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(self, context, value, traceback):\n        \"\"\"\n        context exit will write this\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/Agent/utils/timer/#Agent.utils.timer.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enter to start write this</p> Source code in <code>Agent/utils/timer.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enter to start write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"Sources/Agent/utils/timer/#Agent.utils.timer.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> Source code in <code>Agent/utils/timer.py</code> <pre><code>def __exit__(self, context, value, traceback):\n    \"\"\"\n    context exit will write this\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/Agent/utils/timer/#Agent.utils.timer.timer.__init__","title":"<code>__init__(logger, message)</code>","text":"<p>init the timer</p>"},{"location":"Sources/Agent/utils/timer/#Agent.utils.timer.timer.__init__--parameters","title":"Parameters","text":"<p>logger: Logger     logger to write the logs message: str     message to log, like start xxx</p> Source code in <code>Agent/utils/timer.py</code> <pre><code>def __init__(self, logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Parameters\n    ----------\n    logger: Logger\n        logger to write the logs\n    message: str\n        message to log, like start xxx\n    \"\"\"\n    self.message = message\n    self.logger = logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"},{"location":"Sources/Agent/utils/storage/api_sync_handler/","title":"APISyncHandler","text":""},{"location":"Sources/Agent/utils/storage/api_sync_handler/#Agent.utils.storage.api_sync_handler.APISyncHandler","title":"<code>APISyncHandler</code>","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Sync the files to s3 when they are created, modified, moved or deleted</p> Source code in <code>Agent/utils/storage/api_sync_handler.py</code> <pre><code>class APISyncHandler(FileSystemEventHandler):\n    \"\"\"\n    Sync the files to s3 when they are created, modified, moved or deleted\n    \"\"\"\n\n    def __init__(self, api: API):\n        super().__init__()\n        self.api = api\n\n    def on_any_event(self, event):\n        if event.is_directory:\n            return None\n\n        elif event.event_type in (\"created\", \"modified\", \"moved\", \"deleted\"):\n            # print(f\"Event type: {event.event_type} - Path: {event.src_path}\")\n            # only process .avi and .wav files\n            if event.src_path.split(\"/\")[-1].split(\".\")[-1] not in [\n                \"mp4\",\n                \"wav\",\n                \"mp3\",\n            ]:\n                return None\n            try:\n                self.api.upload_file(\n                    event.src_path,\n                    f\"Responder/{event.src_path.split(DATA_DIR.as_posix())[1].strip('/')}\",\n                )\n                logger.info(f\"Uploaded file to server: {event.src_path}\")\n            except Exception as e:\n                logger.error(f\"Error uploading file to s3: {e}\")\n</code></pre>"},{"location":"Sources/Agent/utils/storage/local_sync_handler/","title":"LocalSyncHandler","text":""},{"location":"Sources/Agent/utils/storage/local_sync_handler/#Agent.utils.storage.local_sync_handler.LocalSyncHandler","title":"<code>LocalSyncHandler</code>","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Sync the files to disk when they are created, modified, moved or deleted</p> Source code in <code>Agent/utils/storage/local_sync_handler.py</code> <pre><code>class LocalSyncHandler(FileSystemEventHandler):\n    \"\"\"\n    Sync the files to disk when they are created, modified, moved or deleted\n    \"\"\"\n\n    def __init__(self, src_path: str, dest_path: str, sshpass: str):\n        \"\"\"\n\n        Args:\n            src_path (str): The source path to sync\n            dest_path (str): The destination path to sync\n            sshpass (str): The password to ssh\n        \"\"\"\n        super().__init__()\n        self.src_path = src_path\n        self.dest_path = dest_path\n        self.sshpass = sshpass\n\n    def on_any_event(self, event):\n        \"\"\"\n        Sync the files to disk when they are created, modified, moved or deleted\n        Args:\n            event:\n\n        Returns:\n\n        \"\"\"\n        if event.is_directory:\n            return None\n        else:\n            if self.sshpass:\n                subprocess.call(\n                    [\n                        \"sshpass\",\n                        \"-p\",\n                        self.sshpass,\n                        \"rsync\",\n                        \"-avz\",\n                        \"--delete\",\n                        self.src_path,\n                        self.dest_path,\n                    ]\n                )\n            else:\n                # wer can set up the authentication first, then we can use the rsync command\n                subprocess.call(\n                    [\"rsync\", \"-avz\", \"--delete\", self.src_path, self.dest_path]\n                )\n</code></pre>"},{"location":"Sources/Agent/utils/storage/local_sync_handler/#Agent.utils.storage.local_sync_handler.LocalSyncHandler.__init__","title":"<code>__init__(src_path, dest_path, sshpass)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src_path</code> <code>str</code> <p>The source path to sync</p> required <code>dest_path</code> <code>str</code> <p>The destination path to sync</p> required <code>sshpass</code> <code>str</code> <p>The password to ssh</p> required Source code in <code>Agent/utils/storage/local_sync_handler.py</code> <pre><code>def __init__(self, src_path: str, dest_path: str, sshpass: str):\n    \"\"\"\n\n    Args:\n        src_path (str): The source path to sync\n        dest_path (str): The destination path to sync\n        sshpass (str): The password to ssh\n    \"\"\"\n    super().__init__()\n    self.src_path = src_path\n    self.dest_path = dest_path\n    self.sshpass = sshpass\n</code></pre>"},{"location":"Sources/Agent/utils/storage/local_sync_handler/#Agent.utils.storage.local_sync_handler.LocalSyncHandler.on_any_event","title":"<code>on_any_event(event)</code>","text":"<p>Sync the files to disk when they are created, modified, moved or deleted Args:     event:</p> <p>Returns:</p> Source code in <code>Agent/utils/storage/local_sync_handler.py</code> <pre><code>def on_any_event(self, event):\n    \"\"\"\n    Sync the files to disk when they are created, modified, moved or deleted\n    Args:\n        event:\n\n    Returns:\n\n    \"\"\"\n    if event.is_directory:\n        return None\n    else:\n        if self.sshpass:\n            subprocess.call(\n                [\n                    \"sshpass\",\n                    \"-p\",\n                    self.sshpass,\n                    \"rsync\",\n                    \"-avz\",\n                    \"--delete\",\n                    self.src_path,\n                    self.dest_path,\n                ]\n            )\n        else:\n            # wer can set up the authentication first, then we can use the rsync command\n            subprocess.call(\n                [\"rsync\", \"-avz\", \"--delete\", self.src_path, self.dest_path]\n            )\n</code></pre>"},{"location":"Sources/Agent/utils/storage/s3_sync_handler/","title":"S3SyncHandler","text":""},{"location":"Sources/Agent/utils/storage/s3_sync_handler/#Agent.utils.storage.s3_sync_handler.S3SyncHandler","title":"<code>S3SyncHandler</code>","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Sync the files to s3 when they are created, modified, moved or deleted</p> Source code in <code>Agent/utils/storage/s3_sync_handler.py</code> <pre><code>class S3SyncHandler(FileSystemEventHandler):\n    \"\"\"\n    Sync the files to s3 when they are created, modified, moved or deleted\n    \"\"\"\n\n    def __init__(self, s3_client):\n        super().__init__()\n        self.s3_client = s3_client\n\n    def on_any_event(self, event):\n        if event.is_directory:\n            return None\n\n        elif event.event_type in (\"created\", \"modified\", \"moved\", \"deleted\"):\n            # print(f\"Event type: {event.event_type} - Path: {event.src_path}\")\n            # only process .avi and .wav files\n            if event.src_path.split(\"/\")[-1].split(\".\")[-1] not in [\n                \"mp4\",\n                \"wav\",\n                \"mp3\",\n            ]:\n                return None\n            try:\n                self.s3_client.upload_file(\n                    event.src_path,\n                    S3_BUCKET,\n                    f\"Responder/{event.src_path.split(DATA_DIR.as_posix())[1].strip('/')}\",\n                )\n                logger.info(f\"Uploaded file to s3: {event.src_path}\")\n                # logger.info(f\"Listener/{event.src_path.split(DATA_DIR.as_posix())[1].strip('/')}\")\n            except Exception as e:\n                logger.error(f\"Error uploading file to s3: {e}\")\n</code></pre>"},{"location":"Sources/Client/Listener/api/","title":"API","text":""},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API","title":"<code>API</code>","text":"<p>This is used to communicate with the API.</p> <ul> <li>Register the device</li> <li>Post audio to the API</li> <li>Post video to the API</li> <li>[Optional] Queue speech to text</li> </ul> Source code in <code>Client/Listener/api.py</code> <pre><code>class API:\n    \"\"\"\n    This is used to communicate with the API.\n\n    - Register the device\n    - Post audio to the API\n    - Post video to the API\n    - [Optional] Queue speech to text\n    \"\"\"\n\n    def __init__(\n        self,\n        domain: str = API_DOMAIN,\n        token: str = \"\",\n        home_id: Optional[int] = None,\n        track_cluster: Optional[str] = None,\n    ):\n        \"\"\"\n        The API class for the responder\n\n        It will require the token and the endpoint to communicate with the API.\n\n        If you deploy the API to a cloud server, do not forget to change the domain to the server's domain.\n\n        Args:\n            domain (str): The domain of the API.\n            token (str): The token for the API.\n            home_id (int): The home ID.\n            track_cluster (str): The track cluster.\n\n        \"\"\"\n        self.domain = domain\n        self.token = token\n        self.mac_address = get_mac_address()\n        self.home_id = home_id\n        self.track_cluster = track_cluster\n\n    def set_track_id(self):\n        if self.track_cluster is None:\n            return None\n        uid = str(uuid4())\n        uid = uid.replace(\"-\", \"\")\n        track_id = f\"T-{self.track_cluster}-{uid}\"\n        logger.info(track_id)\n        return track_id\n\n    def register_device(\n        self,\n        device_name: Optional[str] = None,\n        device_type: Optional[str] = None,\n        description: Optional[str] = None,\n    ):\n        \"\"\"\n        Register the device to the API.\n        Args:\n            device_name (Optional[str]): The device name, you can name it if you want to distinguish it better later\n            device_type (Optional[str]): The device type, this can be used to distinguish the device type\n            description (Optional[str]): The description of the device\n\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/register/\"\n\n        r = requests.post(\n            url,\n            data={\n                \"home\": self.home_id,\n                \"mac_address\": self.mac_address,\n                \"device_name\": device_name,\n                \"device_type\": device_type,\n                \"description\": description,\n            },\n            headers={\"Authorization\": f\"Token {self.token}\"},\n            timeout=30,\n        )\n        logger.info(url)\n\n        logger.info(f\"POST {url} {r.status_code}\")\n\n    def post_audio(\n        self,\n        uid: str,\n        sequence_index: int,\n        audio_file: str,\n        start_time: datetime,\n        end_time: datetime,\n        track_id: str = None,\n    ):\n        \"\"\"\n        Post metadata of the audio to the API.\n        Args:\n            uid (str): uuid of the audio\n            sequence_index (int): The sequence index of the audio in this loop, together with uuid,\n                                  it can be used to identify the audio\n            audio_file (str): Path to the audio file, which will be synced to the API disk storage via another parameter\n            start_time (datetime): The start time of the audio\n            end_time (datetime): The end time of the audio\n            track_id (str): The track id of the task\n\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/audio/\"\n        r = requests.post(\n            url,\n            data={\n                \"home\": self.home_id,\n                \"uid\": uid,\n                \"sequence_index\": sequence_index,\n                \"audio_file\": audio_file,\n                \"start_time\": start_time,\n                \"end_time\": end_time,\n                \"hardware_device_mac_address\": self.mac_address,\n                \"track_id\": track_id,\n            },\n            headers={\"Authorization\": f\"Token {self.token}\"},\n            timeout=30,\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        if r.status_code != 201:\n            return None\n        return r.json()\n\n    def post_video(\n        self, uid: str, video_file: str, start_time: datetime, end_time: datetime\n    ):\n        \"\"\"\n        Post metadata of the video to the API.\n        Args:\n            uid (str): uuid of this video section\n            video_file (str): Path to the video file, which will be synced to the API disk storage via another parameter\n                              it will also hold the information in the file name about the start/end time\n            start_time (datetime): The start time of the video\n            end_time (datetime): The end time of the video\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/video/\"\n        data = {\n            \"home\": self.home_id,\n            \"uid\": uid,\n            \"hardware_device_mac_address\": self.mac_address,\n            \"video_file\": video_file,\n            \"start_time\": start_time.isoformat(),\n            \"end_time\": end_time.isoformat(),\n        }\n        logger.info(data)\n        r = requests.post(\n            url, data=data, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        return r.json()\n\n    def queue_speech_to_text(\n        self, uid: str, audio_index: str, start_time: datetime, end_time: datetime\n    ) -&gt; str:\n        \"\"\"\n        Optional, used to queue the speech to text task\n        Args:\n            uid (str): uuid of the audio\n            audio_index (str): The audio index, which can be used to identify the audio\n            start_time (datetime): The start time of the audio\n            end_time (datetime): The end time of the audio\n\n        Returns:\n            (str): The track id of the task\n\n        \"\"\"\n        track_id = self.set_track_id()\n        url = f\"{self.domain}/queue_task/ai_task/\"\n        data = {\n            \"name\": \"speech_to_text\",\n            \"task_name\": \"speech2text\",\n            \"parameters\": json.dumps(\n                {\n                    \"uid\": uid,\n                    \"home_id\": self.home_id,\n                    \"audio_index\": audio_index,\n                    \"start_time\": start_time.isoformat(),\n                    \"end_time\": end_time.isoformat(),\n                    \"hardware_device_mac_address\": self.mac_address,\n                }\n            ),\n            \"track_id\": track_id,\n        }\n        r = requests.post(\n            url, data=data, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        if r.status_code != 200:\n            logger.info(data)\n            return None\n        logger.info(r.json())\n        return track_id\n\n    def get_storage_solution(self):\n        \"\"\"\n        Get the storage solution from the API\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/storage_solution/\"\n        r = requests.get(\n            url, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n        )\n        logger.info(f\"GET {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        data = r.json()\n        logger.info(data)\n        return data.get(\"storage_solution\", \"local\")\n\n    def upload_file(\n        self,\n        source_file: str,\n        dest_path: str,\n    ):\n        \"\"\"\n        Upload the file to the API\n        \"\"\"\n        url = f\"{self.domain}/hardware/upload_file/\"\n        files = {\"file\": open(source_file, \"rb\")}\n        data = {\n            \"dest_path\": dest_path,\n        }\n        r = requests.post(\n            url,\n            files=files,\n            data=data,\n            headers={\"Authorization\": f\"Token {self.token}\"},\n            timeout=30,\n        )\n        logger.info(f\"POST {url} {r.status_code}\")\n        if r.status_code != 200:\n            return None\n        return True\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.__init__","title":"<code>__init__(domain=API_DOMAIN, token='', home_id=None, track_cluster=None)</code>","text":"<p>The API class for the responder</p> <p>It will require the token and the endpoint to communicate with the API.</p> <p>If you deploy the API to a cloud server, do not forget to change the domain to the server's domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>str</code> <p>The domain of the API.</p> <code>API_DOMAIN</code> <code>token</code> <code>str</code> <p>The token for the API.</p> <code>''</code> <code>home_id</code> <code>int</code> <p>The home ID.</p> <code>None</code> <code>track_cluster</code> <code>str</code> <p>The track cluster.</p> <code>None</code> Source code in <code>Client/Listener/api.py</code> <pre><code>def __init__(\n    self,\n    domain: str = API_DOMAIN,\n    token: str = \"\",\n    home_id: Optional[int] = None,\n    track_cluster: Optional[str] = None,\n):\n    \"\"\"\n    The API class for the responder\n\n    It will require the token and the endpoint to communicate with the API.\n\n    If you deploy the API to a cloud server, do not forget to change the domain to the server's domain.\n\n    Args:\n        domain (str): The domain of the API.\n        token (str): The token for the API.\n        home_id (int): The home ID.\n        track_cluster (str): The track cluster.\n\n    \"\"\"\n    self.domain = domain\n    self.token = token\n    self.mac_address = get_mac_address()\n    self.home_id = home_id\n    self.track_cluster = track_cluster\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.get_storage_solution","title":"<code>get_storage_solution()</code>","text":"<p>Get the storage solution from the API Returns:</p> Source code in <code>Client/Listener/api.py</code> <pre><code>def get_storage_solution(self):\n    \"\"\"\n    Get the storage solution from the API\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/storage_solution/\"\n    r = requests.get(\n        url, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n    )\n    logger.info(f\"GET {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    data = r.json()\n    logger.info(data)\n    return data.get(\"storage_solution\", \"local\")\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.post_audio","title":"<code>post_audio(uid, sequence_index, audio_file, start_time, end_time, track_id=None)</code>","text":"<p>Post metadata of the audio to the API. Args:     uid (str): uuid of the audio     sequence_index (int): The sequence index of the audio in this loop, together with uuid,                           it can be used to identify the audio     audio_file (str): Path to the audio file, which will be synced to the API disk storage via another parameter     start_time (datetime): The start time of the audio     end_time (datetime): The end time of the audio     track_id (str): The track id of the task</p> <p>Returns:</p> Source code in <code>Client/Listener/api.py</code> <pre><code>def post_audio(\n    self,\n    uid: str,\n    sequence_index: int,\n    audio_file: str,\n    start_time: datetime,\n    end_time: datetime,\n    track_id: str = None,\n):\n    \"\"\"\n    Post metadata of the audio to the API.\n    Args:\n        uid (str): uuid of the audio\n        sequence_index (int): The sequence index of the audio in this loop, together with uuid,\n                              it can be used to identify the audio\n        audio_file (str): Path to the audio file, which will be synced to the API disk storage via another parameter\n        start_time (datetime): The start time of the audio\n        end_time (datetime): The end time of the audio\n        track_id (str): The track id of the task\n\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/audio/\"\n    r = requests.post(\n        url,\n        data={\n            \"home\": self.home_id,\n            \"uid\": uid,\n            \"sequence_index\": sequence_index,\n            \"audio_file\": audio_file,\n            \"start_time\": start_time,\n            \"end_time\": end_time,\n            \"hardware_device_mac_address\": self.mac_address,\n            \"track_id\": track_id,\n        },\n        headers={\"Authorization\": f\"Token {self.token}\"},\n        timeout=30,\n    )\n    logger.info(f\"POST {url} {r.status_code}\")\n    if r.status_code != 201:\n        return None\n    return r.json()\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.post_video","title":"<code>post_video(uid, video_file, start_time, end_time)</code>","text":"<p>Post metadata of the video to the API. Args:     uid (str): uuid of this video section     video_file (str): Path to the video file, which will be synced to the API disk storage via another parameter                       it will also hold the information in the file name about the start/end time     start_time (datetime): The start time of the video     end_time (datetime): The end time of the video Returns:</p> Source code in <code>Client/Listener/api.py</code> <pre><code>def post_video(\n    self, uid: str, video_file: str, start_time: datetime, end_time: datetime\n):\n    \"\"\"\n    Post metadata of the video to the API.\n    Args:\n        uid (str): uuid of this video section\n        video_file (str): Path to the video file, which will be synced to the API disk storage via another parameter\n                          it will also hold the information in the file name about the start/end time\n        start_time (datetime): The start time of the video\n        end_time (datetime): The end time of the video\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/video/\"\n    data = {\n        \"home\": self.home_id,\n        \"uid\": uid,\n        \"hardware_device_mac_address\": self.mac_address,\n        \"video_file\": video_file,\n        \"start_time\": start_time.isoformat(),\n        \"end_time\": end_time.isoformat(),\n    }\n    logger.info(data)\n    r = requests.post(\n        url, data=data, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n    )\n    logger.info(f\"POST {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    return r.json()\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.queue_speech_to_text","title":"<code>queue_speech_to_text(uid, audio_index, start_time, end_time)</code>","text":"<p>Optional, used to queue the speech to text task Args:     uid (str): uuid of the audio     audio_index (str): The audio index, which can be used to identify the audio     start_time (datetime): The start time of the audio     end_time (datetime): The end time of the audio</p> <p>Returns:</p> Type Description <code>str</code> <p>The track id of the task</p> Source code in <code>Client/Listener/api.py</code> <pre><code>def queue_speech_to_text(\n    self, uid: str, audio_index: str, start_time: datetime, end_time: datetime\n) -&gt; str:\n    \"\"\"\n    Optional, used to queue the speech to text task\n    Args:\n        uid (str): uuid of the audio\n        audio_index (str): The audio index, which can be used to identify the audio\n        start_time (datetime): The start time of the audio\n        end_time (datetime): The end time of the audio\n\n    Returns:\n        (str): The track id of the task\n\n    \"\"\"\n    track_id = self.set_track_id()\n    url = f\"{self.domain}/queue_task/ai_task/\"\n    data = {\n        \"name\": \"speech_to_text\",\n        \"task_name\": \"speech2text\",\n        \"parameters\": json.dumps(\n            {\n                \"uid\": uid,\n                \"home_id\": self.home_id,\n                \"audio_index\": audio_index,\n                \"start_time\": start_time.isoformat(),\n                \"end_time\": end_time.isoformat(),\n                \"hardware_device_mac_address\": self.mac_address,\n            }\n        ),\n        \"track_id\": track_id,\n    }\n    r = requests.post(\n        url, data=data, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n    )\n    logger.info(f\"POST {url} {r.status_code}\")\n    if r.status_code != 200:\n        logger.info(data)\n        return None\n    logger.info(r.json())\n    return track_id\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.register_device","title":"<code>register_device(device_name=None, device_type=None, description=None)</code>","text":"<p>Register the device to the API. Args:     device_name (Optional[str]): The device name, you can name it if you want to distinguish it better later     device_type (Optional[str]): The device type, this can be used to distinguish the device type     description (Optional[str]): The description of the device</p> <p>Returns:</p> Source code in <code>Client/Listener/api.py</code> <pre><code>def register_device(\n    self,\n    device_name: Optional[str] = None,\n    device_type: Optional[str] = None,\n    description: Optional[str] = None,\n):\n    \"\"\"\n    Register the device to the API.\n    Args:\n        device_name (Optional[str]): The device name, you can name it if you want to distinguish it better later\n        device_type (Optional[str]): The device type, this can be used to distinguish the device type\n        description (Optional[str]): The description of the device\n\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/register/\"\n\n    r = requests.post(\n        url,\n        data={\n            \"home\": self.home_id,\n            \"mac_address\": self.mac_address,\n            \"device_name\": device_name,\n            \"device_type\": device_type,\n            \"description\": description,\n        },\n        headers={\"Authorization\": f\"Token {self.token}\"},\n        timeout=30,\n    )\n    logger.info(url)\n\n    logger.info(f\"POST {url} {r.status_code}\")\n</code></pre>"},{"location":"Sources/Client/Listener/api/#Client.Listener.api.API.upload_file","title":"<code>upload_file(source_file, dest_path)</code>","text":"<p>Upload the file to the API</p> Source code in <code>Client/Listener/api.py</code> <pre><code>def upload_file(\n    self,\n    source_file: str,\n    dest_path: str,\n):\n    \"\"\"\n    Upload the file to the API\n    \"\"\"\n    url = f\"{self.domain}/hardware/upload_file/\"\n    files = {\"file\": open(source_file, \"rb\")}\n    data = {\n        \"dest_path\": dest_path,\n    }\n    r = requests.post(\n        url,\n        files=files,\n        data=data,\n        headers={\"Authorization\": f\"Token {self.token}\"},\n        timeout=30,\n    )\n    logger.info(f\"POST {url} {r.status_code}\")\n    if r.status_code != 200:\n        return None\n    return True\n</code></pre>"},{"location":"Sources/Client/Listener/audios_acquire/","title":"Audio","text":""},{"location":"Sources/Client/Listener/audios_acquire/#Client.Listener.audios_acquire.AudioAcquire","title":"<code>AudioAcquire</code>","text":"Source code in <code>Client/Listener/audios_acquire.py</code> <pre><code>class AudioAcquire:\n    def __init__(\n        self,\n        api_domain: str = \"\",\n        token: str = \"\",\n        home_id: Optional[str] = \"\",\n        energy_threshold: int = 5000,\n        default_microphone: str = \"pulse\",\n        record_timeout: int = 30000,\n        sampling_time: float = 0.25,\n        track_cluster: Optional[str] = None,\n    ):\n        \"\"\"\n        The audio acquire class\n\n        Args:\n            api_domain (str): the api domain\n            token (str): the api token\n            home_id (str): the home id\n            energy_threshold (int): the energy threshold for the audio\n            default_microphone (str): the default microphone\n            record_timeout (int): the record timeout\n            sampling_time (float): the sampling time in seconds, default is 0.25\n            track_cluster (str): the track cluster\n        \"\"\"\n        self.uid = str(uuid.uuid4())\n        self.data_dir = DATA_DIR / \"audio\" / self.uid  # the data dir\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n\n        # api setup\n        self.api = API(\n            domain=api_domain, token=token, home_id=home_id, track_cluster=track_cluster\n        )\n        # register the device\n        self.api.register_device()\n\n        # the energy threshold for the microphone\n        self.energy_threshold = energy_threshold\n        # the default microphone\n        self.default_microphone = default_microphone\n        # the record timeout\n        self.record_timeout = record_timeout\n        # sampling time\n        self.sampling_time = sampling_time\n\n        # the audio index when record starts\n        self.audio_index = 0\n        logger.info(f\"session uid: {self.uid}\")\n        logger.info(f\"starting timestamp {datetime.now()}\")\n        self.source = self.get_source()\n\n    def get_source(self):\n        \"\"\"\n        Get the source of the audio\n        Returns:\n\n        \"\"\"\n\n        source = None\n\n        if \"linux\" in platform:\n            mic_name = self.default_microphone\n            # to do the debug\n            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n                logger.debug(index)\n                logger.debug(name)\n            if not mic_name or mic_name == \"list\":\n                logger.info(\"Available microphone devices are: \")\n                for index, name in enumerate(sr.Microphone.list_microphone_names()):\n                    logger.critical(f'Microphone with name \"{name}\" found')\n                return\n            else:\n                for index, name in enumerate(sr.Microphone.list_microphone_names()):\n                    if mic_name in name:\n                        logger.debug(index)\n                        logger.debug(name)\n                        source = sr.Microphone(sample_rate=16000, device_index=index)\n                        break\n        else:\n            source = sr.Microphone(sample_rate=16000)\n        return source\n\n    def run(self):\n        data_queue = Queue()\n        sample_time_queue = Queue()\n        audio_index_queue = Queue()\n        recorder = sr.Recognizer()\n        recorder.energy_threshold = self.energy_threshold\n        recorder.dynamic_energy_threshold = False\n\n        logger.critical(f\"Using microphone {self.source}\")\n\n        with self.source:\n            recorder.adjust_for_ambient_noise(self.source)\n\n        def record_callback(_, audio: sr.AudioData) -&gt; None:\n            \"\"\"\n            Threaded callback function to receive audio data when recordings finish.\n            Args:\n                _:\n                audio (sr.AudioData): An AudioData containing the recorded bytes.\n\n            Returns:\n\n            \"\"\"\n            with timer(logger, f\"Recording {self.audio_index}\"):\n                data = audio.get_raw_data()\n                wav_data = audio.get_wav_data()\n                sample_time = datetime.now()\n                data_queue.put(data)\n                sample_time_queue.put(sample_time)\n                audio_index_queue.put(self.audio_index)\n                curr_audio_dir = DATA_DIR / \"audio\" / self.uid\n                curr_audio_dir.mkdir(parents=True, exist_ok=True)\n\n                with open(\n                    curr_audio_dir\n                    / f\"{self.audio_index}-{sample_time.strftime('%Y%m%d%H%M%S')}.wav\",\n                    \"wb\",\n                ) as file:\n                    file.write(wav_data)\n                self.audio_index += 1\n\n        # Create a background thread that will pass us raw audio bytes.\n        # We could do this manually, but SpeechRecognizer provides a nice helper.\n        recorder.listen_in_background(\n            self.source, record_callback, phrase_time_limit=self.record_timeout\n        )  # phrase_time_limit continues to monitor the time\n\n        last_sample_start_time = datetime.now()\n        logger.info(\"Model loaded.\")\n        logger.info(\"Listening for audio...\")\n\n        while True:\n            try:\n                if not data_queue.empty():\n                    logger.info(\"no more sound, start transform...\")\n\n                    data_queue.queue.clear()\n                    last_sample_time = sample_time_queue.queue[-1]\n                    sample_time_queue.queue.clear()\n                    audio_index = audio_index_queue.queue[-1]\n                    audio_index_queue.queue.clear()\n\n                    track_id = self.api.queue_speech_to_text(\n                        self.uid,\n                        audio_index=str(audio_index),\n                        start_time=last_sample_start_time,\n                        end_time=last_sample_time,\n                    )\n                    self.api.post_audio(\n                        self.uid,\n                        audio_index,\n                        f\"{audio_index}-{last_sample_time.strftime('%Y%m%d%H%M%S')}.wav\",\n                        last_sample_start_time,\n                        last_sample_time,\n                        track_id=track_id,\n                    )\n                    last_sample_start_time = last_sample_time\n\n                    sleep(self.sampling_time)\n            except KeyboardInterrupt:\n                break\n</code></pre>"},{"location":"Sources/Client/Listener/audios_acquire/#Client.Listener.audios_acquire.AudioAcquire.__init__","title":"<code>__init__(api_domain='', token='', home_id='', energy_threshold=5000, default_microphone='pulse', record_timeout=30000, sampling_time=0.25, track_cluster=None)</code>","text":"<p>The audio acquire class</p> <p>Parameters:</p> Name Type Description Default <code>api_domain</code> <code>str</code> <p>the api domain</p> <code>''</code> <code>token</code> <code>str</code> <p>the api token</p> <code>''</code> <code>home_id</code> <code>str</code> <p>the home id</p> <code>''</code> <code>energy_threshold</code> <code>int</code> <p>the energy threshold for the audio</p> <code>5000</code> <code>default_microphone</code> <code>str</code> <p>the default microphone</p> <code>'pulse'</code> <code>record_timeout</code> <code>int</code> <p>the record timeout</p> <code>30000</code> <code>sampling_time</code> <code>float</code> <p>the sampling time in seconds, default is 0.25</p> <code>0.25</code> <code>track_cluster</code> <code>str</code> <p>the track cluster</p> <code>None</code> Source code in <code>Client/Listener/audios_acquire.py</code> <pre><code>def __init__(\n    self,\n    api_domain: str = \"\",\n    token: str = \"\",\n    home_id: Optional[str] = \"\",\n    energy_threshold: int = 5000,\n    default_microphone: str = \"pulse\",\n    record_timeout: int = 30000,\n    sampling_time: float = 0.25,\n    track_cluster: Optional[str] = None,\n):\n    \"\"\"\n    The audio acquire class\n\n    Args:\n        api_domain (str): the api domain\n        token (str): the api token\n        home_id (str): the home id\n        energy_threshold (int): the energy threshold for the audio\n        default_microphone (str): the default microphone\n        record_timeout (int): the record timeout\n        sampling_time (float): the sampling time in seconds, default is 0.25\n        track_cluster (str): the track cluster\n    \"\"\"\n    self.uid = str(uuid.uuid4())\n    self.data_dir = DATA_DIR / \"audio\" / self.uid  # the data dir\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n\n    # api setup\n    self.api = API(\n        domain=api_domain, token=token, home_id=home_id, track_cluster=track_cluster\n    )\n    # register the device\n    self.api.register_device()\n\n    # the energy threshold for the microphone\n    self.energy_threshold = energy_threshold\n    # the default microphone\n    self.default_microphone = default_microphone\n    # the record timeout\n    self.record_timeout = record_timeout\n    # sampling time\n    self.sampling_time = sampling_time\n\n    # the audio index when record starts\n    self.audio_index = 0\n    logger.info(f\"session uid: {self.uid}\")\n    logger.info(f\"starting timestamp {datetime.now()}\")\n    self.source = self.get_source()\n</code></pre>"},{"location":"Sources/Client/Listener/audios_acquire/#Client.Listener.audios_acquire.AudioAcquire.get_source","title":"<code>get_source()</code>","text":"<p>Get the source of the audio Returns:</p> Source code in <code>Client/Listener/audios_acquire.py</code> <pre><code>def get_source(self):\n    \"\"\"\n    Get the source of the audio\n    Returns:\n\n    \"\"\"\n\n    source = None\n\n    if \"linux\" in platform:\n        mic_name = self.default_microphone\n        # to do the debug\n        for index, name in enumerate(sr.Microphone.list_microphone_names()):\n            logger.debug(index)\n            logger.debug(name)\n        if not mic_name or mic_name == \"list\":\n            logger.info(\"Available microphone devices are: \")\n            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n                logger.critical(f'Microphone with name \"{name}\" found')\n            return\n        else:\n            for index, name in enumerate(sr.Microphone.list_microphone_names()):\n                if mic_name in name:\n                    logger.debug(index)\n                    logger.debug(name)\n                    source = sr.Microphone(sample_rate=16000, device_index=index)\n                    break\n    else:\n        source = sr.Microphone(sample_rate=16000)\n    return source\n</code></pre>"},{"location":"Sources/Client/Listener/audios_acquire/#Client.Listener.audios_acquire.main","title":"<code>main()</code>","text":"<p>The main function</p> Source code in <code>Client/Listener/audios_acquire.py</code> <pre><code>def main():\n    \"\"\"\n    The main function\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--api_domain\", default=\"http://localhost:8000\", help=\"API domain\", type=str\n    )\n    parser.add_argument(\"--token\", default=\"\", help=\"API token\", type=str)\n    parser.add_argument(\"--home_id\", default=None, help=\"which home it is\", type=str)\n\n    parser.add_argument(\n        \"--energy_threshold\",\n        default=5000,\n        help=\"Energy level for mic to detect.\",\n        type=int,\n    )\n    parser.add_argument(\n        \"--record_timeout\",\n        default=30000,\n        help=\"How real time the recording is in seconds.\",\n        type=float,\n    )\n\n    parser.add_argument(\n        \"--default_microphone\",\n        default=\"pulse\",\n        help=\"Default microphone name for SpeechRecognition. \"\n        \"Run this with 'list' to view available Microphones.\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--track_cluster\",\n        default=None,\n        help=\"The track cluster to be used\",\n        type=str,\n    )\n\n    args = parser.parse_args()\n\n    audio_acquire = AudioAcquire(\n        api_domain=args.api_domain,\n        token=args.token,\n        home_id=args.home_id,\n        energy_threshold=args.energy_threshold,\n        default_microphone=args.default_microphone,\n        record_timeout=args.record_timeout,\n        track_cluster=args.track_cluster,\n    )\n    audio_acquire.run()\n</code></pre>"},{"location":"Sources/Client/Listener/constants/","title":"Constants","text":""},{"location":"Sources/Client/Listener/constants/#Client.Listener.constants.S3_BUCKET","title":"<code>S3_BUCKET = 'openomni'</code>  <code>module-attribute</code>","text":""},{"location":"Sources/Client/Listener/constants/#Client.Listener.constants.S3_BUCKET--get-parent-of-current-folder-as-root","title":"get parent of current folder as root","text":"<p>ROOT_PATH = os.path.dirname(os.path.abspath(file)) DATA_DIR = Path(ROOT_PATH) / \"data\"</p> <p>DATA_DIR.mkdir(parents=True, exist_ok=True)</p> <p>API_DOMAIN = \"http://localhost:8000\"</p> <p>S3_BUCKET = \"openomni\" =&gt; change this to your bucket name</p>"},{"location":"Sources/Client/Listener/setup/","title":"Setup","text":""},{"location":"Sources/Client/Listener/storage/","title":"Storage","text":""},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.APISyncHandler","title":"<code>APISyncHandler</code>","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Sync the files to s3 when they are created, modified, moved or deleted</p> Source code in <code>Client/Listener/storage.py</code> <pre><code>class APISyncHandler(FileSystemEventHandler):\n    \"\"\"\n    Sync the files to s3 when they are created, modified, moved or deleted\n    \"\"\"\n\n    def __init__(self, home_id: int, api: API):\n        super().__init__()\n        self.home_id = home_id\n        self.api = api\n\n    def on_any_event(self, event):\n        if event.is_directory:\n            return None\n\n        elif event.event_type in (\"created\", \"modified\", \"moved\", \"deleted\"):\n            # print(f\"Event type: {event.event_type} - Path: {event.src_path}\")\n            # only process .avi and .wav files\n\n            if event.src_path.split(\"/\")[-1].split(\".\")[-1] not in [\n                \"mp4\",\n                \"wav\",\n                \"mp3\",\n                \"jpg\",\n                \"jpeg\",\n                \"png\",\n            ]:\n                return None\n            try:\n                self.api.upload_file(\n                    event.src_path,\n                    f\"Listener/{event.src_path.split(DATA_DIR.as_posix())[1].strip('/')}\",\n                )\n                logger.info(f\"Uploaded file to server: {event.src_path}\")\n            except Exception as e:\n                logger.error(f\"Error uploading file to s3: {e}\")\n</code></pre>"},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.LocalSyncHandler","title":"<code>LocalSyncHandler</code>","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Sync the files to disk when they are created, modified, moved or deleted</p> Source code in <code>Client/Listener/storage.py</code> <pre><code>class LocalSyncHandler(FileSystemEventHandler):\n    \"\"\"\n    Sync the files to disk when they are created, modified, moved or deleted\n    \"\"\"\n\n    def __init__(self, src_path: str, dest_path: str, sshpass: str):\n        \"\"\"\n\n        Args:\n            src_path (str): The source path to sync\n            dest_path (str): The destination path to sync\n            sshpass (str): The password to ssh\n        \"\"\"\n        super().__init__()\n        self.src_path = src_path\n        self.dest_path = dest_path\n        self.sshpass = sshpass\n\n    def on_any_event(self, event):\n        \"\"\"\n        Sync the files to disk when they are created, modified, moved or deleted\n        Args:\n            event:\n\n        Returns:\n\n        \"\"\"\n        if event.is_directory:\n            return None\n        else:\n            if self.sshpass:\n                subprocess.call(\n                    [\n                        \"sshpass\",\n                        \"-p\",\n                        self.sshpass,\n                        \"rsync\",\n                        \"-avz\",\n                        \"--delete\",\n                        self.src_path,\n                        self.dest_path,\n                    ]\n                )\n            else:\n                # wer can set up the authentication first, then we can use the rsync command\n                subprocess.call(\n                    [\"rsync\", \"-avz\", \"--delete\", self.src_path, self.dest_path]\n                )\n</code></pre>"},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.LocalSyncHandler.__init__","title":"<code>__init__(src_path, dest_path, sshpass)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>src_path</code> <code>str</code> <p>The source path to sync</p> required <code>dest_path</code> <code>str</code> <p>The destination path to sync</p> required <code>sshpass</code> <code>str</code> <p>The password to ssh</p> required Source code in <code>Client/Listener/storage.py</code> <pre><code>def __init__(self, src_path: str, dest_path: str, sshpass: str):\n    \"\"\"\n\n    Args:\n        src_path (str): The source path to sync\n        dest_path (str): The destination path to sync\n        sshpass (str): The password to ssh\n    \"\"\"\n    super().__init__()\n    self.src_path = src_path\n    self.dest_path = dest_path\n    self.sshpass = sshpass\n</code></pre>"},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.LocalSyncHandler.on_any_event","title":"<code>on_any_event(event)</code>","text":"<p>Sync the files to disk when they are created, modified, moved or deleted Args:     event:</p> <p>Returns:</p> Source code in <code>Client/Listener/storage.py</code> <pre><code>def on_any_event(self, event):\n    \"\"\"\n    Sync the files to disk when they are created, modified, moved or deleted\n    Args:\n        event:\n\n    Returns:\n\n    \"\"\"\n    if event.is_directory:\n        return None\n    else:\n        if self.sshpass:\n            subprocess.call(\n                [\n                    \"sshpass\",\n                    \"-p\",\n                    self.sshpass,\n                    \"rsync\",\n                    \"-avz\",\n                    \"--delete\",\n                    self.src_path,\n                    self.dest_path,\n                ]\n            )\n        else:\n            # wer can set up the authentication first, then we can use the rsync command\n            subprocess.call(\n                [\"rsync\", \"-avz\", \"--delete\", self.src_path, self.dest_path]\n            )\n</code></pre>"},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.S3SyncHandler","title":"<code>S3SyncHandler</code>","text":"<p>               Bases: <code>FileSystemEventHandler</code></p> <p>Sync the files to s3 when they are created, modified, moved or deleted</p> Source code in <code>Client/Listener/storage.py</code> <pre><code>class S3SyncHandler(FileSystemEventHandler):\n    \"\"\"\n    Sync the files to s3 when they are created, modified, moved or deleted\n    \"\"\"\n\n    def __init__(self, home_id: int, s3_client):\n        super().__init__()\n        self.home_id = home_id\n        self.s3_client = s3_client\n\n    def on_any_event(self, event):\n        if event.is_directory:\n            return None\n\n        elif event.event_type in (\"created\", \"modified\", \"moved\", \"deleted\"):\n            # print(f\"Event type: {event.event_type} - Path: {event.src_path}\")\n            # only process .avi and .wav files\n\n            if event.src_path.split(\"/\")[-1].split(\".\")[-1] not in [\n                \"mp4\",\n                \"wav\",\n                \"mp3\",\n                \"jpg\",\n                \"jpeg\",\n                \"png\",\n            ]:\n                return None\n            try:\n                self.s3_client.upload_file(\n                    event.src_path,\n                    S3_BUCKET,\n                    f\"Listener/{event.src_path.split(DATA_DIR.as_posix())[1].strip('/')}\",\n                )\n                logger.info(f\"Uploaded file to s3: {event.src_path}\")\n                # logger.info(f\"Listener/{event.src_path.split(DATA_DIR.as_posix())[1].strip('/')}\")\n            except Exception as e:\n                logger.error(f\"Error uploading file to s3: {e}\")\n</code></pre>"},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.StorageHandler","title":"<code>StorageHandler</code>","text":"Source code in <code>Client/Listener/storage.py</code> <pre><code>class StorageHandler:\n    def __init__(\n        self,\n        api_domain: str = \"\",\n        token: str = \"\",\n        home_id: int = None,\n        dest_dir: Optional[str] = None,\n        dest_password: Optional[str] = None,\n    ):\n        \"\"\"\n        Args:\n            api_domain (str): the api domain\n            token (str): the api token\n            home_id (int): the home id\n            dest_dir (str): the destination directory to sync, like\n            dest_password (str): the destination password to sync\n        \"\"\"\n        self.home_id = home_id\n        self.dest_dir = dest_dir\n        self.dest_password = dest_password\n        self.api = API(domain=api_domain, token=token, home_id=home_id)\n        self.storage_solution = self.api.get_storage_solution()\n\n    def process(self):\n        if self.storage_solution == STORAGE_SOLUTION_VOLUME:\n            logger.info(\"No need to process files\")\n            return\n\n        if self.storage_solution == STORAGE_SOLUTION_S3:\n            self.process_s3()\n\n        if self.storage_solution == STORAGE_SOLUTION_LOCAL:\n            self.process_local_network()\n\n        if self.storage_solution == STORAGE_SOLUTION_API:\n            self.process_api()\n\n    def process_s3(self):\n        observer = Observer()\n        s3_handler = S3SyncHandler(self.home_id, s3_client=boto3.client(\"s3\"))\n        observer.schedule(s3_handler, str(DATA_DIR), recursive=True)\n        observer.start()\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()\n\n    def process_local_network(self):\n        observer = Observer()\n        if not self.dest_dir:\n            logger.error(\"dest_dir is required for local network sync\")\n            return\n        local_handler = LocalSyncHandler(\n            src_path=str(DATA_DIR),\n            dest_path=self.dest_dir,\n            sshpass=self.dest_password,\n        )\n        observer.schedule(local_handler, str(DATA_DIR), recursive=True)\n        observer.start()\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()\n\n    def process_api(self):\n        observer = Observer()\n        api_handler = APISyncHandler(self.home_id, self.api)\n        observer.schedule(api_handler, str(DATA_DIR), recursive=True)\n        observer.start()\n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            observer.stop()\n        observer.join()\n</code></pre>"},{"location":"Sources/Client/Listener/storage/#Client.Listener.storage.StorageHandler.__init__","title":"<code>__init__(api_domain='', token='', home_id=None, dest_dir=None, dest_password=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>api_domain</code> <code>str</code> <p>the api domain</p> <code>''</code> <code>token</code> <code>str</code> <p>the api token</p> <code>''</code> <code>home_id</code> <code>int</code> <p>the home id</p> <code>None</code> <code>dest_dir</code> <code>str</code> <p>the destination directory to sync, like</p> <code>None</code> <code>dest_password</code> <code>str</code> <p>the destination password to sync</p> <code>None</code> Source code in <code>Client/Listener/storage.py</code> <pre><code>def __init__(\n    self,\n    api_domain: str = \"\",\n    token: str = \"\",\n    home_id: int = None,\n    dest_dir: Optional[str] = None,\n    dest_password: Optional[str] = None,\n):\n    \"\"\"\n    Args:\n        api_domain (str): the api domain\n        token (str): the api token\n        home_id (int): the home id\n        dest_dir (str): the destination directory to sync, like\n        dest_password (str): the destination password to sync\n    \"\"\"\n    self.home_id = home_id\n    self.dest_dir = dest_dir\n    self.dest_password = dest_password\n    self.api = API(domain=api_domain, token=token, home_id=home_id)\n    self.storage_solution = self.api.get_storage_solution()\n</code></pre>"},{"location":"Sources/Client/Listener/utils/","title":"Utils","text":""},{"location":"Sources/Client/Listener/utils/#Client.Listener.utils.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>Client/Listener/utils.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Args:\n            logger: Logger\n                the logger to log the message\n            message: str\n                the message to log\n        \"\"\"\n        self.message = message\n        self.logger = logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enter to start write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(self, context, value, traceback):\n        \"\"\"\n        context exit will write this\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/Client/Listener/utils/#Client.Listener.utils.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enter to start write this</p> Source code in <code>Client/Listener/utils.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enter to start write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"Sources/Client/Listener/utils/#Client.Listener.utils.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> Source code in <code>Client/Listener/utils.py</code> <pre><code>def __exit__(self, context, value, traceback):\n    \"\"\"\n    context exit will write this\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/Client/Listener/utils/#Client.Listener.utils.timer.__init__","title":"<code>__init__(logger, message)</code>","text":"<p>init the timer</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>Logger the logger to log the message</p> required <code>message</code> <code>str</code> <p>str the message to log</p> required Source code in <code>Client/Listener/utils.py</code> <pre><code>def __init__(self, logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Args:\n        logger: Logger\n            the logger to log the message\n        message: str\n            the message to log\n    \"\"\"\n    self.message = message\n    self.logger = logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"},{"location":"Sources/Client/Listener/utils/#Client.Listener.utils.get_logger","title":"<code>get_logger(logger_name=None, stream=True)</code>","text":"<p>init the logger, give it proper format, log them both in terminal stream and file</p> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>Optional[str]</code> <p>str the name of the logger</p> <code>None</code> <code>stream</code> <code>bool</code> <p>bool whether to log in the terminal stream</p> <code>True</code> Source code in <code>Client/Listener/utils.py</code> <pre><code>def get_logger(logger_name: Optional[str] = None, stream: bool = True):\n    \"\"\"\n    init the logger, give it proper format, log them both in terminal stream and file\n\n    Args:\n        logger_name: str\n            the name of the logger\n        stream: bool\n            whether to log in the terminal stream\n    \"\"\"\n    logging.basicConfig(\n        format=\"%(name)s: %(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n        datefmt=\"%Y-%m-%d:%H:%M:%S\",\n        level=logging.INFO,\n    )\n\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n    formatter = logging.Formatter(\n        \"CLIENT: %(name)s | %(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n    )\n    if not logger.hasHandlers() and stream:\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setFormatter(formatter)\n        stdout_handler.setLevel(logging.INFO)\n        logger.addHandler(stdout_handler)\n\n    return logger\n</code></pre>"},{"location":"Sources/Client/Listener/videos_acquire/","title":"Video","text":""},{"location":"Sources/Client/Listener/videos_acquire/#Client.Listener.videos_acquire.logger","title":"<code>logger = get_logger('video_acquire')</code>  <code>module-attribute</code>","text":"<p>PER_LENGTH = 1800  # 30 minutes</p>"},{"location":"Sources/Client/Listener/videos_acquire/#Client.Listener.videos_acquire.logger--the-screen-width-and-height","title":"the screen width and height","text":"<p>WIDTH = 640 HEIGHT = 480 FPS = 24.0</p>"},{"location":"Sources/Client/Listener/videos_acquire/#Client.Listener.videos_acquire.VideoAcquire","title":"<code>VideoAcquire</code>","text":"Source code in <code>Client/Listener/videos_acquire.py</code> <pre><code>class VideoAcquire:\n    def __init__(\n        self,\n        width=WIDTH,\n        height=HEIGHT,\n        fps=FPS,\n        per_video_length=PER_LENGTH,\n        api_domain=\"\",\n        token=\"\",\n        home_id: int = None,\n    ):\n        \"\"\"\n        init the video acquire\n        Args:\n            width: (int) the width of the video\n            height (int): the height of the video\n            fps (float): the frame per second\n            per_video_length (int): the length of the video\n            api_domain (str): the domain of the api\n            token (str): the token of the api\n            home_id (int): the home id\n        \"\"\"\n        self.uid = str(uuid.uuid4())\n        self.data_dir = DATA_DIR / \"videos\" / self.uid  # the data dir\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n        self.width = width  # the width and height of the video\n        self.height = height  # the width and height of the video\n        self.fps = fps  # frame per second\n        self.per_video_length = per_video_length  # the length of the video\n        self.api = API(domain=api_domain, token=token, home_id=home_id)\n        self.api.register_device()\n\n    def record(self):\n        \"\"\"\n        start to record the video\n        \"\"\"\n        segment_images = 60\n        seconds = 0\n        minutes = 1\n\n        # init the recording\n        cap = cv2.VideoCapture(0)\n        # set the width and height\n        cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)\n        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)\n        # set the frame per second\n        cap.set(cv2.CAP_PROP_FPS, 24.0)\n        # use the XVID codec\n        fourcc = cv2.VideoWriter_fourcc(*\"avc1\")  # noqa\n\n        cap_fps = cap.get(5)  # get the fps of the camera\n        logger.info(f\"the fps of the camera is {cap_fps}\")\n\n        start_time = datetime.now()\n        filename = self.data_dir / (start_time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".mp4\")\n\n        out = cv2.VideoWriter(\n            filename.as_posix(), fourcc, self.fps, (self.width, self.height)\n        )  # noqa\n        logger.info(\"start to record the video\")\n        flag = True\n        while flag:\n            try:\n                if (datetime.now() - start_time).seconds &gt;= self.per_video_length:\n                    # stop the recording and save the video when the time is up\n                    logger.info(f\"the recording is finished, saved to file: {filename}\")\n                    out.release()\n                    # TODO: post the video to the server\n                    self.api.post_video(\n                        self.uid,\n                        filename.as_posix().split(\"/\")[-1],\n                        start_time=start_time,\n                        end_time=datetime.now(),\n                    )\n                    # resume the recording\n                    start_time = datetime.now()\n                    filename = self.data_dir / (\n                        start_time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".mp4\"\n                    )\n                    out = cv2.VideoWriter(\n                        filename.as_posix(), fourcc, FPS, (self.width, self.height)\n                    )  # noqa\n                else:\n                    # read the frame\n                    logger.debug(\"Try to process the frame\")\n                    ret, frame = cap.read()\n                    if ret:\n                        logger.debug(\"write the frame\")\n                        out.write(frame)\n                        # cv2.imshow(\"frame\", frame)\n                        if seconds == segment_images:\n                            logger.info(\"begin the next frame segment\")\n                            seconds = 0\n                            minutes += 1\n                        if seconds &lt; segment_images:\n                            image_dir = (\n                                self.data_dir\n                                / \"frames\"\n                                / f\"{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n                            )\n                            image_dir.mkdir(parents=True, exist_ok=True)\n                            cv2.imwrite(\n                                (image_dir / f\"{seconds}.jpg\").as_posix(), frame\n                            )\n                            seconds += 1\n                if cv2.waitKey(1) == ord(\"q\"):\n                    break\n            except KeyboardInterrupt:\n                break\n        cap.release()\n</code></pre>"},{"location":"Sources/Client/Listener/videos_acquire/#Client.Listener.videos_acquire.VideoAcquire.__init__","title":"<code>__init__(width=WIDTH, height=HEIGHT, fps=FPS, per_video_length=PER_LENGTH, api_domain='', token='', home_id=None)</code>","text":"<p>init the video acquire Args:     width: (int) the width of the video     height (int): the height of the video     fps (float): the frame per second     per_video_length (int): the length of the video     api_domain (str): the domain of the api     token (str): the token of the api     home_id (int): the home id</p> Source code in <code>Client/Listener/videos_acquire.py</code> <pre><code>def __init__(\n    self,\n    width=WIDTH,\n    height=HEIGHT,\n    fps=FPS,\n    per_video_length=PER_LENGTH,\n    api_domain=\"\",\n    token=\"\",\n    home_id: int = None,\n):\n    \"\"\"\n    init the video acquire\n    Args:\n        width: (int) the width of the video\n        height (int): the height of the video\n        fps (float): the frame per second\n        per_video_length (int): the length of the video\n        api_domain (str): the domain of the api\n        token (str): the token of the api\n        home_id (int): the home id\n    \"\"\"\n    self.uid = str(uuid.uuid4())\n    self.data_dir = DATA_DIR / \"videos\" / self.uid  # the data dir\n    self.data_dir.mkdir(parents=True, exist_ok=True)\n    self.width = width  # the width and height of the video\n    self.height = height  # the width and height of the video\n    self.fps = fps  # frame per second\n    self.per_video_length = per_video_length  # the length of the video\n    self.api = API(domain=api_domain, token=token, home_id=home_id)\n    self.api.register_device()\n</code></pre>"},{"location":"Sources/Client/Listener/videos_acquire/#Client.Listener.videos_acquire.VideoAcquire.record","title":"<code>record()</code>","text":"<p>start to record the video</p> Source code in <code>Client/Listener/videos_acquire.py</code> <pre><code>def record(self):\n    \"\"\"\n    start to record the video\n    \"\"\"\n    segment_images = 60\n    seconds = 0\n    minutes = 1\n\n    # init the recording\n    cap = cv2.VideoCapture(0)\n    # set the width and height\n    cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.width)\n    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.height)\n    # set the frame per second\n    cap.set(cv2.CAP_PROP_FPS, 24.0)\n    # use the XVID codec\n    fourcc = cv2.VideoWriter_fourcc(*\"avc1\")  # noqa\n\n    cap_fps = cap.get(5)  # get the fps of the camera\n    logger.info(f\"the fps of the camera is {cap_fps}\")\n\n    start_time = datetime.now()\n    filename = self.data_dir / (start_time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".mp4\")\n\n    out = cv2.VideoWriter(\n        filename.as_posix(), fourcc, self.fps, (self.width, self.height)\n    )  # noqa\n    logger.info(\"start to record the video\")\n    flag = True\n    while flag:\n        try:\n            if (datetime.now() - start_time).seconds &gt;= self.per_video_length:\n                # stop the recording and save the video when the time is up\n                logger.info(f\"the recording is finished, saved to file: {filename}\")\n                out.release()\n                # TODO: post the video to the server\n                self.api.post_video(\n                    self.uid,\n                    filename.as_posix().split(\"/\")[-1],\n                    start_time=start_time,\n                    end_time=datetime.now(),\n                )\n                # resume the recording\n                start_time = datetime.now()\n                filename = self.data_dir / (\n                    start_time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".mp4\"\n                )\n                out = cv2.VideoWriter(\n                    filename.as_posix(), fourcc, FPS, (self.width, self.height)\n                )  # noqa\n            else:\n                # read the frame\n                logger.debug(\"Try to process the frame\")\n                ret, frame = cap.read()\n                if ret:\n                    logger.debug(\"write the frame\")\n                    out.write(frame)\n                    # cv2.imshow(\"frame\", frame)\n                    if seconds == segment_images:\n                        logger.info(\"begin the next frame segment\")\n                        seconds = 0\n                        minutes += 1\n                    if seconds &lt; segment_images:\n                        image_dir = (\n                            self.data_dir\n                            / \"frames\"\n                            / f\"{datetime.now().strftime('%Y-%m-%d_%H-%M')}\"\n                        )\n                        image_dir.mkdir(parents=True, exist_ok=True)\n                        cv2.imwrite(\n                            (image_dir / f\"{seconds}.jpg\").as_posix(), frame\n                        )\n                        seconds += 1\n            if cv2.waitKey(1) == ord(\"q\"):\n                break\n        except KeyboardInterrupt:\n            break\n    cap.release()\n</code></pre>"},{"location":"Sources/Client/Listener/mock/data_extraction/","title":"Mock","text":""},{"location":"Sources/Client/Listener/mock/data_extraction/#Client.Listener.mock.data_extraction.DataMock","title":"<code>DataMock</code>","text":"<p>We will first extract the audio and video from the video file. And then treat it as current time + any time in the future.</p> <p>Then save them into the data folder as other did</p> <ul> <li>audio<ul> <li>/audio/uuid/0-datetime.wav</li> </ul> </li> <li>video<ul> <li>/videos/uuid/datetime.mp4</li> <li>/video/uuid/frames/date-time/xx.jpg</li> </ul> </li> </ul> <p>For the mock US-Election debate It is: - 02:53,3:20,20:20,20:39,33:38,34:18,55:15,55:40,80:05,80:18</p> Source code in <code>Client/Listener/mock/data_extraction.py</code> <pre><code>class DataMock:\n    \"\"\"\n    We will first extract the audio and video from the video file.\n    And then treat it as current time + any time in the future.\n\n    Then save them into the data folder as other did\n\n    - audio\n        - /audio/uuid/0-datetime.wav\n    - video\n        - /videos/uuid/datetime.mp4\n        - /video/uuid/frames/date-time/xx.jpg\n\n\n    For the mock US-Election debate\n    It is:\n    - 02:53,3:20,20:20,20:39,33:38,34:18,55:15,55:40,80:05,80:18\n\n    \"\"\"\n\n    def __init__(\n        self,\n        api_domain: str,\n        token: str,\n        home_id: Optional[int],\n        track_cluster: str = None,\n    ):\n        self.api = API(\n            domain=api_domain, token=token, home_id=home_id, track_cluster=track_cluster\n        )\n        self.uid = str(uuid4())\n\n        self.audio_dir = DATA_DIR / \"audio\" / self.uid\n        self.audio_dir.mkdir(parents=True, exist_ok=True)\n        self.video_dir = DATA_DIR / \"videos\" / self.uid\n        self.frames_dir = self.video_dir / \"frames\"\n        self.frames_dir.mkdir(parents=True, exist_ok=True)\n        self.mock_dir = DATA_DIR / \"mock\" / \"output\"\n        self.mock_dir.mkdir(parents=True, exist_ok=True)\n\n        self.current_time = datetime.now()\n\n    def replay(self, time_ranges: List[Tuple[int, int]], input_video_path: str):\n        \"\"\"\n        Replays the audio and video from the specified time\n        Args:\n            time_ranges (List[int, int]): List of time ranges in seconds.\n            input_video_path (str): Path to the input video file.\n\n        Returns:\n\n        \"\"\"\n        for index, time_range in enumerate(time_ranges):\n            start_second, end_second = time_range\n            start_time = self.current_time + timedelta(seconds=start_second)\n            end_time = self.current_time + timedelta(seconds=end_second)\n\n            self.extract_audio_and_video(\n                input_video_path=input_video_path,\n                start_second=start_second,\n                end_second=end_second,\n                start_time=start_time,\n                end_time=end_time,\n                output_audio_path=self.audio_dir\n                / f\"{index}-{end_time.strftime('%Y%m%d%H%M%S')}.wav\",\n            )\n\n            track_id = self.api.queue_speech_to_text(\n                uid=self.uid,\n                audio_index=str(index),\n                start_time=start_time,\n                end_time=end_time,\n            )\n            self.api.post_audio(\n                uid=self.uid,\n                sequence_index=index,\n                audio_file=f\"{index}-{end_time.strftime('%Y%m%d%H%M%S')}.wav\",\n                start_time=start_time,\n                end_time=end_time,\n                track_id=track_id,\n            )\n\n    def extract_audio_and_video(\n        self,\n        input_video_path: str,\n        start_second: int,\n        end_second: int,\n        start_time: datetime,\n        end_time: datetime,\n        output_audio_path: str,\n    ):\n        \"\"\"\n        Extracts the audio and video from a specified segment of a video file.\n\n        Args:\n            input_video_path (str): Path to the input video file.\n            start_second (int): Start time in seconds.\n            end_second (int): End time in seconds.\n            output_audio_path (str): Path to save the extracted audio file.\n        \"\"\"\n        output_video_path = (\n            self.mock_dir\n            / f\"{input_video_path.split('/')[-1]}-{start_second}-{end_second}.mp4\"\n        ).as_posix()\n        # Load the video file\n        video_clip = VideoFileClip(input_video_path)\n\n        # Cut the video clip from start_time to end_time\n        sub_clip = video_clip.subclip(start_second, end_second)\n\n        # Write the video clip to the output path\n        sub_clip.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n\n        # Extract the audio from the sub clip\n        audio_clip = sub_clip.audio\n\n        # Write the audio clip to the output path\n        audio_clip.write_audiofile(output_audio_path)\n\n        # Close the clips\n        # video_clip.close()\n        sub_clip.close()\n        audio_clip.close()\n        video_clip.close()\n        # then I want ot split the video by minutes, each minute will have 1 mp4 file\n        # and the frames\n        start_minute = start_time.replace(second=0, microsecond=0)\n        end_minute = end_time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n\n        for i in range((end_minute - start_minute).seconds // 60):\n            logger.info(f\"Processing minute {i}\")\n            video_clip = VideoFileClip(input_video_path)\n            the_minute_start_time = start_minute + timedelta(minutes=i)\n            the_minute_end_time = start_minute + timedelta(minutes=i + 1)\n            the_minute_output_video_path = (\n                Path(self.video_dir)\n                / (the_minute_start_time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".mp4\")\n            ).as_posix()\n            # recover the seconds range for each minute\n            the_minute_start_second = (\n                the_minute_start_time - self.current_time\n            ).seconds\n            the_minute_end_second = (the_minute_end_time - self.current_time).seconds\n            logger.info(f\"{the_minute_start_second}-{the_minute_end_second}\")\n            minute_clip = video_clip.subclip(\n                the_minute_start_second, the_minute_end_second\n            )\n            minute_clip.write_videofile(\n                the_minute_output_video_path, codec=\"libx264\", audio_codec=\"aac\"\n            )\n            minute_clip.close()\n\n            # frames_folder\n            frames_folder = self.frames_dir / the_minute_start_time.strftime(\n                \"%Y-%m-%d_%H-%M\"\n            )\n            frames_folder.mkdir(parents=True, exist_ok=True)\n            self.split_video_in_minutes(\n                the_minute_output_video_path, frames_folder.as_posix()\n            )\n            self.api.post_video(\n                self.uid,\n                the_minute_output_video_path.split(\"/\")[-1],\n                start_time=the_minute_start_time,\n                end_time=the_minute_end_time,\n            )\n\n            video_clip.close()\n\n    @staticmethod\n    def split_video_in_minutes(video_path, output_folder, fps=1):\n        \"\"\"\n        Splits a video into images.\n\n        Args:\n            video_path (str): Path to the video file.\n            output_folder (str): Folder to save the extracted images.\n            fps (int): Frames per second to extract. Defaults to 1.\n        \"\"\"\n        # Load the video file\n        the_video_clip = VideoFileClip(video_path)\n\n        # Ensure the output folder exists\n        if not os.path.exists(output_folder):\n            os.makedirs(output_folder)\n\n        # Extract frames\n        for i, frame in enumerate(the_video_clip.iter_frames(fps=fps)):\n            # Save each frame as an image\n            frame_path = os.path.join(output_folder, f\"{i}.png\")\n            imageio.imwrite(frame_path, frame)\n\n        # Close the video clip\n        the_video_clip.close()\n</code></pre>"},{"location":"Sources/Client/Listener/mock/data_extraction/#Client.Listener.mock.data_extraction.DataMock.extract_audio_and_video","title":"<code>extract_audio_and_video(input_video_path, start_second, end_second, start_time, end_time, output_audio_path)</code>","text":"<p>Extracts the audio and video from a specified segment of a video file.</p> <p>Parameters:</p> Name Type Description Default <code>input_video_path</code> <code>str</code> <p>Path to the input video file.</p> required <code>start_second</code> <code>int</code> <p>Start time in seconds.</p> required <code>end_second</code> <code>int</code> <p>End time in seconds.</p> required <code>output_audio_path</code> <code>str</code> <p>Path to save the extracted audio file.</p> required Source code in <code>Client/Listener/mock/data_extraction.py</code> <pre><code>def extract_audio_and_video(\n    self,\n    input_video_path: str,\n    start_second: int,\n    end_second: int,\n    start_time: datetime,\n    end_time: datetime,\n    output_audio_path: str,\n):\n    \"\"\"\n    Extracts the audio and video from a specified segment of a video file.\n\n    Args:\n        input_video_path (str): Path to the input video file.\n        start_second (int): Start time in seconds.\n        end_second (int): End time in seconds.\n        output_audio_path (str): Path to save the extracted audio file.\n    \"\"\"\n    output_video_path = (\n        self.mock_dir\n        / f\"{input_video_path.split('/')[-1]}-{start_second}-{end_second}.mp4\"\n    ).as_posix()\n    # Load the video file\n    video_clip = VideoFileClip(input_video_path)\n\n    # Cut the video clip from start_time to end_time\n    sub_clip = video_clip.subclip(start_second, end_second)\n\n    # Write the video clip to the output path\n    sub_clip.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n\n    # Extract the audio from the sub clip\n    audio_clip = sub_clip.audio\n\n    # Write the audio clip to the output path\n    audio_clip.write_audiofile(output_audio_path)\n\n    # Close the clips\n    # video_clip.close()\n    sub_clip.close()\n    audio_clip.close()\n    video_clip.close()\n    # then I want ot split the video by minutes, each minute will have 1 mp4 file\n    # and the frames\n    start_minute = start_time.replace(second=0, microsecond=0)\n    end_minute = end_time.replace(second=0, microsecond=0) + timedelta(minutes=1)\n\n    for i in range((end_minute - start_minute).seconds // 60):\n        logger.info(f\"Processing minute {i}\")\n        video_clip = VideoFileClip(input_video_path)\n        the_minute_start_time = start_minute + timedelta(minutes=i)\n        the_minute_end_time = start_minute + timedelta(minutes=i + 1)\n        the_minute_output_video_path = (\n            Path(self.video_dir)\n            / (the_minute_start_time.strftime(\"%Y-%m-%d_%H-%M-%S\") + \".mp4\")\n        ).as_posix()\n        # recover the seconds range for each minute\n        the_minute_start_second = (\n            the_minute_start_time - self.current_time\n        ).seconds\n        the_minute_end_second = (the_minute_end_time - self.current_time).seconds\n        logger.info(f\"{the_minute_start_second}-{the_minute_end_second}\")\n        minute_clip = video_clip.subclip(\n            the_minute_start_second, the_minute_end_second\n        )\n        minute_clip.write_videofile(\n            the_minute_output_video_path, codec=\"libx264\", audio_codec=\"aac\"\n        )\n        minute_clip.close()\n\n        # frames_folder\n        frames_folder = self.frames_dir / the_minute_start_time.strftime(\n            \"%Y-%m-%d_%H-%M\"\n        )\n        frames_folder.mkdir(parents=True, exist_ok=True)\n        self.split_video_in_minutes(\n            the_minute_output_video_path, frames_folder.as_posix()\n        )\n        self.api.post_video(\n            self.uid,\n            the_minute_output_video_path.split(\"/\")[-1],\n            start_time=the_minute_start_time,\n            end_time=the_minute_end_time,\n        )\n\n        video_clip.close()\n</code></pre>"},{"location":"Sources/Client/Listener/mock/data_extraction/#Client.Listener.mock.data_extraction.DataMock.replay","title":"<code>replay(time_ranges, input_video_path)</code>","text":"<p>Replays the audio and video from the specified time Args:     time_ranges (List[int, int]): List of time ranges in seconds.     input_video_path (str): Path to the input video file.</p> <p>Returns:</p> Source code in <code>Client/Listener/mock/data_extraction.py</code> <pre><code>def replay(self, time_ranges: List[Tuple[int, int]], input_video_path: str):\n    \"\"\"\n    Replays the audio and video from the specified time\n    Args:\n        time_ranges (List[int, int]): List of time ranges in seconds.\n        input_video_path (str): Path to the input video file.\n\n    Returns:\n\n    \"\"\"\n    for index, time_range in enumerate(time_ranges):\n        start_second, end_second = time_range\n        start_time = self.current_time + timedelta(seconds=start_second)\n        end_time = self.current_time + timedelta(seconds=end_second)\n\n        self.extract_audio_and_video(\n            input_video_path=input_video_path,\n            start_second=start_second,\n            end_second=end_second,\n            start_time=start_time,\n            end_time=end_time,\n            output_audio_path=self.audio_dir\n            / f\"{index}-{end_time.strftime('%Y%m%d%H%M%S')}.wav\",\n        )\n\n        track_id = self.api.queue_speech_to_text(\n            uid=self.uid,\n            audio_index=str(index),\n            start_time=start_time,\n            end_time=end_time,\n        )\n        self.api.post_audio(\n            uid=self.uid,\n            sequence_index=index,\n            audio_file=f\"{index}-{end_time.strftime('%Y%m%d%H%M%S')}.wav\",\n            start_time=start_time,\n            end_time=end_time,\n            track_id=track_id,\n        )\n</code></pre>"},{"location":"Sources/Client/Listener/mock/data_extraction/#Client.Listener.mock.data_extraction.DataMock.split_video_in_minutes","title":"<code>split_video_in_minutes(video_path, output_folder, fps=1)</code>  <code>staticmethod</code>","text":"<p>Splits a video into images.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the video file.</p> required <code>output_folder</code> <code>str</code> <p>Folder to save the extracted images.</p> required <code>fps</code> <code>int</code> <p>Frames per second to extract. Defaults to 1.</p> <code>1</code> Source code in <code>Client/Listener/mock/data_extraction.py</code> <pre><code>@staticmethod\ndef split_video_in_minutes(video_path, output_folder, fps=1):\n    \"\"\"\n    Splits a video into images.\n\n    Args:\n        video_path (str): Path to the video file.\n        output_folder (str): Folder to save the extracted images.\n        fps (int): Frames per second to extract. Defaults to 1.\n    \"\"\"\n    # Load the video file\n    the_video_clip = VideoFileClip(video_path)\n\n    # Ensure the output folder exists\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Extract frames\n    for i, frame in enumerate(the_video_clip.iter_frames(fps=fps)):\n        # Save each frame as an image\n        frame_path = os.path.join(output_folder, f\"{i}.png\")\n        imageio.imwrite(frame_path, frame)\n\n    # Close the video clip\n    the_video_clip.close()\n</code></pre>"},{"location":"Sources/Client/Responder/api/","title":"API","text":""},{"location":"Sources/Client/Responder/api/#Client.Responder.api.API","title":"<code>API</code>","text":"Source code in <code>Client/Responder/api.py</code> <pre><code>class API:\n    def __init__(\n        self,\n        domain: str = API_DOMAIN,\n        token: str = \"\",\n        home_id: int = None,\n    ):\n        \"\"\"\n        init the api\n        Args:\n            domain (str): the domain of the api\n            token (str): the token of the api\n            home_id (int): the home id\n        \"\"\"\n        self.domain = domain\n        self.token = token\n        self.mac_address = get_mac_address()\n        self.home_id = home_id\n\n    def register_device(\n        self,\n        device_name: Optional[str] = None,\n        device_type: Optional[str] = None,\n        description: Optional[str] = None,\n    ):\n        \"\"\"\n        register the device\n        Args:\n            device_name (Optional[str]): the device name\n            device_type (Optional[str]): the device type\n            description (Optional[str]): the description of the device\n\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/register/\"\n\n        r = requests.post(\n            url,\n            data={\n                \"home\": self.home_id,\n                \"mac_address\": self.mac_address,\n                \"device_name\": device_name,\n                \"device_type\": device_type,\n                \"description\": description,\n            },\n            headers={\"Authorization\": f\"Token {self.token}\"},\n        )\n        logger.info(url)\n        logger.info(f\"POST {url} {r.status_code}\")\n\n    def get_spoken_speech(self):\n        \"\"\"\n        Call the API to get the speech to play\n        Returns:\n\n        \"\"\"\n        url = f\"{self.domain}/hardware/speech/?home_id={self.home_id}\"\n        logger.info(url)\n        r = requests.get(\n            url, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n        )\n\n        logger.info(f\"get {url} {r.status_code}\")\n        # logger.info(r.text)\n        if r.status_code != 200:\n            return []\n        return r.json()\n</code></pre>"},{"location":"Sources/Client/Responder/api/#Client.Responder.api.API.__init__","title":"<code>__init__(domain=API_DOMAIN, token='', home_id=None)</code>","text":"<p>init the api Args:     domain (str): the domain of the api     token (str): the token of the api     home_id (int): the home id</p> Source code in <code>Client/Responder/api.py</code> <pre><code>def __init__(\n    self,\n    domain: str = API_DOMAIN,\n    token: str = \"\",\n    home_id: int = None,\n):\n    \"\"\"\n    init the api\n    Args:\n        domain (str): the domain of the api\n        token (str): the token of the api\n        home_id (int): the home id\n    \"\"\"\n    self.domain = domain\n    self.token = token\n    self.mac_address = get_mac_address()\n    self.home_id = home_id\n</code></pre>"},{"location":"Sources/Client/Responder/api/#Client.Responder.api.API.get_spoken_speech","title":"<code>get_spoken_speech()</code>","text":"<p>Call the API to get the speech to play Returns:</p> Source code in <code>Client/Responder/api.py</code> <pre><code>def get_spoken_speech(self):\n    \"\"\"\n    Call the API to get the speech to play\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/speech/?home_id={self.home_id}\"\n    logger.info(url)\n    r = requests.get(\n        url, headers={\"Authorization\": f\"Token {self.token}\"}, timeout=30\n    )\n\n    logger.info(f\"get {url} {r.status_code}\")\n    # logger.info(r.text)\n    if r.status_code != 200:\n        return []\n    return r.json()\n</code></pre>"},{"location":"Sources/Client/Responder/api/#Client.Responder.api.API.register_device","title":"<code>register_device(device_name=None, device_type=None, description=None)</code>","text":"<p>register the device Args:     device_name (Optional[str]): the device name     device_type (Optional[str]): the device type     description (Optional[str]): the description of the device</p> <p>Returns:</p> Source code in <code>Client/Responder/api.py</code> <pre><code>def register_device(\n    self,\n    device_name: Optional[str] = None,\n    device_type: Optional[str] = None,\n    description: Optional[str] = None,\n):\n    \"\"\"\n    register the device\n    Args:\n        device_name (Optional[str]): the device name\n        device_type (Optional[str]): the device type\n        description (Optional[str]): the description of the device\n\n    Returns:\n\n    \"\"\"\n    url = f\"{self.domain}/hardware/register/\"\n\n    r = requests.post(\n        url,\n        data={\n            \"home\": self.home_id,\n            \"mac_address\": self.mac_address,\n            \"device_name\": device_name,\n            \"device_type\": device_type,\n            \"description\": description,\n        },\n        headers={\"Authorization\": f\"Token {self.token}\"},\n    )\n    logger.info(url)\n    logger.info(f\"POST {url} {r.status_code}\")\n</code></pre>"},{"location":"Sources/Client/Responder/constants/","title":"Constants","text":""},{"location":"Sources/Client/Responder/constants/#Client.Responder.constants.API_DOMAIN","title":"<code>API_DOMAIN = 'http://localhost:8000'</code>  <code>module-attribute</code>","text":"<pre><code># get parent of current folder as root\nROOT_PATH = os.path.dirname(os.path.abspath(__file__))\nDATA_DIR = Path(ROOT_PATH) / \"data\"\n\nDATA_DIR.mkdir(parents=True, exist_ok=True)\n\nAPI_DOMAIN = \"http://localhost:8000\"\n</code></pre>"},{"location":"Sources/Client/Responder/play_speech/","title":"PlaySpeech","text":""},{"location":"Sources/Client/Responder/play_speech/#Client.Responder.play_speech.PlaySpeech","title":"<code>PlaySpeech</code>","text":"Source code in <code>Client/Responder/play_speech.py</code> <pre><code>class PlaySpeech:\n    @staticmethod\n    def text_to_speech_and_play(content: str):\n        \"\"\"\n        Convert text to speech and play\n        Args:\n            content (str): The content to be converted to speech\n\n        Returns:\n\n        \"\"\"\n        # Convert text to speech\n        with timer(logger, \"Text to speech\"):\n            tts = gTTS(text=content, lang=\"en\")\n        mp3_fp = io.BytesIO()\n        tts.write_to_fp(mp3_fp)\n        mp3_fp.seek(0)\n        with timer(logger, \"Load audio\"):\n            # Load the audio into pydub\n            audio = AudioSegment.from_file(mp3_fp, format=\"mp3\")\n\n        with timer(logger, \"Play audio\"):\n            # Play the audio\n            play(audio)\n\n    @staticmethod\n    def play_audio_url(url: str):\n        \"\"\"\n        Play audio file from the given\n        Args:\n            url (str): The URL of the audio file\n\n        Returns:\n\n        \"\"\"\n        response = requests.get(url)\n        response.raise_for_status()  # This will raise an exception for HTTP errors\n        with NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_file:\n            temp_file.write(response.content)\n            temp_file.flush()  # Make sure all data is written to the file\n\n            # Load the audio into pydub\n            audio = AudioSegment.from_file(temp_file.name, format=\"mp3\")\n\n            # Play the audio\n            play(audio)\n\n    @staticmethod\n    def play_audio_file(file_path: Path):\n        \"\"\"\n        Play audio file from the given\n        Args:\n            file_path (Path): The path of the audio file\n\n        Returns:\n\n        \"\"\"\n        # Load the audio into pydub\n        audio = AudioSegment.from_file(file_path, format=\"mp3\")\n\n        # Play the audio\n        play(audio)\n</code></pre>"},{"location":"Sources/Client/Responder/play_speech/#Client.Responder.play_speech.PlaySpeech.play_audio_file","title":"<code>play_audio_file(file_path)</code>  <code>staticmethod</code>","text":"<p>Play audio file from the given Args:     file_path (Path): The path of the audio file</p> <p>Returns:</p> Source code in <code>Client/Responder/play_speech.py</code> <pre><code>@staticmethod\ndef play_audio_file(file_path: Path):\n    \"\"\"\n    Play audio file from the given\n    Args:\n        file_path (Path): The path of the audio file\n\n    Returns:\n\n    \"\"\"\n    # Load the audio into pydub\n    audio = AudioSegment.from_file(file_path, format=\"mp3\")\n\n    # Play the audio\n    play(audio)\n</code></pre>"},{"location":"Sources/Client/Responder/play_speech/#Client.Responder.play_speech.PlaySpeech.play_audio_url","title":"<code>play_audio_url(url)</code>  <code>staticmethod</code>","text":"<p>Play audio file from the given Args:     url (str): The URL of the audio file</p> <p>Returns:</p> Source code in <code>Client/Responder/play_speech.py</code> <pre><code>@staticmethod\ndef play_audio_url(url: str):\n    \"\"\"\n    Play audio file from the given\n    Args:\n        url (str): The URL of the audio file\n\n    Returns:\n\n    \"\"\"\n    response = requests.get(url)\n    response.raise_for_status()  # This will raise an exception for HTTP errors\n    with NamedTemporaryFile(delete=True, suffix=\".mp3\") as temp_file:\n        temp_file.write(response.content)\n        temp_file.flush()  # Make sure all data is written to the file\n\n        # Load the audio into pydub\n        audio = AudioSegment.from_file(temp_file.name, format=\"mp3\")\n\n        # Play the audio\n        play(audio)\n</code></pre>"},{"location":"Sources/Client/Responder/play_speech/#Client.Responder.play_speech.PlaySpeech.text_to_speech_and_play","title":"<code>text_to_speech_and_play(content)</code>  <code>staticmethod</code>","text":"<p>Convert text to speech and play Args:     content (str): The content to be converted to speech</p> <p>Returns:</p> Source code in <code>Client/Responder/play_speech.py</code> <pre><code>@staticmethod\ndef text_to_speech_and_play(content: str):\n    \"\"\"\n    Convert text to speech and play\n    Args:\n        content (str): The content to be converted to speech\n\n    Returns:\n\n    \"\"\"\n    # Convert text to speech\n    with timer(logger, \"Text to speech\"):\n        tts = gTTS(text=content, lang=\"en\")\n    mp3_fp = io.BytesIO()\n    tts.write_to_fp(mp3_fp)\n    mp3_fp.seek(0)\n    with timer(logger, \"Load audio\"):\n        # Load the audio into pydub\n        audio = AudioSegment.from_file(mp3_fp, format=\"mp3\")\n\n    with timer(logger, \"Play audio\"):\n        # Play the audio\n        play(audio)\n</code></pre>"},{"location":"Sources/Client/Responder/setup/","title":"Setup","text":""},{"location":"Sources/Client/Responder/utils/","title":"Utils","text":""},{"location":"Sources/Client/Responder/utils/#Client.Responder.utils.timer","title":"<code>timer</code>","text":"<p>util function used to log the time taken by a part of program</p> Source code in <code>Client/Responder/utils.py</code> <pre><code>class timer:\n    \"\"\"\n    util function used to log the time taken by a part of program\n    \"\"\"\n\n    def __init__(self, logger: Logger, message: str):\n        \"\"\"\n        init the timer\n\n        Args:\n            logger (Logger): the logger\n            message (str): the message to be logged\n        \"\"\"\n        self.message = message\n        self.logger = logger\n        self.start = 0\n        self.duration = 0\n        self.sub_timers = []\n\n    def __enter__(self):\n        \"\"\"\n        context enter to start write this\n        \"\"\"\n        self.start = time.time()\n        self.logger.info(\"Starting %s\" % self.message)\n        return self\n\n    def __exit__(\n        self,\n        context: Optional[Type[BaseException]],\n        value: Optional[BaseException],\n        traceback: Optional[TracebackType],\n    ):\n        \"\"\"\n        context exit will write this\n\n        Args:\n            context (Optional[Type[BaseException]]): the context\n            value (Optional[BaseException]): the value\n            traceback (Optional[TracebackType]): the traceback\n        \"\"\"\n        self.duration = time.time() - self.start\n        self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/Client/Responder/utils/#Client.Responder.utils.timer.__enter__","title":"<code>__enter__()</code>","text":"<p>context enter to start write this</p> Source code in <code>Client/Responder/utils.py</code> <pre><code>def __enter__(self):\n    \"\"\"\n    context enter to start write this\n    \"\"\"\n    self.start = time.time()\n    self.logger.info(\"Starting %s\" % self.message)\n    return self\n</code></pre>"},{"location":"Sources/Client/Responder/utils/#Client.Responder.utils.timer.__exit__","title":"<code>__exit__(context, value, traceback)</code>","text":"<p>context exit will write this</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[Type[BaseException]]</code> <p>the context</p> required <code>value</code> <code>Optional[BaseException]</code> <p>the value</p> required <code>traceback</code> <code>Optional[TracebackType]</code> <p>the traceback</p> required Source code in <code>Client/Responder/utils.py</code> <pre><code>def __exit__(\n    self,\n    context: Optional[Type[BaseException]],\n    value: Optional[BaseException],\n    traceback: Optional[TracebackType],\n):\n    \"\"\"\n    context exit will write this\n\n    Args:\n        context (Optional[Type[BaseException]]): the context\n        value (Optional[BaseException]): the value\n        traceback (Optional[TracebackType]): the traceback\n    \"\"\"\n    self.duration = time.time() - self.start\n    self.logger.info(f\"Finished {self.message}, that took {self.duration:.3f}\")\n</code></pre>"},{"location":"Sources/Client/Responder/utils/#Client.Responder.utils.timer.__init__","title":"<code>__init__(logger, message)</code>","text":"<p>init the timer</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>the logger</p> required <code>message</code> <code>str</code> <p>the message to be logged</p> required Source code in <code>Client/Responder/utils.py</code> <pre><code>def __init__(self, logger: Logger, message: str):\n    \"\"\"\n    init the timer\n\n    Args:\n        logger (Logger): the logger\n        message (str): the message to be logged\n    \"\"\"\n    self.message = message\n    self.logger = logger\n    self.start = 0\n    self.duration = 0\n    self.sub_timers = []\n</code></pre>"},{"location":"Sources/Client/Responder/utils/#Client.Responder.utils.get_logger","title":"<code>get_logger(logger_name=None, stream=True)</code>","text":"<p>init the logger, give it proper format, log them both in terminal stream and file</p> <p>Parameters:</p> Name Type Description Default <code>logger_name</code> <code>Optional[str]</code> <p>the logger name</p> <code>None</code> <code>stream</code> <code>bool</code> <p>whether to log in the terminal stream</p> <code>True</code> Source code in <code>Client/Responder/utils.py</code> <pre><code>def get_logger(logger_name: Optional[str] = None, stream: bool = True):\n    \"\"\"\n    init the logger, give it proper format, log them both in terminal stream and file\n\n    Args:\n        logger_name (Optional[str]): the logger name\n        stream (bool): whether to log in the terminal stream\n    \"\"\"\n    logging.basicConfig(\n        format=\"%(name)s: %(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n        datefmt=\"%Y-%m-%d:%H:%M:%S\",\n        level=logging.INFO,\n    )\n\n    logger = logging.getLogger(logger_name)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n    formatter = logging.Formatter(\n        \"CLIENT: %(name)s | %(asctime)s,%(msecs)d %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s\",\n    )\n    if not logger.hasHandlers() and stream:\n        stdout_handler = logging.StreamHandler()\n        stdout_handler.setFormatter(formatter)\n        stdout_handler.setLevel(logging.INFO)\n        logger.addHandler(stdout_handler)\n\n    return logger\n</code></pre>"},{"location":"Tutorial/","title":"Table of Contents","text":"<p>We will present setup and run the end to end pipeline.</p> <p>Mainly will include these sections:</p> <ul> <li>Setup and run the pipeline successfully</li> <li>Evaluation and Annotation Benchmark</li> <li>Pipeline customisation</li> <li>Annotation customisation</li> <li>Case Study</li> <li>Video Demo</li> </ul>"},{"location":"Tutorial/annotation_customisation/","title":"Annotation customisation","text":""},{"location":"Tutorial/annotation_customisation/#explanation","title":"Explanation","text":"<p>The annotation is built upon the Django Admin interface, which is a powerful tool to manage the data and easy to do the customisation.</p> <p>This is one of the reason why we choose Django as the backend framework, as there are heaps of documentations, tools, packages if you need something more here.</p>"},{"location":"Tutorial/annotation_customisation/#conversation-annotation","title":"Conversation Annotation","text":"<p>It is built upon the Django <code>change_list</code> template, the code to make this happen is in <code>API/hardware/admin.py</code> and <code>API/hardware/forms.py</code></p> <p>The class <code>DataMultiModalConversationAdmin</code> is where we implement it.</p> <p>First we will look at the conversation model to see where we store the annotation data:</p> <pre><code># API/hardware/models.py\n\nclass DataMultiModalConversation(models.Model):\n    # ...\n    annotations = models.JSONField(\n        help_text=\"The annotations of the emotion detection\",\n        null=True,\n        blank=True,\n        default=dict,\n    )\n\n    multi_turns_annotations = models.JSONField(\n        help_text=\"The annotations of the multi-turns\",\n        null=True,\n        blank=True,\n        default=dict,\n    )\n    tags = TaggableManager(blank=True)\n</code></pre> <p>Which means, the annotation will be saved to these two json field: <code>annotations</code> and <code>multi_turns_annotations</code>.</p> <p>The saved json will be in following schema:</p> <pre><code>{\n  \"1\": {\n    // 1 is the user id\n    \"annotation_speech2text\": \"your annotation\",\n    \"annotation_speech2text_score\": 3\n    // from 0-5\n    // ...\n  }\n}\n</code></pre> <p>The annotation field is in</p> <pre><code># in API/hardware/forms.py\nclass MultiModalAnnotationForm(forms.ModelForm):\n    annotation_speech2text = forms.CharField(\n        required=False,\n        widget=forms.Textarea(attrs={\"rows\": 1}),\n        help_text=\"Please provide your annotation for the speech-to-text task.\",\n    )\n    annotation_speech2text_score = forms.IntegerField(\n        initial=0,\n        widget=forms.NumberInput(attrs={\"min\": 0, \"max\": 5}),\n        required=False,\n        help_text=\"Score for the speech-to-text results, score from 0 to 5.\",\n    )\n    annotation_text_generation = forms.CharField(\n        required=False,\n        widget=forms.Textarea(attrs={\"rows\": 1}),\n        help_text=\"Please provide your annotation for the text generation task.\",\n    )\n\n    annotation_text_generation_score = forms.IntegerField(\n        initial=0,\n        widget=forms.NumberInput(attrs={\"min\": 0, \"max\": 5}),\n        required=False,\n        help_text=\"Score for the text generation results, score from 0 to 5.\",\n    )\n\n    annotation_text2speech_score = forms.IntegerField(\n        initial=0,\n        widget=forms.NumberInput(attrs={\"min\": 0, \"max\": 5}),\n        required=False,\n        help_text=\"Score for the text-to-speech results, score from 0 to 5.\",\n    )\n\n    annotation_overall_score = forms.IntegerField(\n        initial=0,\n        widget=forms.NumberInput(attrs={\"min\": 0, \"max\": 5}),\n        required=False,\n        help_text=\"Overall score for this multi-modal task, score from 0 to 5.\",\n    )\n\n    annotation_overall_comment = forms.CharField(\n        required=False,\n        widget=forms.Textarea(attrs={\"rows\": 1}),\n        help_text=\"Please provide your overall annotation for this multi-modal task.\",\n    )\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if self.instance.annotations:\n            current_user_annotation = self.instance.annotations.get(\n                str(self.current_user.id), {}\n            )\n            for key, value in current_user_annotation.items():\n                if key in self.fields:\n                    self.fields[key].initial = value\n        if self.instance.multi_turns_annotations:\n            current_user_annotation = self.instance.multi_turns_annotations.get(\n                str(self.current_user.id), {}\n            )\n            for key, value in current_user_annotation.items():\n                if key in self.fields:\n                    self.fields[key].initial = value\n</code></pre> <p>These are the fields show up in the change_list page, as show below:</p> <p></p> <p>The code inside the <code>__init__</code> function is in charge of making sure present the data you already annotated to you.</p> <p>The annotation benchmark and details are analysing the data in the <code>annotations</code> and <code>multi_turns_annotations</code> field.</p> <p>If you want to add a customised annotation field, all you need to do is to add a field in the <code>MultiModalAnnotationForm</code>.</p>"},{"location":"Tutorial/annotation_customisation/#customised-component-annotation","title":"Customised Component Annotation","text":"<p>For example, we have an emotion detection task, which will not fit into the conversation model to do the annotation, it is more like an intermediate task and output for the whole pipeline, however, the quality of it still very important.</p> <p>So we also want to be able to annotate these types of tasks.</p> <p>Especially during the process to do the application development, we may introduce specific specialised tasks for different purposes.</p> <p>In general, to annotate these types of tasks, the context is still the input and output of the conversation, so when we design the model, using emotion detection as an example, we use a FK point to the Conversation model.</p> <pre><code>class ContextEmotionDetection(models.Model):\n    multi_modal_conversation = models.ForeignKey(\n        DataMultiModalConversation,\n        on_delete=models.CASCADE,\n        related_name=\"emotion_detection\",\n        null=True,\n        blank=True,\n    )\n    result = models.JSONField(\n        help_text=\"The emotion result of the text\", null=True, blank=True, default=dict\n    )\n    logs = models.JSONField(\n        help_text=\"The logs of the emotion detection\",\n        null=True,\n        blank=True,\n        default=dict,\n    )\n    created_at = models.DateTimeField(\n        auto_now_add=True, help_text=\"The created time of the emotion detection\"\n    )\n    updated_at = models.DateTimeField(\n        auto_now=True, help_text=\"The updated time of the emotion detection\"\n    )\n\n    annotations = models.JSONField(\n        help_text=\"The annotations of the emotion detection\",\n        null=True,\n        blank=True,\n        default=dict,\n    )\n\n    class Meta:\n        verbose_name = \"Context Emotion\"\n        verbose_name_plural = \"Context Emotions\"\n</code></pre> <p>As the conversation model, we also have the <code>annotations</code> field to store the annotation data.</p> <p>The schema of the annotation data is the same as the conversation model.</p> <p>The annotation form is in <code>API/hardware/forms.py</code>:</p> <pre><code>\n\nclass MultiModalFKEmotionDetectionAnnotationForm(forms.ModelForm):\n    annotation_overall = forms.IntegerField(\n        initial=0,\n        help_text=\"Overall score for this emotion detection task, score from 0 to 5.\",\n    )\n    annotation_overall.widget.attrs.update({\"min\": 0, \"max\": 5})\n\n    annotation_text_modality = forms.IntegerField(\n        initial=0, help_text=\"Score for text modality.\"\n    )\n    annotation_text_modality.widget.attrs.update({\"min\": 0, \"max\": 5})\n\n    annotation_audio_modality = forms.IntegerField(\n        initial=0, help_text=\"Score for audio modality.\"\n    )\n    annotation_audio_modality.widget.attrs.update({\"min\": 0, \"max\": 5})\n\n    annotation_video_modality = forms.IntegerField(\n        initial=0, help_text=\"Score for video modality.\"\n    )\n    annotation_video_modality.widget.attrs.update({\"min\": 0, \"max\": 5})\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        if self.instance.annotations:\n            current_user_annotation = self.instance.annotations.get(\n                str(self.current_user.id), {}\n            )\n            for key, value in current_user_annotation.items():\n                if key in self.fields:\n                    self.fields[key].initial = value\n</code></pre> <p>The form is similar to the conversation annotation form, but the fields are different.</p> <p>So if you want to implement one by yourself, all you need to do is copying this for your model.</p>"},{"location":"Tutorial/benchmark_and_annotation/","title":"Evaluation and Annotation Benchmark","text":"<p>So as we mentioned before, we will have two perspectives to evaluate the performance of the pipeline.</p> <ul> <li>Latency</li> <li>Accuracy</li> </ul>"},{"location":"Tutorial/benchmark_and_annotation/#latency","title":"Latency","text":"<p>For the latency part, if you log the time point and duration of each task within the Agent module, you should be able to automatically get the latency of each round of conversation. And the results will be presented in two ways:</p> <ul> <li>Summary Latency Benchmark</li> <li>Detailed Latency Benchmark</li> </ul> <p>The above figure is the Detailed Latency Benchmark, which will show the latency of each round of conversation.</p> <p>The below figure is the Summary Latency Benchmark, which will show the summary statistics of the latency.</p> <p></p>"},{"location":"Tutorial/benchmark_and_annotation/#accuracy","title":"Accuracy","text":"<p>For the accuracy part, some of the metrics can be automatically calculated, such as WER for Speech2Text. However, currently, most of the metrics will need human annotation. And research about how to get this to be automated is still ongoing and worth investigating.</p> <p>So to solve the problem, we build the annotation functionality for the accuracy part.</p>"},{"location":"Tutorial/benchmark_and_annotation/#annotation-system","title":"Annotation System","text":"<p>Conversation Annotation</p> <p>We have a table(Model concept in Django) called conversation to record each round of conversation.</p> <p>The conversation will be associated with the input</p> <ul> <li>Data Audio</li> <li>Data Video (associated with the Data Audio)</li> <li>Data Text (Converted by Speech2Text Task)</li> </ul> <p>and the output</p> <ul> <li>Response Text (Generated by the Agent module, Optional)</li> <li>Generated Audio (Generated by the Text2Speech or directly module, Optional)</li> </ul> <p>And the annotation will be based on the input and output.</p> <p>As shown in this figure:</p> <p></p> <p>Initially, the evaluation measurement metrics we built in includes a score [0,5], 5 means the response is perfect, 0 means the response is totally wrong. In this way, we can calculate a quantitative score for the performance of each component within pipeline.</p> <p>So for each conversation, you can annotate</p> <ul> <li>Speech2Text Score: Whether it is perfect, excellent, good, fair, poor, or bad</li> <li>Give Speech2Text correct text</li> <li>Text Generation Score: Same as the score above, evaluate the response text</li> <li>Give Text Generation proper text</li> <li>Text2Speech Score: Same as the score above, evaluate the generated audio</li> <li>Overall Score: Overall score for the conversation</li> <li>Comments: Any comments you want to add, which will be also shown in the benchmark page</li> </ul> <p>One conversation can be annotated by multiple people, and the final score will be the average of all the scores.</p> <p>The below figure shows the default annotation score:</p> <p></p> <p>And the overview of the annotation for a conversation:</p> <p></p> <p>The annotated details will be shown in the bottom.</p> <p>Customised Component Annotation</p> <p>And for a specific component within the pipeline, which will not fit in the conversation table above, we will have a separate table to record the annotation. For example, the emotion detection will be a customized task we defined and developed, so we will have a separate table to record the annotation.</p> <p>Compared to the above setup, the context part(input and output) will be the same, the annotation measurement metrics will be different:</p> <p></p> <p>Multi-turn Conversation Annotation</p> <p>The conversations actually is mulit-turn, which means we also want to be able to annotate multiple turns conversation.</p> <p>This is also supported.</p> <p>You can assign a \"Tag\" to a group of conversations, then the last conversation within the group will have an extra annotated field called \"Multi turn annotation overall score and comment\".</p> <p></p> <p>During and after the annotation process, you can track the progress by the Accuracy|Detail page. For example:</p> <p></p> <p>After all annotation is done, you can view the summary of the accuracy by the Accuracy|Benchmark page.</p> <p></p> <p>And your multi turn conversation results can be checked with Accuracy|Multi-Turn Conversation page</p> <p></p> <p>In summary, for the evaluation benchmark, latency can be automatically calculated, and accuracy will need human annotation. Our tool can help the advancement of the auto or semi-auto evaluation accuracy metrics development by collecting this kind of data.</p>"},{"location":"Tutorial/case_study/","title":"Case Study","text":"<p>After the system is set up, we tested the pipeline with two cases:</p> <ul> <li>US Election Debate:<ul> <li>In this case, we download the video, and then sample the video into segments, which will be used as video and   audio input for the pipeline.</li> <li>This way can solve one of the problem that the multimodal conversational Agent research is lacking of the dataset.</li> </ul> </li> <li>Assist the visually impaired:<ul> <li>This is in the real world scenario, where we will use the pipeline to assist the visually impaired people when   they   are indoors.</li> <li>It is not that critical for the latency, so potential can be applied even in current latency situation, because   the Agent accuracy is quite high now.</li> </ul> </li> </ul>"},{"location":"Tutorial/case_study/#us-election-debate","title":"US Election Debate","text":"<p>One of the intensive conversational scenarios is the debate. We extracted segments from the US Presidential Debate 2024 between Biden and Trump, focusing on Biden addressing the public and handling questions. These segments were fed into our pipeline to evaluate its performance under different configurations:</p> <ul> <li>OpenAI's Whisper for speech-to-text, GPT-4o vision model, and text-to-speech (GPT4O_ETE);</li> <li>a locally deployed quantization LLM with Whisper, text-to-speech, and our emotion detection model for video (   QuantizationLLM_ETE)</li> <li>replacing the quantization LLM with Hugging Face LLM for inference (HF_ETE)</li> <li>and a version using only Whisper, GPT-3.5, and text-to-speech, ignoring video modality (GPT35_ETE).</li> </ul> <p>We ran the Agent modules on a NVIDIA-3080 GPU with 12GB memory.</p> <p>To replicate what we are doing, you can download the video from link</p> <p>Put in under the folder <code>Client/Listener/data/mock/US-Election-2024.mp4</code></p> <p>And then you can run the following command:</p> <pre><code>cd ./Client/Listener\nsource venv/bin/activate\n python3 -m mock.data_extraction --input_video_path your/repo/path/US-Election-2024.mp4 --api_domain  https://openomni.ai4wa.com --token your_token --time_points 02:53,3:20,20:20,20:39,33:38,34:18,55:15,55:40,80:05,80:18\n</code></pre> <p>You need to run the rest modules, make the running properly</p> <p>And all the pipelines we have set up will be fired, all you need to do is to wait for the results.</p>"},{"location":"Tutorial/case_study/#results","title":"Results","text":"<p>After annotation, the accuracy performance is here:</p> <p></p> <p>The fastest configuration is GPT35_ETE, averaging around 15 seconds, with most of the time consumed by the text-to-speech part, as the generated content is quite long and comprehensive. The slowest configuration is HF_ETE, taking around 189 seconds, with the LLM model inference step taking the longest time. QuantizationLLM_ETE takes around 60 seconds, with LLM model inference taking around 28 seconds and our emotion detection model taking around 10 seconds.</p> <p>After annotation with our provided interface, the accuracy statistics are automatically generated. As shown in Figure~ \\ref{fig:gpt4oaccuracy}, speech-to-text accuracy is good, while text-to-speech can be improved with more natural emotion or personality. The text generation, however, is often too general and sometimes inappropriate. Biden's responses are more in-context and supported by evidence. The only question where our pipeline performed well was the subjective question about Biden's age, where the GPT-4o pipeline excelled.</p> <p>The GPT35_ETE pipeline had the best overall accuracy, but its responses were often in-context yet pompous. Thus, Biden still outperforms AI.</p> <p>In conclusion, AI cannot be the President of the US for now, from both latency and accuracy perspectives.</p>"},{"location":"Tutorial/case_study/#help-visual-impaired-people-when-indoors","title":"Help visual impaired people when indoors","text":"<p>While latency and the need for external information currently prevent AI from being the President of the US, the current state of conversational Agent can be production-ready and useful for areas that are not latency-critical and do not require extensive external knowledge. Assisting indoor activities for the visually impaired can be one such potential application area.</p> <p>We set up a camera, microphone, and speaker, and then prepared several types of questions useful for the visually impaired population. These included questions about the location of specific objects and how to grab them, navigating to another position indoors, and inquiries about the surrounding environment. We sampled six questions and fed them to the GPT4O_ETE pipeline. The latency statistics show that each conversational request from the user is responded to within approximately 30 seconds, which is expected under the hardware setup.</p> <p>After annotation with context, the accuracy performance is impressive, with an overall score of 4.7/5. Most responses are accurate; however, the LLM lacks specific skills for assisting the visually impaired. For example, when asked where the keyboard is, the response could include more instructive steps on how to grab it rather than a general description. This indicates that while the conversational Agent is nearly production-ready for assisting the visually impaired with indoor activities, improvements in latency and response content are still needed.</p>"},{"location":"Tutorial/pipeline_customisation/","title":"Pipeline customisation","text":""},{"location":"Tutorial/pipeline_customisation/#explanation","title":"Explanation","text":"<p>We have provided a list of built-in Pipelines for demonstration and evaluation purpose.</p> <p>For example, we got:</p> <ul> <li>CLUSTER_Q_ETE_CONVERSATION:<ul> <li>Speech2Text with local Whisper</li> <li>Emotion Detection</li> <li>Quantization Local LLM</li> <li>Text2Speech</li> </ul> </li> <li>CLUSTER_Q_NO_EMOTION_ETE_CONVERSATION_NAME:<ul> <li>Speech2Text with local Whisper</li> <li>Emotion Detection</li> <li>Quantization Local LLM</li> <li>Text2Speech</li> </ul> </li> <li>CLUSTER_HF_ETE_CONVERSATION:<ul> <li>Speech2Text with local Whisper</li> <li>Emotion Detection</li> <li>HuggingFace Local LLM</li> <li>Text2Speech</li> </ul> </li> <li>CLUSTER_GPT_4O_ETE_CONVERSATION:<ul> <li>Speech2Text OpenAI API</li> <li>GPT-4o with image and text</li> <li>Text2Speech</li> </ul> </li> <li>CLUSTER_GPT_4O_TEXT_ETE_CONVERSATION:<ul> <li>Speech2Text OpenAI API</li> <li>GPT-4o with text only</li> <li>Text2Speech</li> </ul> </li> <li>CLUSTER_GPT_35_ETE_CONVERSATION:<ul> <li>Speech2Text OpenAI API</li> <li>GPT-3.5 with text</li> <li>Text2Speech</li> </ul> </li> <li>CLUSTER_GPT_35_RAG_ETE_CONVERSATION:<ul> <li>Speech2Text OpenAI API</li> <li>GPT-3.5 with text</li> <li>RAG</li> <li>Text2Speech</li> </ul> </li> </ul> <p>After the evaluation, we found out, for all the pipelines, under the Nvidia 3080 GPU, none of the latency is acceptable. The best performance is the GPT-3.5 pipeline with text only as input, which has a latency of around 8-10 seconds. For the GPT-4o, the API latency is around 3-8 seconds, when you feed more images data in, the latency will increase significantly.</p> <p>So if you have an idea, and a solution, you want to test out whether it is acceptable, how should you do that?</p> <p>First go to the code place: <code>API/orchestrator/chian/clusters.py</code></p> <p>This is the places we put all the pipeline configurations, as shown above.</p> <p>Here is an example of the pipeline configuration:</p> <pre><code>\n\"\"\"\nCluster for gpt3.5 model and gpt3.5 with RAG\n\"\"\"\nCLUSTER_GPT_35_RAG_ETE_CONVERSATION_NAME = \"CLUSTER_GPT_35_RAG_ETE_CONVERSATION\"\nCLUSTER_GPT_35_RAG_ETE_CONVERSATION = {\n    \"openai_speech2text\": {\n        \"order\": 0,\n        \"extra_params\": {},\n        \"component_type\": \"task\",\n        \"task_name\": \"openai_speech2text\",\n    },\n    \"completed_openai_speech2text\": {\n        \"order\": 1,\n        \"extra_params\": {},\n        \"component_type\": \"signal\",\n        \"task_name\": None,\n    },\n    \"created_data_text\": {\n        \"order\": 2,\n        \"extra_params\": {},\n        \"component_type\": \"signal\",\n        \"task_name\": None,\n    },\n    \"completed_rag\": {\n        \"order\": 3,\n        \"extra_params\": {},\n        \"component_type\": \"task\",\n        \"task_name\": \"rag\",\n    },\n    \"completed_openai_gpt_35\": {\n        \"order\": 4,\n        \"extra_params\": {\n            \"prompt_template\": \"\"\"{text}\"\"\",\n        },\n        \"component_type\": \"task\",\n        \"task_name\": \"openai_gpt_35\",\n    },\n    \"completed_openai_text2speech\": {\n        \"order\": 5,\n        \"extra_params\": {},\n        \"component_type\": \"task\",\n        \"task_name\": \"openai_text2speech\",\n    },\n}\n</code></pre> <p>First, we need to define a cluster name, which is the pipeline. This cluster name will be the one when you started your audio acquisition, you can specify which cluster you want to use by the <code>--track_cluster</code></p> <p>When you stop talk and the audio acquisition will send the audio you spoke to the API with a track_id, which is in the format of <code>T-{cluster_name}-{uid_for_this_conversation}</code>. Like: <code>T-CLUSTER_GPT_35_RAG_ETE_CONVERSATION-f6bf3b78e4f5484abf949790c8451856</code>.</p> <p>API side will base on the cluster_name to trigger the relevant pipeline and tasks, and all the downstream task for this conversation within the pipeline will be grouped with this track_id to ensure the pipeline observability.</p> <p>We have a table called <code>Task</code> to manage all the different types of tasks, this can be decomposed to a queue system if we want to bring this into production for more complex design. Currently, to maintain a simple and flexible design, every Agent task will be recorded inside the <code>Task</code> table, and we will base on this table to analyse the progress of the pipeline, health of the system.</p> <p></p> <p>For example, with the track_id above, the example pipeline will be triggered.</p> <p>First, it will go to create a task, which name will be <code>openai_speech2text</code>, and status will be <code>pending</code> with proper parameters.</p> <p>Agent consumer will consume this task, and after the task is done, it will update this task record with the status <code>completed</code>. And the metadata generated during the Agent module running process will be saved in the <code>result_json</code> field, with two primary key</p> <ul> <li>result_profile: this will store the results we expect for this task, like the generated text</li> <li>latency_profile: this will store the time point information for critical time points and duration information for both   model inference and data transfer.</li> </ul> <p>It will be like this:</p> <p></p> <p>When Agent module call API endpoint to update the task status, it will trigger a <code>completed_task</code> Signal ( check Django Signal for further details), which is acting as the <code>Router</code> to dispatch different following tasks.</p> <p>The specific code to implement this is in <code>API/orchestrator/models.py, line 114-119</code></p> <pre><code>def save(self, *args, **kwargs):\n    # if it is updated, then we need to call the chain\n    if self.result_status == \"completed\":\n        completed_task.send(sender=self, data=self.__dict__)\n    super().save(*args, **kwargs)\n</code></pre> <p>This will override the Django Model save function for <code>Task</code>, when the result_status is changing to completed, it will trigger the <code>completed_task</code> signal.</p> <p>The <code>completed_task</code> signal is defined in <code>API/orchestrator/signals.py</code></p> <pre><code>from django.dispatch import Signal\n\ncompleted_task = Signal()  # task itself\n</code></pre> <p>The <code>receiver</code> of this signal is defined in <code>API/orchestrator/chain/complted_task.py</code></p> <pre><code>from django.dispatch import receiver\n\nfrom authenticate.utils.get_logger import get_logger\nfrom orchestrator.chain.models import TaskData\nfrom orchestrator.chain.signals import (\n    completed_emotion_detection,\n    completed_hf_llm,\n    completed_openai_gpt_4o_text_and_image,\n    completed_openai_gpt_4o_text_only,\n    completed_openai_gpt_35,\n    completed_openai_speech2text,\n    completed_openai_text2speech,\n    completed_quantization_llm,\n    completed_rag,\n    completed_speech2text,\n    completed_task,\n    completed_text2speech,\n)\nfrom orchestrator.models import Task\n\nlogger = get_logger(__name__)\n\n\n@receiver(completed_task)\ndef trigger_completed_task(sender, **kwargs):\n    \"\"\"\n    Trigger the multi-modal emotion detection.\n    \"\"\"\n    data = kwargs.get(\"data\", {})\n    task_data = TaskData(**data)\n\n    if task_data.task_name == \"speech2text\":\n        return completed_speech2text.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"emotion_detection\":\n        return completed_emotion_detection.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"quantization_llm\":\n        return completed_quantization_llm.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"text2speech\":\n        logger.info(\"Text2Speech task completed\")\n        return completed_text2speech.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"hf_llm\":\n        logger.info(\"HF LLM task completed\")\n        return completed_hf_llm.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"openai_speech2text\":\n        logger.info(\"OpenAI Speech2Text task completed\")\n        return completed_openai_speech2text.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"openai_gpt_4o_text_and_image\":\n        logger.info(\"OpenAI GPT4O task completed\")\n        return completed_openai_gpt_4o_text_and_image.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n    if task_data.task_name == \"openai_gpt_35\":\n        logger.info(\"OpenAI GPT3.5 task completed\")\n        return completed_openai_gpt_35.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    if task_data.task_name == \"openai_gpt_4o_text_only\":\n        logger.info(\"OpenAI GPT4O Text Only task completed\")\n        return completed_openai_gpt_4o_text_only.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n    if task_data.task_name == \"rag\":\n        logger.info(\"RAG task completed\")\n        return completed_rag.send(sender=sender, data=data, track_id=task_data.track_id)\n\n    if task_data.task_name == \"openai_text2speech\":\n        logger.info(\"OpenAI Text2Speech task completed\")\n        return completed_openai_text2speech.send(\n            sender=sender, data=data, track_id=task_data.track_id\n        )\n\n    task_name_choices = Task.get_task_name_choices()\n    task_name_choices_list = [task[0] for task in task_name_choices]\n    if task_data.task_name not in task_name_choices_list:\n        logger.error(\"Task name not found is not in the choices list\")\n        return\n    logger.critical(f\"{task_data.task_name} task completed, however, no action taken.\")\n</code></pre> <p>We can see from the code, what it is doing is to use the track_id to match the cluster name, and then base on the configuration of this cluster, identify the next component within the cluster(pipeline).</p> <p>For example, the steps will be like:</p> <ul> <li>when <code>openai_speech2text</code> task is finished</li> <li><code>completed_task</code> is then triggered</li> <li>It will base on the current task name trigger the downstream Signal, which will go to trigger   the <code>completed_openai_speech2text</code></li> <li><code>completed_openai_speech2text</code> receiver is in <code>API/orchestrator/chain/completed_openai_speech2text.py</code>, it will   process the results into <code>DataText</code> object, save it to the database.</li> <li>Then it will identify the current cluster based on track_id, and then identify the next component within the pipeline   based on current task name, which is <code>created_data_text</code>. The class <code>ClusterManager</code> will be in charge of this.</li> <li>If it is a signal component, the signal will be dispatch, and the receiver will take the input and do the next step.</li> <li>If it is a task component, it will create next task, with the <code>extra_params</code> added to the parameters, and then save   it to the database, the Agent module will listen to this, and consume it.</li> <li>The process will repeat like this until it reaches the end of the pipeline.</li> </ul> <p>ClusterManager code is in <code>API/orchestrator/chain/manager.py</code></p> <pre><code>\"\"\"\n\nHere will define a list of clusters\n\nEach cluster will have a list of chain components\n\nFor example, end-to-end conversation chain will have the following components:\n\n- completed_speech2text\n- created_data_text\n- completed_emotion_detection\n- completed_quantization_llm\n- completed_text2speech\n\"\"\"\n\nfrom typing import Optional, Tuple\n\nfrom authenticate.utils.get_logger import get_logger\nfrom orchestrator.chain.clusters import CLUSTERS\nfrom orchestrator.chain.signals import created_data_text\nfrom orchestrator.models import Task\n\nlogger = get_logger(__name__)\n\n\nclass ClusterManager:\n\n    @staticmethod\n    def get_cluster(cluster_name: str):\n        \"\"\"\n        Get the cluster\n\n        Args:\n            cluster_name (str): The cluster name\n        \"\"\"\n        if cluster_name in CLUSTERS:\n            return CLUSTERS[cluster_name]\n        return None\n\n    @staticmethod\n    def get_next_chain_component(\n            cluster: dict, current_component: str\n    ) -&gt; Tuple[Optional[str], Optional[dict]]:\n        \"\"\"\n        Get the next chain\n\n        Args:\n            cluster (dict): The cluster\n            current_component (str): The current component\n\n        Return:\n            Tuple[Optional[str], Optional[dict]]: The next component and its parameters if exists, otherwise None\n        \"\"\"\n        chain = []\n        for key, value in cluster.items():\n            chain.append(key)\n        chain.sort(key=lambda x: cluster[x][\"order\"])\n        if current_component == \"init\":\n            \"\"\"\n            If this is the start of the chain, then return the first component\n            \"\"\"\n            return chain[0], cluster[chain[0]]\n        # index of the current component\n        current_component_index = chain.index(current_component)\n        next_index = current_component_index + 1\n        if next_index &gt;= len(chain):\n            return None, None\n        return chain[next_index], cluster[chain[next_index]]\n\n    @classmethod\n    def get_next(cls, cluster_name: str, current_component: str):\n        \"\"\"\n        Get the next component\n\n        Args:\n            cluster_name (str): The cluster name\n            current_component (str): The current component\n        \"\"\"\n        cluster = cls.get_cluster(cluster_name)\n        if cluster is None:\n            return None\n        return ClusterManager.get_next_chain_component(cluster, current_component)\n\n    @classmethod\n    def chain_next(\n            cls,\n            track_id: Optional[str],\n            current_component: str,\n            next_component_params: dict,\n            name: str = None,\n            user=None,\n    ):\n        \"\"\"\n        Chain to the next component\n\n        Args:\n            current_component (str): The current component\n            track_id (str): The track ID\n            next_component_params (dict): The next component parameters\n            name (str): The task name, it will be used to aggregate the task\n            user (None): The user\n        \"\"\"\n        logger.info(f\"Current component: {current_component}\")\n        logger.info(f\"Next component params: {next_component_params}\")\n        cluster_name = track_id.split(\"-\")[1]\n        next_component_name, next_component = cls.get_next(\n            cluster_name, current_component\n        )\n        logger.info(f\"Next component: {next_component_name}\")\n\n        if next_component_name is None:\n            return\n        # do something with the next component\n        # It can be a task or a signal\n        next_parameters = {\n            **next_component_params,\n            **next_component.get(\"extra_params\", {}),\n        }\n        logger.info(next_parameters)\n        logger.info(next_component_name)\n\n        if next_component[\"component_type\"] == \"task\":\n            task = Task.create_task(\n                user=user,\n                name=name or next_component[\"task_name\"],\n                task_name=next_component[\"task_name\"],\n                parameters=next_parameters,\n                track_id=track_id,\n            )\n            logger.info(f\"Task {task.id} created for {next_component['task_name']}\")\n            return task.id\n        elif next_component[\"component_type\"] == \"signal\":\n            if next_component_name == \"created_data_text\":\n                created_data_text.send(\n                    sender=next_component_params.get(\"sender\"),\n                    data=next_component_params.get(\"data\"),\n                    track_id=track_id,\n                    user=user,\n                )\n        return None\n</code></pre>"},{"location":"Tutorial/pipeline_customisation/#demonstration","title":"Demonstration","text":""},{"location":"Tutorial/pipeline_customisation/#api-end","title":"API end","text":"<p>So if you want to customise the pipeline, you can add your own cluster configuration in <code>API/orchestrator/chian/clusters.py</code>.</p> <p>For example, if we want to add a cluster called <code>CLUSTER_VOICE_ETE_CONVERSATION</code>, which will first get the image to description text, and then feed to an end-to-end voice model with the audio, generate the output audio</p> <ul> <li>First it will do the image2text</li> <li>And then it will trigger the voice2voice model</li> </ul> <p>The configuration will be like this:</p> <pre><code>CLUSTER_VOICE_ETE_CONVERSATION_NAME = \"CLUSTER_VOICE_ETE_CONVERSATION\"\n\nCLUSTER_VOICE_ETE_CONVERSATION = {\n    \"image2text\": {\n        \"order\": 0,\n        \"extra_params\": {},\n        \"component_type\": \"task\",\n        \"task_name\": image2text,\n    },\n    \"completed_image2text\": {\n        \"order\": 1,\n        \"extra_params\": {},\n        \"component_type\": \"signal\",\n        \"task_name\": None,\n    },\n    \"completed_voice2voice\": {\n        \"order\": 2,\n        \"extra_params\": {},\n        \"component_type\": \"task\",\n        \"task_name\": \"voice2voice\",\n    },\n}\n\nCLUSTERS = {\n    # ...\n    CLUSTER_VOICE_ETE_CONVERSATION_NAME: CLUSTER_VOICE_ETE_CONVERSATION,\n}\n</code></pre> <p>Then add new added task_name to the task_name_choices in <code>API/orchestrator/models.py</code></p> <pre><code>\n@staticmethod\ndef get_task_name_choices():\n    \"\"\"\n    Get dynamic task name choices\n    Returns:\n        list: List of tuples containing task name choices\n    \"\"\"\n    # Here you can fetch the choices from an external source or database\n    return [\n        #  ...\n        (\"rag\", \"RAG\"),\n        (\"image2text\", \"Image2Text\"),\n        (\"voice2voice\", \"Voice2Voice\"),\n    ]\n\n\n@staticmethod\ndef task_ml_task_mapping() -&gt; dict:\n    return {\n        # ...\n        \"rag\": \"rag\",\n        \"image2text\": \"image2text\",\n        \"voice2voice\": \"voice2voice\",\n    }\n</code></pre> <p>This will make sure you can choose the two new added task when create a new <code>Task</code>.</p> <p>Next, you will need to create two new Signals:</p> <ul> <li>completed_image2text</li> <li>completed_voice2voice</li> </ul> <p>in <code>API/orchestrator/chain/signals.py</code></p> <pre><code>\nfrom django.dispatch import Signal\n\ncompleted_task = Signal()  # task itself\n# ....\ncompleted_image2text = Signal()\ncompleted_voice2voice = Signal()\n</code></pre> <p>Then create the two receiver to handle the two signals in <code>API/orchestrator/chain/completed_image2text.py</code> and <code>API/orchestrator/chain/completed_voice2voice.py</code></p> <p>Use other existing receiver as reference to implement the new receiver.</p> <p>Then you need to register the two signal in the <code>orchestrator/chain/apps.py</code></p> <pre><code>def ready(self):  # noqa\n    # Import signals\n    # ....\n    import orchestrator.chain.completed_image2text  # noqa\n    import orchestrator.chain.completed_voice2voice  # noqa\n</code></pre> <p>Until now, the API end is done for this newly added pipeline.</p>"},{"location":"Tutorial/pipeline_customisation/#agent-end","title":"Agent end","text":"<p>You will need to go to implement the <code>Agent</code> module to consume the new added pipeline, mainly is the added type of tasks.</p> <p>We have added two type of tasks, which means we will need to add two modules to handle this.</p> <p>So create a image2text module in <code>Agent/modules/image2text/__init__.py</code> and a voice2voice module in <code>Agent/modules/voice2voice/__init__.py</code></p> <p>You can then implement the code as you want within each of the respective folder, use other modules as reference to implement it.</p> <p>After that, you will need to register both task in the <code>main.py</code></p> <p>It will not be hard to add them in if you follow what we have done for others.</p>"},{"location":"Tutorial/pipeline_customisation/#test-out","title":"Test out","text":"<p>Then go to the client end, start the audio acquisition, and specify the cluster name to the newly added cluster name.</p> <p>Then start talking, and you will see the pipeline is triggered, and the Agent module will consume the task.</p> <p>If there is anything wrong, try to use the above explanation to debug the problem.</p>"},{"location":"Tutorial/setup/","title":"Setup and run the pipeline successfully","text":"<p>Deployment mode will be All in One Local Machine for demonstration purposes. This means all of your components will be running on your local machine or your PC. To get started, you will need a decent machine (as we will run some local LLMs) with camera, microphone and speaker, which most of the laptops have.</p> <p>And you will also need to have Python, Docker installed on your machine.</p> <p>Step 1: Clone the repository</p> <pre><code># switch to a proper directory\ngit clone git@github.com:AI4WA/OpenOmniFramework.git\n</code></pre> <p>Step 2: Get API running</p> <pre><code>cd ./OpenOmniFramework\ncd ./API\n# Run it inside docker, this is the easiest way to get started\ndocker compose up\n</code></pre> <p>After this, you should be able to access the API at <code>http://localhost:8000</code>. Username/Password will be <code>admin/password</code>.</p> <p>Step 3: Grab the Token for Authentication</p> <p>Login to the API admin, go to <code>http://localhost:8000/authtoken/tokenproxy/</code> and click <code>Add Token</code>.</p> <p></p> <p>Step 4: Collect Audio and Video Data</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run video acquire\npython3 videos_acquire.py --token your_token_from_step_3\n</code></pre> <p>You should be able to see something like this: </p> <p>Then open a new terminal</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Listener\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run audio acquire\npython3 audios_acquire.py --token your_token_from_step_3 --track_cluster CLUSTER_GPT_4O_ETE_CONVERSATION \n# you can change the cluster to the one your need\n</code></pre> <p>You will see something like this: </p> <p>If everything works, you should be able to check the newly create <code>Data Audios</code>, <code>Data Videos</code> and <code>Speech2Text</code> <code>Tasks</code> in API Admin page. Something like below:  </p> <p>Step 5: Run Agent models Now we need to start Agent module to consume the <code>Tasks</code>.</p> <pre><code>cd ./OpenOmniFramework\ncd ./Agent\n\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n</code></pre> <p>Before we start the Agent module, there are some pre-configurations we need to do.</p> <p>As provided functionalities within Agent modules support OpenAI call, HuggingFace call, and there is also our provided emotion detection module.</p> <p>We need to get them setup first.</p> <p>Setup OpenAI and HuggingFace Environment Variable</p> <p>Create a <code>.env</code> file in <code>./Agent</code> folder, and add the following content:</p> <pre><code>HF_TOKEN=Your_HuggingFace_Token\nOPENAI_API_KEY=Your_OpenAI_API_KEY\n</code></pre> <p>Otherwise, you can run</p> <pre><code>export HF_TOKEN=Your_HuggingFace_Token\nexport OPENAI_API_KEY=Your_OpenAI_API_KEY\n</code></pre> <p>For the model part, if you want to get our emotion detection model running, you will need to download the model from download link</p> <p>And put it in the folder: <code>./Agent/data/models/emotion_detection/model_data</code>. It should be like this</p> <p></p> <p>Then you should be ready to run the Agent module.</p> <pre><code># run the Agent module\npython3 main.py --token your_token_from_step_3\n</code></pre> <p>You can also skip the steps to install the requirements, directly run the Agent module with docker.</p> <pre><code>TOKEN=XXX docker compose up\n</code></pre> <p>This will allow you to utilise the GPU resources on your machine if you have one.</p> <p></p> <p>Until now, you will have the client side to feed the video/audio data to the API, and the Agent module to consume the data.</p> <p>Step 6: Play speech audio in client side</p> <pre><code>cd ./OpenOmniFramework\ncd ./Client/Responder\n\n# create the virtual environment if this is your first time run this\npython3 -m venv venv\nsource venv/bin/activate\npip3 install -r requirements.txt\npip3 install -r requirements.dev.txt # if you are doing further development\n\n# run the audio player\n\npython3 play_speech.py --token your_token_from_step_3\n</code></pre> <p>You will see something like this:</p> <p></p> <p>Until now, you should have the whole pipeline running on your local machine.</p> <p>You should see new tasks created as expected in the <code>Tasks</code> page in the API admin page. As shown below:</p> <p></p> <p>And in the Detailed Latency Benchmark page, you should be able to see the latency of each round of conversation.</p> <p></p>"},{"location":"Tutorial/video_demo/","title":"Video Demo","text":""},{"location":"Tutorial/video_demo/#scripts","title":"Scripts","text":"<p>Notes: Here we will use the architecture diagram to explain the scripts.</p> <p>Hello everyone, excited to introduce your our latest work, multimodal Open Source Conversational AI Framework: OpenOmni Framework.</p> <p>Why We build this comes from these points:</p> <ul> <li>It is approachable to build an end-to-end conversational AI system now with current models and tools available.</li> <li>However, if someone want to try out, they will need to speed quite a lot of time to get it work from scratch.</li> <li>And we do not know whether the latency and accuracy is acceptable or not.</li> </ul> <p>So To make sure people do not re-invent the wheel, we build this framework, for details, you can check our documentations.</p> <p>Here what we will demo is one of the useful scenario for the framework, use conversational AI to help visually impaired people to navigate indoors.</p> <p>Notes: Here video will show the three devices, one is the AI module, one is the API module, and one is the client module.</p> <p>We will use the local network deployment option, deploy AI/API/Client modules within three different devices within the same network.</p> <p>So the audio, and video data will be collected from this raspberry pi, and then sync to the API server, together with the metadata.</p> <p>Then the API end will base on the parameters, allocate the task for the AI module, AI will then process the task. For example, speech2text, llm generation, text2speech.</p> <p>All the computational heavy work will happen here.</p> <p>When the results are finished, the data or generated audio will be sent back to the API side</p> <p>And the client side will have another thread to list to the API side, and then play the audio, fulfill the conversation.</p> <p>Note: then next is the demo.</p> <p>Ok, let's start the audio collection process, all other modules are currently running now.</p> <p>Hi, where is my cup of coffee, can you tell me how to grab it?</p> <p>Notes: Then wait for the response, and then play the audio.</p> <p>After this finished, as a research or benchmark process.</p> <p>Note: show the API interface here.</p> <p>You will directly get the latency details and summary stats from our API interface.</p> <p>We can see how long each module takes, and in total how long it takes to finish the whole process, which part are the model inference time, which part is data transfer time.</p> <p>Also, we can annotate and benchmark the accuracy of the process, whether the response tones, content is fit for the scenario.</p> <p>After the annotation, you will be able to see the details and summaries in this page.</p> <p>This can be powerful for the conversational AI system research and application development, you can use this evaluate different combination of pipeline.</p> <p>Gathering datasets, etc.</p> <p>Hopefully this can benefit the wider community, and we are looking forward to your feedback.</p>"},{"location":"Tutorial/video_demo/#procedure-to-start-the-demo","title":"Procedure to start the demo","text":"<ul> <li>API: login and run <code>docker compose up</code> for the API module, make sure <code>export STORAGE_SOLUTION=local</code></li> <li>Client:<ul> <li>login raspberry pi, for Listener, run <code>./start_pi.sh</code></li> <li>login raspberry pi, for Responder, run <code>./start_pi.sh</code></li> </ul> </li> <li>AI:<ul> <li>Run <code>python3 storage.py xxx</code> to sync the data</li> <li>Run <code>python3 main.py</code> to start the AI module</li> </ul> </li> </ul>"}]}